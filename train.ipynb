{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! 注意 !! 訓練前需先執行過voc_annotation.py 檔 !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_path = 'model_data/cls_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "model_path      = 'model_data/yolo4_weight.h5'\n",
    "train_annotation_path = 'Data/train/list/yolo_train.txt'\n",
    "val_annotation_path = 'Data/train/list/yolo_val.txt'\n",
    "\n",
    "class_names = ['ContainerNum']\n",
    "num_classes = 1\n",
    "input_shape = [416, 416]\n",
    "Max_epoch = 10000\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path, encoding='utf-8') as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    anchors = np.array(anchors).reshape(-1, 2)\n",
    "    return anchors, len(anchors)\n",
    "\n",
    "anchors, num_anchors = get_anchors(anchors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init_lr_fit 0.0003\n",
      "Min_lr_fit 2.9999999999999997e-06\n"
     ]
    }
   ],
   "source": [
    "Min_lr = lr * 0.01\n",
    "Init_lr_fit = min(max(4 / 64 * lr, 3e-4), 1e-3)\n",
    "Min_lr_fit = min(max(4 / 64 * Min_lr, 3e-4 * 1e-2), 1e-3 * 1e-2)\n",
    "print('Init_lr_fit', Init_lr_fit)\n",
    "print('Min_lr_fit', Min_lr_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import YoloDatasets\n",
    "\n",
    "with open(train_annotation_path, encoding='utf-8') as f:\n",
    "    train_lines = f.readlines()\n",
    "with open(val_annotation_path, encoding='utf-8') as f:\n",
    "    val_lines   = f.readlines()\n",
    "\n",
    "train_dataloader = YoloDatasets(train_lines, input_shape, anchors, batch_size=4, num_classes=1, anchors_mask=anchors_mask, epoch_now=0, epoch_length=Max_epoch, mosaic=True, train=True)\n",
    "val_dataloader = YoloDatasets(val_lines, input_shape, anchors, batch_size=4, num_classes=1, anchors_mask=anchors_mask, epoch_now=0, epoch_length=Max_epoch, mosaic=False, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立Yolo模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 416, 416, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mish (Mish)                     (None, 416, 416, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 417, 417, 32) 0           mish[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 208, 208, 64) 18432       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 208, 208, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_1 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 208, 208, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_3 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 208, 208, 32) 2048        mish_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 208, 208, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_4 (Mish)                   (None, 208, 208, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 208, 208, 64) 18432       mish_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 208, 208, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_5 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 208, 208, 64) 0           mish_3[0][0]                     \n",
      "                                                                 mish_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 208, 208, 64) 4096        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 208, 208, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 208, 208, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_6 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mish_2 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 208, 208, 128 0           mish_6[0][0]                     \n",
      "                                                                 mish_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 208, 208, 64) 8192        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 208, 208, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_7 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 209, 209, 64) 0           mish_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 104, 104, 128 73728       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 104, 104, 128 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_8 (Mish)                   (None, 104, 104, 128 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 104, 104, 64) 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_10 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 104, 104, 64) 4096        mish_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 104, 104, 64) 256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_11 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 104, 104, 64) 36864       mish_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 104, 104, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_12 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 104, 104, 64) 0           mish_10[0][0]                    \n",
      "                                                                 mish_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 104, 104, 64) 4096        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 104, 104, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_13 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 104, 104, 64) 36864       mish_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 104, 104, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_14 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 104, 104, 64) 0           add_1[0][0]                      \n",
      "                                                                 mish_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 104, 104, 64) 4096        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 104, 104, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 104, 104, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_15 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_9 (Mish)                   (None, 104, 104, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 104, 104, 128 0           mish_15[0][0]                    \n",
      "                                                                 mish_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 104, 104, 128 16384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 104, 104, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_16 (Mish)                  (None, 104, 104, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 105, 105, 128 0           mish_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 52, 52, 256)  294912      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 52, 52, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_17 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 52, 52, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_19 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 52, 52, 128)  16384       mish_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 52, 52, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_20 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 52, 52, 128)  147456      mish_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 52, 52, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_21 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 52, 52, 128)  0           mish_19[0][0]                    \n",
      "                                                                 mish_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 52, 52, 128)  16384       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 52, 52, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_22 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 52, 52, 128)  147456      mish_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 52, 52, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_23 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 52, 52, 128)  0           add_3[0][0]                      \n",
      "                                                                 mish_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 52, 52, 128)  16384       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 52, 52, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_24 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 52, 52, 128)  147456      mish_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 52, 52, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_25 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 52, 52, 128)  0           add_4[0][0]                      \n",
      "                                                                 mish_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 52, 52, 128)  16384       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 52, 52, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_26 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 52, 52, 128)  147456      mish_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 52, 52, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_27 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 52, 52, 128)  0           add_5[0][0]                      \n",
      "                                                                 mish_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 52, 52, 128)  16384       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 52, 52, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_28 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 52, 52, 128)  147456      mish_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 52, 52, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_29 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 52, 52, 128)  0           add_6[0][0]                      \n",
      "                                                                 mish_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 52, 52, 128)  16384       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 52, 52, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_30 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 52, 52, 128)  147456      mish_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 52, 52, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_31 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 52, 52, 128)  0           add_7[0][0]                      \n",
      "                                                                 mish_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 52, 52, 128)  16384       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 52, 52, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_32 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 52, 52, 128)  147456      mish_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 52, 52, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_33 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 52, 52, 128)  0           add_8[0][0]                      \n",
      "                                                                 mish_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 52, 52, 128)  16384       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 52, 52, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_34 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 52, 52, 128)  147456      mish_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 52, 52, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_35 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 52, 52, 128)  0           add_9[0][0]                      \n",
      "                                                                 mish_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 52, 52, 128)  16384       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 52, 52, 128)  512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 52, 52, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_36 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_18 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 52, 52, 256)  0           mish_36[0][0]                    \n",
      "                                                                 mish_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 52, 52, 256)  65536       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 52, 52, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_37 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 53, 53, 256)  0           mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 26, 26, 512)  1179648     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 26, 26, 512)  2048        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_38 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 26, 26, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_40 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 26, 26, 256)  65536       mish_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 26, 26, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_41 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 26, 26, 256)  589824      mish_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 26, 26, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_42 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 26, 26, 256)  0           mish_40[0][0]                    \n",
      "                                                                 mish_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 26, 26, 256)  65536       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 26, 26, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_43 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 26, 26, 256)  589824      mish_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 26, 26, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_44 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 26, 26, 256)  0           add_11[0][0]                     \n",
      "                                                                 mish_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 26, 26, 256)  65536       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 26, 26, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_45 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 26, 26, 256)  589824      mish_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 26, 26, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_46 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 26, 26, 256)  0           add_12[0][0]                     \n",
      "                                                                 mish_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 26, 26, 256)  65536       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 26, 26, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_47 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 26, 26, 256)  589824      mish_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 26, 26, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_48 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 26, 26, 256)  0           add_13[0][0]                     \n",
      "                                                                 mish_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 26, 26, 256)  65536       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 26, 26, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_49 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 26, 26, 256)  589824      mish_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 26, 26, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_50 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 26, 26, 256)  0           add_14[0][0]                     \n",
      "                                                                 mish_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 26, 26, 256)  65536       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 26, 26, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_51 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 26, 26, 256)  589824      mish_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 26, 26, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_52 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 26, 26, 256)  0           add_15[0][0]                     \n",
      "                                                                 mish_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 26, 26, 256)  65536       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 26, 26, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_53 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 26, 26, 256)  589824      mish_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 26, 26, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_54 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 26, 26, 256)  0           add_16[0][0]                     \n",
      "                                                                 mish_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 26, 26, 256)  65536       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 26, 26, 256)  1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_55 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 26, 26, 256)  589824      mish_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 26, 26, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_56 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 26, 26, 256)  0           add_17[0][0]                     \n",
      "                                                                 mish_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 26, 26, 256)  65536       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 26, 26, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 26, 26, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_57 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_39 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 26, 26, 512)  0           mish_57[0][0]                    \n",
      "                                                                 mish_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 26, 26, 512)  262144      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 26, 26, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_58 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 27, 27, 512)  0           mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 13, 13, 1024) 4718592     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 13, 13, 1024) 4096        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_59 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 13, 13, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_61 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 13, 13, 512)  262144      mish_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 13, 13, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_62 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 13, 13, 512)  2359296     mish_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 13, 13, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_63 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 13, 13, 512)  0           mish_61[0][0]                    \n",
      "                                                                 mish_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 13, 13, 512)  262144      add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 13, 13, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_64 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 13, 13, 512)  2359296     mish_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 13, 13, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_65 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 13, 13, 512)  0           add_19[0][0]                     \n",
      "                                                                 mish_65[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 13, 13, 512)  262144      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 13, 13, 512)  2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_66 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 13, 13, 512)  2359296     mish_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 13, 13, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_67 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 13, 13, 512)  0           add_20[0][0]                     \n",
      "                                                                 mish_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 13, 13, 512)  262144      add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 13, 13, 512)  2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_68 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 13, 13, 512)  2359296     mish_68[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 13, 13, 512)  2048        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_69 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 13, 13, 512)  0           add_21[0][0]                     \n",
      "                                                                 mish_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 13, 13, 512)  262144      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 13, 13, 512)  2048        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 13, 13, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_70 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_60 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 13, 13, 1024) 0           mish_70[0][0]                    \n",
      "                                                                 mish_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 13, 13, 1024) 1048576     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 13, 13, 1024) 4096        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_71 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 13, 13, 512)  524288      mish_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 13, 13, 512)  2048        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 13, 13, 512)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 13, 13, 1024) 4096        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 13, 13, 512)  2048        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 13, 13, 2048) 0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 13, 13, 512)  1048576     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 13, 13, 512)  2048        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 13, 13, 1024) 4096        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 13, 13, 512)  2048        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 13, 13, 256)  131072      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 26, 26, 256)  131072      mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 13, 13, 256)  1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 26, 26, 256)  1024        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 13, 13, 256)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 26, 26, 256)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_7[0][0]              \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 26, 26, 256)  1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 26, 26, 512)  2048        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 512)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 26, 26, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 26, 26, 512)  2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 26, 26, 256)  1024        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 26, 26, 128)  32768       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 52, 52, 128)  32768       mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 26, 26, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 52, 52, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 26, 26, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 52, 52, 128)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 52, 52, 256)  0           leaky_re_lu_14[0][0]             \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 52, 52, 128)  32768       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 52, 52, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 52, 52, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 52, 52, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 52, 52, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 52, 52, 128)  512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 53, 53, 128)  0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 26, 26, 256)  294912      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 26, 26, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_21[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 26, 26, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 26, 26, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 26, 26, 256)  1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 26, 26, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 26, 26, 256)  1024        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 27, 27, 256)  0           leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 13, 13, 512)  1179648     zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 13, 13, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 13, 13, 1024) 0           leaky_re_lu_28[0][0]             \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 13, 13, 512)  524288      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 13, 13, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 13, 13, 1024) 4096        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 13, 13, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 13, 13, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 13, 13, 512)  2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 26, 26, 512)  1179648     leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 13, 13, 1024) 4096        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 26, 26, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 52, 52, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 13, 13, 18)   18450       leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 26, 26, 18)   9234        leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 52, 52, 18)   4626        leaky_re_lu_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 64,003,990\n",
      "Trainable params: 63,937,686\n",
      "Non-trainable params: 66,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from nets.yolo import yolo_body, get_train_model\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "model_body  = yolo_body((416, 416, 3), anchors_mask, num_classes)\n",
    "model_body.load_weights(model_path, by_name=True, skip_mismatch=True)\n",
    "\n",
    "model_body.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2正則"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "for layer in model_body.layers:\n",
    "    if isinstance(layer, DepthwiseConv2D):\n",
    "            layer.add_loss(lambda: l2(0)(layer.depthwise_kernel))\n",
    "    elif isinstance(layer, Conv2D) or isinstance(layer, Dense):\n",
    "            layer.add_loss(lambda: l2(0)(layer.kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 凍結權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze the first 249 layers of total 370 layers.\n"
     ]
    }
   ],
   "source": [
    "freeze_layers = 249\n",
    "\n",
    "for i in range(freeze_layers):\n",
    "    model_body.layers[i].trainable = False\n",
    "    \n",
    "print('Freeze the first {} layers of total {} layers.'.format(freeze_layers, len(model_body.layers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 416, 416, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mish (Mish)                     (None, 416, 416, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 417, 417, 32) 0           mish[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 208, 208, 64) 18432       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 208, 208, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_1 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 208, 208, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_3 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 208, 208, 32) 2048        mish_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 208, 208, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_4 (Mish)                   (None, 208, 208, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 208, 208, 64) 18432       mish_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 208, 208, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_5 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 208, 208, 64) 0           mish_3[0][0]                     \n",
      "                                                                 mish_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 208, 208, 64) 4096        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 208, 208, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 208, 208, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_6 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mish_2 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 208, 208, 128 0           mish_6[0][0]                     \n",
      "                                                                 mish_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 208, 208, 64) 8192        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 208, 208, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_7 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 209, 209, 64) 0           mish_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 104, 104, 128 73728       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 104, 104, 128 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_8 (Mish)                   (None, 104, 104, 128 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 104, 104, 64) 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_10 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 104, 104, 64) 4096        mish_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 104, 104, 64) 256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_11 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 104, 104, 64) 36864       mish_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 104, 104, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_12 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 104, 104, 64) 0           mish_10[0][0]                    \n",
      "                                                                 mish_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 104, 104, 64) 4096        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 104, 104, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_13 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 104, 104, 64) 36864       mish_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 104, 104, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_14 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 104, 104, 64) 0           add_1[0][0]                      \n",
      "                                                                 mish_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 104, 104, 64) 4096        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 104, 104, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 104, 104, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_15 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_9 (Mish)                   (None, 104, 104, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 104, 104, 128 0           mish_15[0][0]                    \n",
      "                                                                 mish_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 104, 104, 128 16384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 104, 104, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_16 (Mish)                  (None, 104, 104, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 105, 105, 128 0           mish_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 52, 52, 256)  294912      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 52, 52, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_17 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 52, 52, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_19 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 52, 52, 128)  16384       mish_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 52, 52, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_20 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 52, 52, 128)  147456      mish_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 52, 52, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_21 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 52, 52, 128)  0           mish_19[0][0]                    \n",
      "                                                                 mish_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 52, 52, 128)  16384       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 52, 52, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_22 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 52, 52, 128)  147456      mish_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 52, 52, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_23 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 52, 52, 128)  0           add_3[0][0]                      \n",
      "                                                                 mish_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 52, 52, 128)  16384       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 52, 52, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_24 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 52, 52, 128)  147456      mish_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 52, 52, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_25 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 52, 52, 128)  0           add_4[0][0]                      \n",
      "                                                                 mish_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 52, 52, 128)  16384       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 52, 52, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_26 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 52, 52, 128)  147456      mish_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 52, 52, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_27 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 52, 52, 128)  0           add_5[0][0]                      \n",
      "                                                                 mish_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 52, 52, 128)  16384       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 52, 52, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_28 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 52, 52, 128)  147456      mish_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 52, 52, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_29 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 52, 52, 128)  0           add_6[0][0]                      \n",
      "                                                                 mish_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 52, 52, 128)  16384       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 52, 52, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_30 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 52, 52, 128)  147456      mish_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 52, 52, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_31 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 52, 52, 128)  0           add_7[0][0]                      \n",
      "                                                                 mish_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 52, 52, 128)  16384       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 52, 52, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_32 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 52, 52, 128)  147456      mish_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 52, 52, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_33 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 52, 52, 128)  0           add_8[0][0]                      \n",
      "                                                                 mish_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 52, 52, 128)  16384       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 52, 52, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_34 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 52, 52, 128)  147456      mish_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 52, 52, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_35 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 52, 52, 128)  0           add_9[0][0]                      \n",
      "                                                                 mish_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 52, 52, 128)  16384       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 52, 52, 128)  512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 52, 52, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_36 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_18 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 52, 52, 256)  0           mish_36[0][0]                    \n",
      "                                                                 mish_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 52, 52, 256)  65536       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 52, 52, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_37 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 53, 53, 256)  0           mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 26, 26, 512)  1179648     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 26, 26, 512)  2048        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_38 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 26, 26, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_40 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 26, 26, 256)  65536       mish_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 26, 26, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_41 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 26, 26, 256)  589824      mish_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 26, 26, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_42 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 26, 26, 256)  0           mish_40[0][0]                    \n",
      "                                                                 mish_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 26, 26, 256)  65536       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 26, 26, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_43 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 26, 26, 256)  589824      mish_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 26, 26, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_44 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 26, 26, 256)  0           add_11[0][0]                     \n",
      "                                                                 mish_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 26, 26, 256)  65536       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 26, 26, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_45 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 26, 26, 256)  589824      mish_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 26, 26, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_46 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 26, 26, 256)  0           add_12[0][0]                     \n",
      "                                                                 mish_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 26, 26, 256)  65536       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 26, 26, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_47 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 26, 26, 256)  589824      mish_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 26, 26, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_48 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 26, 26, 256)  0           add_13[0][0]                     \n",
      "                                                                 mish_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 26, 26, 256)  65536       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 26, 26, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_49 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 26, 26, 256)  589824      mish_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 26, 26, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_50 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 26, 26, 256)  0           add_14[0][0]                     \n",
      "                                                                 mish_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 26, 26, 256)  65536       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 26, 26, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_51 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 26, 26, 256)  589824      mish_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 26, 26, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_52 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 26, 26, 256)  0           add_15[0][0]                     \n",
      "                                                                 mish_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 26, 26, 256)  65536       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 26, 26, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_53 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 26, 26, 256)  589824      mish_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 26, 26, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_54 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 26, 26, 256)  0           add_16[0][0]                     \n",
      "                                                                 mish_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 26, 26, 256)  65536       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 26, 26, 256)  1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_55 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 26, 26, 256)  589824      mish_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 26, 26, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_56 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 26, 26, 256)  0           add_17[0][0]                     \n",
      "                                                                 mish_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 26, 26, 256)  65536       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 26, 26, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 26, 26, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_57 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_39 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 26, 26, 512)  0           mish_57[0][0]                    \n",
      "                                                                 mish_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 26, 26, 512)  262144      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 26, 26, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_58 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 27, 27, 512)  0           mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 13, 13, 1024) 4718592     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 13, 13, 1024) 4096        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_59 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 13, 13, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_61 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 13, 13, 512)  262144      mish_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 13, 13, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_62 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 13, 13, 512)  2359296     mish_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 13, 13, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_63 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 13, 13, 512)  0           mish_61[0][0]                    \n",
      "                                                                 mish_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 13, 13, 512)  262144      add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 13, 13, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_64 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 13, 13, 512)  2359296     mish_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 13, 13, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_65 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 13, 13, 512)  0           add_19[0][0]                     \n",
      "                                                                 mish_65[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 13, 13, 512)  262144      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 13, 13, 512)  2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_66 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 13, 13, 512)  2359296     mish_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 13, 13, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_67 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 13, 13, 512)  0           add_20[0][0]                     \n",
      "                                                                 mish_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 13, 13, 512)  262144      add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 13, 13, 512)  2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_68 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 13, 13, 512)  2359296     mish_68[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 13, 13, 512)  2048        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_69 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 13, 13, 512)  0           add_21[0][0]                     \n",
      "                                                                 mish_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 13, 13, 512)  262144      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 13, 13, 512)  2048        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 13, 13, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_70 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_60 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 13, 13, 1024) 0           mish_70[0][0]                    \n",
      "                                                                 mish_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 13, 13, 1024) 1048576     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 13, 13, 1024) 4096        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_71 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 13, 13, 512)  524288      mish_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 13, 13, 512)  2048        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 13, 13, 512)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 13, 13, 1024) 4096        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 13, 13, 512)  2048        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 13, 13, 2048) 0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 13, 13, 512)  1048576     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 13, 13, 512)  2048        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 13, 13, 1024) 4096        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 13, 13, 512)  2048        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 13, 13, 256)  131072      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 26, 26, 256)  131072      mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 13, 13, 256)  1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 26, 26, 256)  1024        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 13, 13, 256)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 26, 26, 256)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_7[0][0]              \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 26, 26, 256)  1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 26, 26, 512)  2048        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 512)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 26, 26, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 26, 26, 512)  2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 26, 26, 256)  1024        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 26, 26, 128)  32768       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 52, 52, 128)  32768       mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 26, 26, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 52, 52, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 26, 26, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 52, 52, 128)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 52, 52, 256)  0           leaky_re_lu_14[0][0]             \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 52, 52, 128)  32768       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 52, 52, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 52, 52, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 52, 52, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 52, 52, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 52, 52, 128)  512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 53, 53, 128)  0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 26, 26, 256)  294912      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 26, 26, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_21[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 26, 26, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 26, 26, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 26, 26, 256)  1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 26, 26, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 26, 26, 256)  1024        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 27, 27, 256)  0           leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 13, 13, 512)  1179648     zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 13, 13, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 13, 13, 1024) 0           leaky_re_lu_28[0][0]             \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 13, 13, 512)  524288      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 13, 13, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 13, 13, 1024) 4096        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 13, 13, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 13, 13, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 13, 13, 512)  2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 26, 26, 512)  1179648     leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 13, 13, 1024) 4096        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 26, 26, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 52, 52, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 13, 13, 18)   18450       leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 26, 26, 18)   9234        leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 52, 52, 18)   4626        leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 13, 13, 3, 6 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 26, 26, 3, 6 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 52, 52, 3, 6 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_loss (Lambda)              ()                   0           conv2d_109[0][0]                 \n",
      "                                                                 conv2d_101[0][0]                 \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 64,003,990\n",
      "Trainable params: 37,320,502\n",
      "Non-trainable params: 26,683,488\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_train_model(model_body, input_shape, num_classes, anchors, anchors_mask, label_smoothing=0.005, focal_loss=False, alpha=0.25, gamma=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "opt = optimizers.Adam(lr = lr, beta_1 = 0.937)\n",
    "\n",
    "model.compile(optimizer = opt, loss={'yolo_loss': lambda y_true, y_pred: y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History, TensorBoard, LearningRateScheduler\n",
    "from nets.yolo_training import get_lr_scheduler\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('logs/best_epoch_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping  = EarlyStopping(monitor='val_loss', min_delta = 0, patience = 120, verbose = 1)\n",
    "\n",
    "lr_scheduler_func = get_lr_scheduler('cos', Init_lr_fit, Min_lr_fit, total_iters=Max_epoch)\n",
    "lr_scheduler    = LearningRateScheduler(lr_scheduler_func, verbose = 1)\n",
    "\n",
    "callbacks = [checkpoint_callback, lr_scheduler, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 凍結訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 2.9999999999999997e-05.\n",
      "Epoch 1/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 18.8718\n",
      "Epoch 00001: val_loss improved from inf to 18.75172, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 18.8718 - val_loss: 18.7517\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.9999999999999995e-05.\n",
      "Epoch 2/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 18.6345\n",
      "Epoch 00002: val_loss improved from 18.75172 to 18.74543, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 18.6345 - val_loss: 18.7454\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00014999999999999996.\n",
      "Epoch 3/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 18.1275\n",
      "Epoch 00003: val_loss improved from 18.74543 to 18.69491, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 18.1275 - val_loss: 18.6949\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0002999999999999999.\n",
      "Epoch 4/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 17.0431\n",
      "Epoch 00004: val_loss improved from 18.69491 to 18.58365, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 17.0431 - val_loss: 18.5836\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00029999999264536593.\n",
      "Epoch 5/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 15.5009\n",
      "Epoch 00005: val_loss improved from 18.58365 to 18.45281, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 15.5009 - val_loss: 18.4528\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00029999997058146444.\n",
      "Epoch 6/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 14.0180\n",
      "Epoch 00006: val_loss improved from 18.45281 to 18.30384, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 14.0180 - val_loss: 18.3038\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0002999999338082978.\n",
      "Epoch 7/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 12.5690\n",
      "Epoch 00007: val_loss improved from 18.30384 to 18.10467, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 12.5690 - val_loss: 18.1047\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0002999998823258696.\n",
      "Epoch 8/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 11.2420\n",
      "Epoch 00008: val_loss improved from 18.10467 to 17.89126, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 11.2420 - val_loss: 17.8913\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002999998161341849.\n",
      "Epoch 9/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 10.0966\n",
      "Epoch 00009: val_loss improved from 17.89126 to 17.65610, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 10.0966 - val_loss: 17.6561\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00029999973523325027.\n",
      "Epoch 10/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 9.1353\n",
      "Epoch 00010: val_loss improved from 17.65610 to 17.40363, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 9.1353 - val_loss: 17.4036\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00029999963962307376.\n",
      "Epoch 11/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 8.3143\n",
      "Epoch 00011: val_loss improved from 17.40363 to 17.14748, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 8.3143 - val_loss: 17.1475\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002999995293036648.\n",
      "Epoch 12/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 7.6050\n",
      "Epoch 00012: val_loss improved from 17.14748 to 16.87858, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 7.6050 - val_loss: 16.8786\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002999994042750344.\n",
      "Epoch 13/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 6.9934\n",
      "Epoch 00013: val_loss improved from 16.87858 to 16.58947, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 6.9934 - val_loss: 16.5895\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002999992645371949.\n",
      "Epoch 14/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 6.4799\n",
      "Epoch 00014: val_loss improved from 16.58947 to 16.27787, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 6.4799 - val_loss: 16.2779\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0002999991100901601.\n",
      "Epoch 15/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 5.9929\n",
      "Epoch 00015: val_loss improved from 16.27787 to 15.95009, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 5.9929 - val_loss: 15.9501\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0002999989409339453.\n",
      "Epoch 16/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 5.5438\n",
      "Epoch 00016: val_loss improved from 15.95009 to 15.60431, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 5.5438 - val_loss: 15.6043\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0002999987570685673.\n",
      "Epoch 17/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 5.1601\n",
      "Epoch 00017: val_loss improved from 15.60431 to 15.24224, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 5.1601 - val_loss: 15.2422\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0002999985584940443.\n",
      "Epoch 18/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.8066\n",
      "Epoch 00018: val_loss improved from 15.24224 to 14.86712, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 4.8066 - val_loss: 14.8671\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0002999983452103959.\n",
      "Epoch 19/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.4982\n",
      "Epoch 00019: val_loss improved from 14.86712 to 14.48488, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.4982 - val_loss: 14.4849\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00029999811721764336.\n",
      "Epoch 20/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.1950\n",
      "Epoch 00020: val_loss improved from 14.48488 to 14.09536, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 4.1950 - val_loss: 14.0954\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0002999978745158092.\n",
      "Epoch 21/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.9465\n",
      "Epoch 00021: val_loss improved from 14.09536 to 13.69873, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.9465 - val_loss: 13.6987\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0002999976171049174.\n",
      "Epoch 22/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.6957\n",
      "Epoch 00022: val_loss improved from 13.69873 to 13.29892, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.6957 - val_loss: 13.2989\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00029999734498499357.\n",
      "Epoch 23/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.4896\n",
      "Epoch 00023: val_loss improved from 13.29892 to 12.89688, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.4896 - val_loss: 12.8969\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0002999970581560646.\n",
      "Epoch 24/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.2676\n",
      "Epoch 00024: val_loss improved from 12.89688 to 12.49752, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.2676 - val_loss: 12.4975\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0002999967566181588.\n",
      "Epoch 25/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.0732\n",
      "Epoch 00025: val_loss improved from 12.49752 to 12.10276, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.0732 - val_loss: 12.1028\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0002999964403713063.\n",
      "Epoch 26/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.9007\n",
      "Epoch 00026: val_loss improved from 12.10276 to 11.71598, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.9007 - val_loss: 11.7160\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0002999961094155381.\n",
      "Epoch 27/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.7583\n",
      "Epoch 00027: val_loss improved from 11.71598 to 11.33340, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.7583 - val_loss: 11.3334\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00029999576375088724.\n",
      "Epoch 28/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.5960\n",
      "Epoch 00028: val_loss improved from 11.33340 to 10.95623, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.5960 - val_loss: 10.9562\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00029999540337738786.\n",
      "Epoch 29/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.4783\n",
      "Epoch 00029: val_loss improved from 10.95623 to 10.59153, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.4783 - val_loss: 10.5915\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0002999950282950757.\n",
      "Epoch 30/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.3570\n",
      "Epoch 00030: val_loss improved from 10.59153 to 10.23000, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.3570 - val_loss: 10.2300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00029999463850398784.\n",
      "Epoch 31/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.2242\n",
      "Epoch 00031: val_loss improved from 10.23000 to 9.87078, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.2242 - val_loss: 9.8708\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00029999423400416293.\n",
      "Epoch 32/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.1311\n",
      "Epoch 00032: val_loss improved from 9.87078 to 9.52150, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.1311 - val_loss: 9.5215\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00029999381479564103.\n",
      "Epoch 33/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.0286\n",
      "Epoch 00033: val_loss improved from 9.52150 to 9.18939, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0286 - val_loss: 9.1894\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00029999338087846367.\n",
      "Epoch 34/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.9291\n",
      "Epoch 00034: val_loss improved from 9.18939 to 8.86599, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.9291 - val_loss: 8.8660\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00029999293225267383.\n",
      "Epoch 35/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.8518\n",
      "Epoch 00035: val_loss improved from 8.86599 to 8.54311, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.8518 - val_loss: 8.5431\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0002999924689183159.\n",
      "Epoch 36/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.7759\n",
      "Epoch 00036: val_loss improved from 8.54311 to 8.23486, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.7759 - val_loss: 8.2349\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0002999919908754359.\n",
      "Epoch 37/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.7032\n",
      "Epoch 00037: val_loss improved from 8.23486 to 7.93881, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.7032 - val_loss: 7.9388\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0002999914981240811.\n",
      "Epoch 38/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6552\n",
      "Epoch 00038: val_loss improved from 7.93881 to 7.64338, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.6552 - val_loss: 7.6434\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00029999099066430025.\n",
      "Epoch 39/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.5580\n",
      "Epoch 00039: val_loss improved from 7.64338 to 7.34696, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.5580 - val_loss: 7.3470\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00029999046849614366.\n",
      "Epoch 40/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.5040\n",
      "Epoch 00040: val_loss improved from 7.34696 to 7.06480, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.5040 - val_loss: 7.0648\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0002999899316196631.\n",
      "Epoch 41/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.4524\n",
      "Epoch 00041: val_loss improved from 7.06480 to 6.80944, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.4524 - val_loss: 6.8094\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00029998938003491176.\n",
      "Epoch 42/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 00042: val_loss improved from 6.80944 to 6.56777, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.4219 - val_loss: 6.5678\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00029998881374194416.\n",
      "Epoch 43/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.3592\n",
      "Epoch 00043: val_loss improved from 6.56777 to 6.32559, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.3592 - val_loss: 6.3256\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00029998823274081653.\n",
      "Epoch 44/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.3010\n",
      "Epoch 00044: val_loss improved from 6.32559 to 6.09104, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.3010 - val_loss: 6.0910\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00029998763703158627.\n",
      "Epoch 45/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.2499\n",
      "Epoch 00045: val_loss improved from 6.09104 to 5.87552, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.2499 - val_loss: 5.8755\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0002999870266143125.\n",
      "Epoch 46/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.2181\n",
      "Epoch 00046: val_loss improved from 5.87552 to 5.67290, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.2181 - val_loss: 5.6729\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0002999864014890557.\n",
      "Epoch 47/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1837\n",
      "Epoch 00047: val_loss improved from 5.67290 to 5.47238, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.1837 - val_loss: 5.4724\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00029998576165587765.\n",
      "Epoch 48/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1398\n",
      "Epoch 00048: val_loss improved from 5.47238 to 5.26370, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.1398 - val_loss: 5.2637\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0002999851071148419.\n",
      "Epoch 49/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1181\n",
      "Epoch 00049: val_loss improved from 5.26370 to 5.05701, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.1181 - val_loss: 5.0570\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0002999844378660132.\n",
      "Epoch 50/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0851\n",
      "Epoch 00050: val_loss improved from 5.05701 to 4.86791, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.0851 - val_loss: 4.8679\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0002999837539094578.\n",
      "Epoch 51/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0446\n",
      "Epoch 00051: val_loss improved from 4.86791 to 4.69388, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.0446 - val_loss: 4.6939\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0002999830552452435.\n",
      "Epoch 52/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0399\n",
      "Epoch 00052: val_loss improved from 4.69388 to 4.52663, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.0399 - val_loss: 4.5266\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0002999823418734395.\n",
      "Epoch 53/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0022\n",
      "Epoch 00053: val_loss improved from 4.52663 to 4.36239, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.0022 - val_loss: 4.3624\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0002999816137941165.\n",
      "Epoch 54/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9650\n",
      "Epoch 00054: val_loss improved from 4.36239 to 4.20645, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.9650 - val_loss: 4.2064\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.00029998087100734647.\n",
      "Epoch 55/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9354\n",
      "Epoch 00055: val_loss improved from 4.20645 to 4.06900, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.9354 - val_loss: 4.0690\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0002999801135132032.\n",
      "Epoch 56/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9329\n",
      "Epoch 00056: val_loss improved from 4.06900 to 3.93307, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.9329 - val_loss: 3.9331\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.00029997934131176154.\n",
      "Epoch 57/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8820\n",
      "Epoch 00057: val_loss improved from 3.93307 to 3.78403, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.8820 - val_loss: 3.7840\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0002999785544030981.\n",
      "Epoch 58/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8709\n",
      "Epoch 00058: val_loss improved from 3.78403 to 3.64374, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.8709 - val_loss: 3.6437\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0002999777527872907.\n",
      "Epoch 59/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8457\n",
      "Epoch 00059: val_loss improved from 3.64374 to 3.52458, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.8457 - val_loss: 3.5246\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0002999769364644189.\n",
      "Epoch 60/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8209\n",
      "Epoch 00060: val_loss improved from 3.52458 to 3.41619, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.8209 - val_loss: 3.4162\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00029997610543456345.\n",
      "Epoch 61/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8075\n",
      "Epoch 00061: val_loss improved from 3.41619 to 3.30276, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.8075 - val_loss: 3.3028\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0002999752596978067.\n",
      "Epoch 62/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7741\n",
      "Epoch 00062: val_loss improved from 3.30276 to 3.18992, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.7741 - val_loss: 3.1899\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0002999743992542324.\n",
      "Epoch 63/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7768\n",
      "Epoch 00063: val_loss improved from 3.18992 to 3.08536, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.7768 - val_loss: 3.0854\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.00029997352410392577.\n",
      "Epoch 64/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7591\n",
      "Epoch 00064: val_loss improved from 3.08536 to 2.97947, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.7591 - val_loss: 2.9795\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.00029997263424697354.\n",
      "Epoch 65/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7236\n",
      "Epoch 00065: val_loss improved from 2.97947 to 2.87517, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.7236 - val_loss: 2.8752\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0002999717296834638.\n",
      "Epoch 66/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7252\n",
      "Epoch 00066: val_loss improved from 2.87517 to 2.78640, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.7252 - val_loss: 2.7864\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00029997081041348615.\n",
      "Epoch 67/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7094\n",
      "Epoch 00067: val_loss improved from 2.78640 to 2.69973, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.7094 - val_loss: 2.6997\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0002999698764371317.\n",
      "Epoch 68/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6902\n",
      "Epoch 00068: val_loss improved from 2.69973 to 2.60902, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6902 - val_loss: 2.6090\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.000299968927754493.\n",
      "Epoch 69/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6787\n",
      "Epoch 00069: val_loss improved from 2.60902 to 2.52099, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6787 - val_loss: 2.5210\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0002999679643656639.\n",
      "Epoch 70/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6663\n",
      "Epoch 00070: val_loss improved from 2.52099 to 2.45298, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6663 - val_loss: 2.4530\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00029996698627073986.\n",
      "Epoch 71/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6393\n",
      "Epoch 00071: val_loss improved from 2.45298 to 2.38637, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6393 - val_loss: 2.3864\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0002999659934698179.\n",
      "Epoch 72/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6606\n",
      "Epoch 00072: val_loss improved from 2.38637 to 2.31667, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6606 - val_loss: 2.3167\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00029996498596299615.\n",
      "Epoch 73/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6344\n",
      "Epoch 00073: val_loss improved from 2.31667 to 2.24547, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6344 - val_loss: 2.2455\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0002999639637503745.\n",
      "Epoch 74/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6213\n",
      "Epoch 00074: val_loss improved from 2.24547 to 2.17748, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6213 - val_loss: 2.1775\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0002999629268320542.\n",
      "Epoch 75/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5988\n",
      "Epoch 00075: val_loss improved from 2.17748 to 2.10838, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5988 - val_loss: 2.1084\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00029996187520813796.\n",
      "Epoch 76/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5902\n",
      "Epoch 00076: val_loss improved from 2.10838 to 2.04490, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5902 - val_loss: 2.0449\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00029996080887873.\n",
      "Epoch 77/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5996\n",
      "Epoch 00077: val_loss improved from 2.04490 to 1.98463, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5996 - val_loss: 1.9846\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0002999597278439358.\n",
      "Epoch 78/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5744\n",
      "Epoch 00078: val_loss improved from 1.98463 to 1.92209, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5744 - val_loss: 1.9221\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0002999586321038626.\n",
      "Epoch 79/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5470\n",
      "Epoch 00079: val_loss improved from 1.92209 to 1.86145, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5470 - val_loss: 1.8615\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.00029995752165861886.\n",
      "Epoch 80/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5251\n",
      "Epoch 00080: val_loss improved from 1.86145 to 1.79943, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5251 - val_loss: 1.7994\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.00029995639650831456.\n",
      "Epoch 81/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5378\n",
      "Epoch 00081: val_loss improved from 1.79943 to 1.73958, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5378 - val_loss: 1.7396\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0002999552566530612.\n",
      "Epoch 82/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5194\n",
      "Epoch 00082: val_loss improved from 1.73958 to 1.68783, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5194 - val_loss: 1.6878\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.00029995410209297156.\n",
      "Epoch 83/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5218\n",
      "Epoch 00083: val_loss improved from 1.68783 to 1.64007, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5218 - val_loss: 1.6401\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0002999529328281602.\n",
      "Epoch 84/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5230\n",
      "Epoch 00084: val_loss improved from 1.64007 to 1.59278, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5230 - val_loss: 1.5928\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.00029995174885874277.\n",
      "Epoch 85/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4903\n",
      "Epoch 00085: val_loss improved from 1.59278 to 1.54349, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4903 - val_loss: 1.5435\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.00029995055018483655.\n",
      "Epoch 86/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5078\n",
      "Epoch 00086: val_loss improved from 1.54349 to 1.49642, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5078 - val_loss: 1.4964\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0002999493368065604.\n",
      "Epoch 87/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4763\n",
      "Epoch 00087: val_loss improved from 1.49642 to 1.45715, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4763 - val_loss: 1.4572\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.00029994810872403446.\n",
      "Epoch 88/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4619\n",
      "Epoch 00088: val_loss improved from 1.45715 to 1.42245, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4619 - val_loss: 1.4224\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.0002999468659373803.\n",
      "Epoch 89/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4703\n",
      "Epoch 00089: val_loss improved from 1.42245 to 1.38026, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4703 - val_loss: 1.3803\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0002999456084467211.\n",
      "Epoch 90/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4619\n",
      "Epoch 00090: val_loss improved from 1.38026 to 1.34034, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4619 - val_loss: 1.3403\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00029994433625218133.\n",
      "Epoch 91/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4466\n",
      "Epoch 00091: val_loss improved from 1.34034 to 1.30002, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4466 - val_loss: 1.3000\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0002999430493538871.\n",
      "Epoch 92/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4385\n",
      "Epoch 00092: val_loss improved from 1.30002 to 1.26461, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4385 - val_loss: 1.2646\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.00029994174775196584.\n",
      "Epoch 93/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4688\n",
      "Epoch 00093: val_loss improved from 1.26461 to 1.23492, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4688 - val_loss: 1.2349\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0002999404314465465.\n",
      "Epoch 94/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4256\n",
      "Epoch 00094: val_loss improved from 1.23492 to 1.20460, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4256 - val_loss: 1.2046\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.00029993910043775944.\n",
      "Epoch 95/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4199\n",
      "Epoch 00095: val_loss improved from 1.20460 to 1.17970, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4199 - val_loss: 1.1797\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.00029993775472573645.\n",
      "Epoch 96/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4150\n",
      "Epoch 00096: val_loss improved from 1.17970 to 1.16301, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4150 - val_loss: 1.1630\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0002999363943106109.\n",
      "Epoch 97/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4160\n",
      "Epoch 00097: val_loss improved from 1.16301 to 1.13567, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4160 - val_loss: 1.1357\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.00029993501919251756.\n",
      "Epoch 98/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4102\n",
      "Epoch 00098: val_loss improved from 1.13567 to 1.09543, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4102 - val_loss: 1.0954\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.00029993362937159255.\n",
      "Epoch 99/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3893\n",
      "Epoch 00099: val_loss improved from 1.09543 to 1.06509, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3893 - val_loss: 1.0651\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0002999322248479736.\n",
      "Epoch 100/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4074\n",
      "Epoch 00100: val_loss improved from 1.06509 to 1.04552, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4074 - val_loss: 1.0455\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0002999308056217998.\n",
      "Epoch 101/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3964\n",
      "Epoch 00101: val_loss improved from 1.04552 to 1.02479, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3964 - val_loss: 1.0248\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.00029992937169321174.\n",
      "Epoch 102/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4138\n",
      "Epoch 00102: val_loss improved from 1.02479 to 1.00742, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4138 - val_loss: 1.0074\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.00029992792306235145.\n",
      "Epoch 103/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3862\n",
      "Epoch 00103: val_loss improved from 1.00742 to 0.98817, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3862 - val_loss: 0.9882\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.00029992645972936244.\n",
      "Epoch 104/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3967\n",
      "Epoch 00104: val_loss improved from 0.98817 to 0.96789, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3967 - val_loss: 0.9679\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.00029992498169438965.\n",
      "Epoch 105/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4039\n",
      "Epoch 00105: val_loss improved from 0.96789 to 0.95308, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4039 - val_loss: 0.9531\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.00029992348895757946.\n",
      "Epoch 106/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3729\n",
      "Epoch 00106: val_loss improved from 0.95308 to 0.93451, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3729 - val_loss: 0.9345\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.00029992198151907975.\n",
      "Epoch 107/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3699\n",
      "Epoch 00107: val_loss improved from 0.93451 to 0.91036, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3699 - val_loss: 0.9104\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.00029992045937903987.\n",
      "Epoch 108/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3517\n",
      "Epoch 00108: val_loss improved from 0.91036 to 0.89221, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3517 - val_loss: 0.8922\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0002999189225376105.\n",
      "Epoch 109/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3476\n",
      "Epoch 00109: val_loss improved from 0.89221 to 0.88842, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3476 - val_loss: 0.8884\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.00029991737099494393.\n",
      "Epoch 110/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3378\n",
      "Epoch 00110: val_loss improved from 0.88842 to 0.88661, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3378 - val_loss: 0.8866\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.00029991580475119384.\n",
      "Epoch 111/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3500\n",
      "Epoch 00111: val_loss improved from 0.88661 to 0.87320, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3500 - val_loss: 0.8732\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.00029991422380651534.\n",
      "Epoch 112/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3490\n",
      "Epoch 00112: val_loss improved from 0.87320 to 0.84410, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3490 - val_loss: 0.8441\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.00029991262816106505.\n",
      "Epoch 113/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3236\n",
      "Epoch 00113: val_loss improved from 0.84410 to 0.81797, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3236 - val_loss: 0.8180\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.00029991101781500103.\n",
      "Epoch 114/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3226\n",
      "Epoch 00114: val_loss improved from 0.81797 to 0.80691, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3226 - val_loss: 0.8069\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0002999093927684828.\n",
      "Epoch 115/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3490\n",
      "Epoch 00115: val_loss improved from 0.80691 to 0.80525, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3490 - val_loss: 0.8053\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.00029990775302167125.\n",
      "Epoch 116/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3286\n",
      "Epoch 00116: val_loss improved from 0.80525 to 0.80018, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3286 - val_loss: 0.8002\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0002999060985747289.\n",
      "Epoch 117/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3311\n",
      "Epoch 00117: val_loss improved from 0.80018 to 0.79030, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3311 - val_loss: 0.7903\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.00029990442942781956.\n",
      "Epoch 118/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3003\n",
      "Epoch 00118: val_loss improved from 0.79030 to 0.77615, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3003 - val_loss: 0.7762\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0002999027455811086.\n",
      "Epoch 119/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2997\n",
      "Epoch 00119: val_loss improved from 0.77615 to 0.76000, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2997 - val_loss: 0.7600\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0002999010470347628.\n",
      "Epoch 120/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3021\n",
      "Epoch 00120: val_loss improved from 0.76000 to 0.74252, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3021 - val_loss: 0.7425\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0002998993337889504.\n",
      "Epoch 121/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3096\n",
      "Epoch 00121: val_loss improved from 0.74252 to 0.72803, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3096 - val_loss: 0.7280\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.00029989760584384106.\n",
      "Epoch 122/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2977\n",
      "Epoch 00122: val_loss improved from 0.72803 to 0.71860, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2977 - val_loss: 0.7186\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.00029989586319960603.\n",
      "Epoch 123/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3033\n",
      "Epoch 00123: val_loss improved from 0.71860 to 0.71480, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3033 - val_loss: 0.7148\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0002998941058564178.\n",
      "Epoch 124/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2845\n",
      "Epoch 00124: val_loss improved from 0.71480 to 0.70923, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2845 - val_loss: 0.7092\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0002998923338144506.\n",
      "Epoch 125/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3017\n",
      "Epoch 00125: val_loss improved from 0.70923 to 0.69674, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3017 - val_loss: 0.6967\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.00029989054707387977.\n",
      "Epoch 126/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2908\n",
      "Epoch 00126: val_loss improved from 0.69674 to 0.67959, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2908 - val_loss: 0.6796\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.00029988874563488245.\n",
      "Epoch 127/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2888\n",
      "Epoch 00127: val_loss improved from 0.67959 to 0.65653, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2888 - val_loss: 0.6565\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.000299886929497637.\n",
      "Epoch 128/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2866\n",
      "Epoch 00128: val_loss improved from 0.65653 to 0.63999, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2866 - val_loss: 0.6400\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0002998850986623232.\n",
      "Epoch 129/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2797\n",
      "Epoch 00129: val_loss improved from 0.63999 to 0.63642, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2797 - val_loss: 0.6364\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.00029988325312912267.\n",
      "Epoch 130/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2747\n",
      "Epoch 00130: val_loss did not improve from 0.63642\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.2747 - val_loss: 0.6383\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0002998813928982181.\n",
      "Epoch 131/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2727\n",
      "Epoch 00131: val_loss did not improve from 0.63642\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.2727 - val_loss: 0.6402\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0002998795179697936.\n",
      "Epoch 132/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3020\n",
      "Epoch 00132: val_loss improved from 0.63642 to 0.63074, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3020 - val_loss: 0.6307\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.000299877628344035.\n",
      "Epoch 133/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2568\n",
      "Epoch 00133: val_loss improved from 0.63074 to 0.61598, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2568 - val_loss: 0.6160\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0002998757240211295.\n",
      "Epoch 134/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2979\n",
      "Epoch 00134: val_loss improved from 0.61598 to 0.60789, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2979 - val_loss: 0.6079\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.00029987380500126577.\n",
      "Epoch 135/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2845\n",
      "Epoch 00135: val_loss improved from 0.60789 to 0.60177, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2845 - val_loss: 0.6018\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.00029987187128463374.\n",
      "Epoch 136/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2642\n",
      "Epoch 00136: val_loss improved from 0.60177 to 0.59698, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 4s 1s/step - loss: 0.2642 - val_loss: 0.5970\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.00029986992287142503.\n",
      "Epoch 137/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2676\n",
      "Epoch 00137: val_loss improved from 0.59698 to 0.59024, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2676 - val_loss: 0.5902\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0002998679597618327.\n",
      "Epoch 138/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2444\n",
      "Epoch 00138: val_loss improved from 0.59024 to 0.57833, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2444 - val_loss: 0.5783\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0002998659819560511.\n",
      "Epoch 139/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2761\n",
      "Epoch 00139: val_loss improved from 0.57833 to 0.57063, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2761 - val_loss: 0.5706\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0002998639894542762.\n",
      "Epoch 140/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2618\n",
      "Epoch 00140: val_loss improved from 0.57063 to 0.56546, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2618 - val_loss: 0.5655\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0002998619822567053.\n",
      "Epoch 141/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2570\n",
      "Epoch 00141: val_loss improved from 0.56546 to 0.56037, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2570 - val_loss: 0.5604\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0002998599603635373.\n",
      "Epoch 142/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2535\n",
      "Epoch 00142: val_loss improved from 0.56037 to 0.54822, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2535 - val_loss: 0.5482\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0002998579237749724.\n",
      "Epoch 143/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2632\n",
      "Epoch 00143: val_loss improved from 0.54822 to 0.53515, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2632 - val_loss: 0.5352\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.00029985587249121233.\n",
      "Epoch 144/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2604\n",
      "Epoch 00144: val_loss improved from 0.53515 to 0.52998, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2604 - val_loss: 0.5300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0002998538065124603.\n",
      "Epoch 145/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2467\n",
      "Epoch 00145: val_loss did not improve from 0.52998\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.2467 - val_loss: 0.5302\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.00029985172583892104.\n",
      "Epoch 146/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2754\n",
      "Epoch 00146: val_loss did not improve from 0.52998\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.2754 - val_loss: 0.5306\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0002998496304708005.\n",
      "Epoch 147/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2512\n",
      "Epoch 00147: val_loss improved from 0.52998 to 0.52547, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2512 - val_loss: 0.5255\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0002998475204083063.\n",
      "Epoch 148/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2690\n",
      "Epoch 00148: val_loss improved from 0.52547 to 0.51636, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2690 - val_loss: 0.5164\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0002998453956516474.\n",
      "Epoch 149/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2447\n",
      "Epoch 00149: val_loss improved from 0.51636 to 0.50541, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2447 - val_loss: 0.5054\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0002998432562010343.\n",
      "Epoch 150/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2547\n",
      "Epoch 00150: val_loss improved from 0.50541 to 0.49825, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2547 - val_loss: 0.4982\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.00029984110205667894.\n",
      "Epoch 151/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2407\n",
      "Epoch 00151: val_loss improved from 0.49825 to 0.49763, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2407 - val_loss: 0.4976\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0002998389332187947.\n",
      "Epoch 152/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2391\n",
      "Epoch 00152: val_loss improved from 0.49763 to 0.49328, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2391 - val_loss: 0.4933\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0002998367496875963.\n",
      "Epoch 153/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2349\n",
      "Epoch 00153: val_loss improved from 0.49328 to 0.48674, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2349 - val_loss: 0.4867\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0002998345514633001.\n",
      "Epoch 154/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2297\n",
      "Epoch 00154: val_loss improved from 0.48674 to 0.47843, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2297 - val_loss: 0.4784\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0002998323385461239.\n",
      "Epoch 155/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2224\n",
      "Epoch 00155: val_loss improved from 0.47843 to 0.47421, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2224 - val_loss: 0.4742\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0002998301109362868.\n",
      "Epoch 156/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2348\n",
      "Epoch 00156: val_loss did not improve from 0.47421\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.2348 - val_loss: 0.4759\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.00029982786863400946.\n",
      "Epoch 157/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00157: val_loss did not improve from 0.47421\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.2455 - val_loss: 0.4750\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.00029982561163951403.\n",
      "Epoch 158/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2298\n",
      "Epoch 00158: val_loss improved from 0.47421 to 0.47196, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2298 - val_loss: 0.4720\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.00029982333995302407.\n",
      "Epoch 159/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2164\n",
      "Epoch 00159: val_loss improved from 0.47196 to 0.46527, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2164 - val_loss: 0.4653\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0002998210535747645.\n",
      "Epoch 160/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00160: val_loss improved from 0.46527 to 0.45876, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2054 - val_loss: 0.4588\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0002998187525049619.\n",
      "Epoch 161/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2180\n",
      "Epoch 00161: val_loss improved from 0.45876 to 0.45172, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2180 - val_loss: 0.4517\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0002998164367438442.\n",
      "Epoch 162/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2362\n",
      "Epoch 00162: val_loss improved from 0.45172 to 0.44535, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2362 - val_loss: 0.4453\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0002998141062916407.\n",
      "Epoch 163/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2195\n",
      "Epoch 00163: val_loss improved from 0.44535 to 0.43648, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2195 - val_loss: 0.4365\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.00029981176114858235.\n",
      "Epoch 164/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1913\n",
      "Epoch 00164: val_loss improved from 0.43648 to 0.42999, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1913 - val_loss: 0.4300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0002998094013149013.\n",
      "Epoch 165/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1961\n",
      "Epoch 00165: val_loss improved from 0.42999 to 0.42735, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1961 - val_loss: 0.4274\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0002998070267908314.\n",
      "Epoch 166/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2176\n",
      "Epoch 00166: val_loss improved from 0.42735 to 0.42621, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2176 - val_loss: 0.4262\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0002998046375766078.\n",
      "Epoch 167/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00167: val_loss improved from 0.42621 to 0.42513, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2185 - val_loss: 0.4251\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0002998022336724672.\n",
      "Epoch 168/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2092\n",
      "Epoch 00168: val_loss did not improve from 0.42513\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.2092 - val_loss: 0.4254\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0002997998150786477.\n",
      "Epoch 169/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2148\n",
      "Epoch 00169: val_loss did not improve from 0.42513\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.2148 - val_loss: 0.4261\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.00029979738179538885.\n",
      "Epoch 170/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2327\n",
      "Epoch 00170: val_loss improved from 0.42513 to 0.41857, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2327 - val_loss: 0.4186\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.00029979493382293167.\n",
      "Epoch 171/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1942\n",
      "Epoch 00171: val_loss improved from 0.41857 to 0.40799, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1942 - val_loss: 0.4080\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0002997924711615186.\n",
      "Epoch 172/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2186\n",
      "Epoch 00172: val_loss improved from 0.40799 to 0.39778, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2186 - val_loss: 0.3978\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.00029978999381139365.\n",
      "Epoch 173/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2248\n",
      "Epoch 00173: val_loss improved from 0.39778 to 0.39097, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2248 - val_loss: 0.3910\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0002997875017728022.\n",
      "Epoch 174/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2189\n",
      "Epoch 00174: val_loss did not improve from 0.39097\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.2189 - val_loss: 0.3915\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.000299784995045991.\n",
      "Epoch 175/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00175: val_loss did not improve from 0.39097\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.1893 - val_loss: 0.3919\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.00029978247363120854.\n",
      "Epoch 176/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00176: val_loss improved from 0.39097 to 0.38574, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2073 - val_loss: 0.3857\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0002997799375287044.\n",
      "Epoch 177/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2149\n",
      "Epoch 00177: val_loss improved from 0.38574 to 0.37534, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2149 - val_loss: 0.3753\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0002997773867387298.\n",
      "Epoch 178/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1995\n",
      "Epoch 00178: val_loss improved from 0.37534 to 0.36509, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1995 - val_loss: 0.3651\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.00029977482126153745.\n",
      "Epoch 179/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00179: val_loss improved from 0.36509 to 0.35976, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2032 - val_loss: 0.3598\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.00029977224109738144.\n",
      "Epoch 180/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00180: val_loss did not improve from 0.35976\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1917 - val_loss: 0.3630\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.00029976964624651737.\n",
      "Epoch 181/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00181: val_loss did not improve from 0.35976\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.2087 - val_loss: 0.3676\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.00029976703670920224.\n",
      "Epoch 182/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00182: val_loss did not improve from 0.35976\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.2025 - val_loss: 0.3646\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0002997644124856945.\n",
      "Epoch 183/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00183: val_loss improved from 0.35976 to 0.35367, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1760 - val_loss: 0.3537\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0002997617735762542.\n",
      "Epoch 184/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1848\n",
      "Epoch 00184: val_loss improved from 0.35367 to 0.34551, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1848 - val_loss: 0.3455\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0002997591199811426.\n",
      "Epoch 185/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1930\n",
      "Epoch 00185: val_loss improved from 0.34551 to 0.34410, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1930 - val_loss: 0.3441\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0002997564517006227.\n",
      "Epoch 186/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2276\n",
      "Epoch 00186: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.2276 - val_loss: 0.3463\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0002997537687349586.\n",
      "Epoch 187/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1821\n",
      "Epoch 00187: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1821 - val_loss: 0.3494\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.00029975107108441614.\n",
      "Epoch 188/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1983\n",
      "Epoch 00188: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1983 - val_loss: 0.3523\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.00029974835874926263.\n",
      "Epoch 189/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00189: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1893 - val_loss: 0.3526\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.00029974563172976656.\n",
      "Epoch 190/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2166\n",
      "Epoch 00190: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.2166 - val_loss: 0.3518\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.00029974289002619817.\n",
      "Epoch 191/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1752\n",
      "Epoch 00191: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1752 - val_loss: 0.3485\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.000299740133638829.\n",
      "Epoch 192/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1865\n",
      "Epoch 00192: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.1865 - val_loss: 0.3480\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.00029973736256793206.\n",
      "Epoch 193/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1693\n",
      "Epoch 00193: val_loss did not improve from 0.34410\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1693 - val_loss: 0.3461\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.00029973457681378187.\n",
      "Epoch 194/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1999\n",
      "Epoch 00194: val_loss improved from 0.34410 to 0.33986, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1999 - val_loss: 0.3399\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.00029973177637665434.\n",
      "Epoch 195/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1751\n",
      "Epoch 00195: val_loss improved from 0.33986 to 0.33422, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1751 - val_loss: 0.3342\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0002997289612568268.\n",
      "Epoch 196/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1702\n",
      "Epoch 00196: val_loss improved from 0.33422 to 0.32953, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1702 - val_loss: 0.3295\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.00029972613145457823.\n",
      "Epoch 197/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00197: val_loss improved from 0.32953 to 0.32680, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1861 - val_loss: 0.3268\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0002997232869701888.\n",
      "Epoch 198/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1766\n",
      "Epoch 00198: val_loss did not improve from 0.32680\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1766 - val_loss: 0.3271\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.00029972042780394034.\n",
      "Epoch 199/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00199: val_loss did not improve from 0.32680\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1844 - val_loss: 0.3274\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.00029971755395611597.\n",
      "Epoch 200/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00200: val_loss did not improve from 0.32680\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.2071 - val_loss: 0.3289\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.00029971466542700047.\n",
      "Epoch 201/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00201: val_loss did not improve from 0.32680\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1889 - val_loss: 0.3292\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0002997117622168799.\n",
      "Epoch 202/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00202: val_loss improved from 0.32680 to 0.32475, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1621 - val_loss: 0.3247\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0002997088443260418.\n",
      "Epoch 203/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1736\n",
      "Epoch 00203: val_loss improved from 0.32475 to 0.31764, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1736 - val_loss: 0.3176\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.00029970591175477524.\n",
      "Epoch 204/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1916\n",
      "Epoch 00204: val_loss improved from 0.31764 to 0.31195, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1916 - val_loss: 0.3119\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.00029970296450337067.\n",
      "Epoch 205/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1693\n",
      "Epoch 00205: val_loss improved from 0.31195 to 0.30864, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1693 - val_loss: 0.3086\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.00029970000257212005.\n",
      "Epoch 206/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1688\n",
      "Epoch 00206: val_loss improved from 0.30864 to 0.30542, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1688 - val_loss: 0.3054\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0002996970259613167.\n",
      "Epoch 207/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1646\n",
      "Epoch 00207: val_loss improved from 0.30542 to 0.30187, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1646 - val_loss: 0.3019\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.00029969403467125554.\n",
      "Epoch 208/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1648\n",
      "Epoch 00208: val_loss improved from 0.30187 to 0.29762, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1648 - val_loss: 0.2976\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.0002996910287022328.\n",
      "Epoch 209/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00209: val_loss improved from 0.29762 to 0.29478, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1897 - val_loss: 0.2948\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.0002996880080545463.\n",
      "Epoch 210/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1775\n",
      "Epoch 00210: val_loss improved from 0.29478 to 0.29174, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1775 - val_loss: 0.2917\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.0002996849727284952.\n",
      "Epoch 211/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1668\n",
      "Epoch 00211: val_loss improved from 0.29174 to 0.28860, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1668 - val_loss: 0.2886\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.0002996819227243801.\n",
      "Epoch 212/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1577\n",
      "Epoch 00212: val_loss improved from 0.28860 to 0.28429, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1577 - val_loss: 0.2843\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.0002996788580425032.\n",
      "Epoch 213/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1776\n",
      "Epoch 00213: val_loss improved from 0.28429 to 0.27799, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1776 - val_loss: 0.2780\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.00029967577868316803.\n",
      "Epoch 214/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1767\n",
      "Epoch 00214: val_loss improved from 0.27799 to 0.27373, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1767 - val_loss: 0.2737\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.00029967268464667964.\n",
      "Epoch 215/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00215: val_loss improved from 0.27373 to 0.27085, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1760 - val_loss: 0.2708\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.00029966957593334444.\n",
      "Epoch 216/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00216: val_loss improved from 0.27085 to 0.26877, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1894 - val_loss: 0.2688\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.0002996664525434704.\n",
      "Epoch 217/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1588\n",
      "Epoch 00217: val_loss did not improve from 0.26877\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1588 - val_loss: 0.2688\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.00029966331447736687.\n",
      "Epoch 218/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1808\n",
      "Epoch 00218: val_loss improved from 0.26877 to 0.26750, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1808 - val_loss: 0.2675\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.0002996601617353447.\n",
      "Epoch 219/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1652\n",
      "Epoch 00219: val_loss improved from 0.26750 to 0.26433, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1652 - val_loss: 0.2643\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.0002996569943177162.\n",
      "Epoch 220/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1616\n",
      "Epoch 00220: val_loss improved from 0.26433 to 0.26043, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1616 - val_loss: 0.2604\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 0.00029965381222479506.\n",
      "Epoch 221/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1603\n",
      "Epoch 00221: val_loss improved from 0.26043 to 0.25773, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1603 - val_loss: 0.2577\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 0.0002996506154568965.\n",
      "Epoch 222/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1750\n",
      "Epoch 00222: val_loss improved from 0.25773 to 0.25496, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1750 - val_loss: 0.2550\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 0.0002996474040143372.\n",
      "Epoch 223/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1570\n",
      "Epoch 00223: val_loss improved from 0.25496 to 0.25077, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1570 - val_loss: 0.2508\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 0.0002996441778974351.\n",
      "Epoch 224/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1615\n",
      "Epoch 00224: val_loss improved from 0.25077 to 0.24770, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1615 - val_loss: 0.2477\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 0.00029964093710650996.\n",
      "Epoch 225/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1725\n",
      "Epoch 00225: val_loss improved from 0.24770 to 0.24555, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1725 - val_loss: 0.2456\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 0.00029963768164188275.\n",
      "Epoch 226/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1824\n",
      "Epoch 00226: val_loss improved from 0.24555 to 0.24121, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1824 - val_loss: 0.2412\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 0.00029963441150387585.\n",
      "Epoch 227/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1523\n",
      "Epoch 00227: val_loss improved from 0.24121 to 0.23394, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1523 - val_loss: 0.2339\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 0.0002996311266928132.\n",
      "Epoch 228/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1781\n",
      "Epoch 00228: val_loss improved from 0.23394 to 0.23039, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1781 - val_loss: 0.2304\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 0.0002996278272090202.\n",
      "Epoch 229/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00229: val_loss improved from 0.23039 to 0.22882, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1636 - val_loss: 0.2288\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 0.00029962451305282366.\n",
      "Epoch 230/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00230: val_loss improved from 0.22882 to 0.22603, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1621 - val_loss: 0.2260\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 0.00029962118422455187.\n",
      "Epoch 231/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1609\n",
      "Epoch 00231: val_loss improved from 0.22603 to 0.22325, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1609 - val_loss: 0.2233\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 0.0002996178407245345.\n",
      "Epoch 232/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1635\n",
      "Epoch 00232: val_loss improved from 0.22325 to 0.22178, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1635 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 0.00029961448255310275.\n",
      "Epoch 233/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1614\n",
      "Epoch 00233: val_loss improved from 0.22178 to 0.22027, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1614 - val_loss: 0.2203\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 0.00029961110971058933.\n",
      "Epoch 234/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1596\n",
      "Epoch 00234: val_loss improved from 0.22027 to 0.21920, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1596 - val_loss: 0.2192\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 0.00029960772219732816.\n",
      "Epoch 235/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1671\n",
      "Epoch 00235: val_loss improved from 0.21920 to 0.21786, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1671 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 0.000299604320013655.\n",
      "Epoch 236/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1578\n",
      "Epoch 00236: val_loss improved from 0.21786 to 0.21598, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1578 - val_loss: 0.2160\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 0.0002996009031599067.\n",
      "Epoch 237/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1689\n",
      "Epoch 00237: val_loss improved from 0.21598 to 0.21314, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1689 - val_loss: 0.2131\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 0.00029959747163642173.\n",
      "Epoch 238/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1626\n",
      "Epoch 00238: val_loss improved from 0.21314 to 0.21051, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1626 - val_loss: 0.2105\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 0.00029959402544354005.\n",
      "Epoch 239/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1466\n",
      "Epoch 00239: val_loss improved from 0.21051 to 0.20626, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1466 - val_loss: 0.2063\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 0.00029959056458160295.\n",
      "Epoch 240/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1368\n",
      "Epoch 00240: val_loss improved from 0.20626 to 0.20101, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1368 - val_loss: 0.2010\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 0.00029958708905095317.\n",
      "Epoch 241/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1700\n",
      "Epoch 00241: val_loss improved from 0.20101 to 0.19602, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1700 - val_loss: 0.1960\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 0.0002995835988519352.\n",
      "Epoch 242/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1390\n",
      "Epoch 00242: val_loss improved from 0.19602 to 0.19214, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1390 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 0.0002995800939848945.\n",
      "Epoch 243/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1523\n",
      "Epoch 00243: val_loss improved from 0.19214 to 0.18850, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1523 - val_loss: 0.1885\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 0.0002995765744501783.\n",
      "Epoch 244/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1439\n",
      "Epoch 00244: val_loss improved from 0.18850 to 0.18524, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1439 - val_loss: 0.1852\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 0.0002995730402481353.\n",
      "Epoch 245/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1507\n",
      "Epoch 00245: val_loss improved from 0.18524 to 0.17975, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1507 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 0.0002995694913791156.\n",
      "Epoch 246/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1565\n",
      "Epoch 00246: val_loss improved from 0.17975 to 0.17387, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1565 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 0.0002995659278434706.\n",
      "Epoch 247/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1578\n",
      "Epoch 00247: val_loss improved from 0.17387 to 0.16886, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1578 - val_loss: 0.1689\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 0.0002995623496415533.\n",
      "Epoch 248/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1615\n",
      "Epoch 00248: val_loss improved from 0.16886 to 0.16440, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1615 - val_loss: 0.1644\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 0.0002995587567737182.\n",
      "Epoch 249/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1376\n",
      "Epoch 00249: val_loss improved from 0.16440 to 0.16199, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1376 - val_loss: 0.1620\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 0.0002995551492403211.\n",
      "Epoch 250/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1555\n",
      "Epoch 00250: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1555 - val_loss: 0.1639\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 0.00029955152704171935.\n",
      "Epoch 251/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1440\n",
      "Epoch 00251: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.1440 - val_loss: 0.1681\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 0.00029954789017827186.\n",
      "Epoch 252/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1697\n",
      "Epoch 00252: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1697 - val_loss: 0.1690\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 0.0002995442386503387.\n",
      "Epoch 253/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1442\n",
      "Epoch 00253: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1442 - val_loss: 0.1681\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 0.0002995405724582817.\n",
      "Epoch 254/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1829\n",
      "Epoch 00254: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1829 - val_loss: 0.1695\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 0.00029953689160246383.\n",
      "Epoch 255/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1447\n",
      "Epoch 00255: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1447 - val_loss: 0.1703\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 0.00029953319608324984.\n",
      "Epoch 256/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00256: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1286 - val_loss: 0.1695\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 0.0002995294859010058.\n",
      "Epoch 257/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1445\n",
      "Epoch 00257: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1445 - val_loss: 0.1671\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 0.0002995257610560991.\n",
      "Epoch 258/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1567\n",
      "Epoch 00258: val_loss did not improve from 0.16199\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1567 - val_loss: 0.1640\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 0.00029952202154889866.\n",
      "Epoch 259/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1547\n",
      "Epoch 00259: val_loss improved from 0.16199 to 0.16067, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1547 - val_loss: 0.1607\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 0.0002995182673797751.\n",
      "Epoch 260/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1481\n",
      "Epoch 00260: val_loss improved from 0.16067 to 0.15794, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1481 - val_loss: 0.1579\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 0.0002995144985491001.\n",
      "Epoch 261/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1315\n",
      "Epoch 00261: val_loss improved from 0.15794 to 0.15682, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1315 - val_loss: 0.1568\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 0.000299510715057247.\n",
      "Epoch 262/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1451\n",
      "Epoch 00262: val_loss improved from 0.15682 to 0.15522, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1451 - val_loss: 0.1552\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 0.0002995069169045906.\n",
      "Epoch 263/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00263: val_loss improved from 0.15522 to 0.15280, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1359 - val_loss: 0.1528\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 0.00029950310409150707.\n",
      "Epoch 264/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1522\n",
      "Epoch 00264: val_loss improved from 0.15280 to 0.15104, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1522 - val_loss: 0.1510\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 0.0002994992766183741.\n",
      "Epoch 265/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00265: val_loss improved from 0.15104 to 0.14938, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1291 - val_loss: 0.1494\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 0.00029949543448557083.\n",
      "Epoch 266/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1494\n",
      "Epoch 00266: val_loss improved from 0.14938 to 0.14814, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1494 - val_loss: 0.1481\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 0.00029949157769347784.\n",
      "Epoch 267/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00267: val_loss improved from 0.14814 to 0.14768, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1289 - val_loss: 0.1477\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 0.0002994877062424771.\n",
      "Epoch 268/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1587\n",
      "Epoch 00268: val_loss improved from 0.14768 to 0.14647, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1587 - val_loss: 0.1465\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 0.00029948382013295213.\n",
      "Epoch 269/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1725\n",
      "Epoch 00269: val_loss improved from 0.14647 to 0.14545, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1725 - val_loss: 0.1455\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 0.00029947991936528785.\n",
      "Epoch 270/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1689\n",
      "Epoch 00270: val_loss improved from 0.14545 to 0.14474, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1689 - val_loss: 0.1447\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 0.0002994760039398707.\n",
      "Epoch 271/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1430\n",
      "Epoch 00271: val_loss did not improve from 0.14474\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1430 - val_loss: 0.1456\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 0.00029947207385708837.\n",
      "Epoch 272/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1498\n",
      "Epoch 00272: val_loss did not improve from 0.14474\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1498 - val_loss: 0.1472\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 0.00029946812911733027.\n",
      "Epoch 273/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00273: val_loss did not improve from 0.14474\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1590 - val_loss: 0.1480\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 0.00029946416972098704.\n",
      "Epoch 274/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1434\n",
      "Epoch 00274: val_loss did not improve from 0.14474\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1434 - val_loss: 0.1485\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 0.00029946019566845093.\n",
      "Epoch 275/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00275: val_loss did not improve from 0.14474\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1282 - val_loss: 0.1476\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 0.0002994562069601156.\n",
      "Epoch 276/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00276: val_loss did not improve from 0.14474\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1352 - val_loss: 0.1458\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 0.00029945220359637605.\n",
      "Epoch 277/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00277: val_loss improved from 0.14474 to 0.14317, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1272 - val_loss: 0.1432\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 0.0002994481855776289.\n",
      "Epoch 278/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00278: val_loss improved from 0.14317 to 0.13840, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1467 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 0.00029944415290427214.\n",
      "Epoch 279/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1544\n",
      "Epoch 00279: val_loss improved from 0.13840 to 0.13397, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1544 - val_loss: 0.1340\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 0.0002994401055767052.\n",
      "Epoch 280/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1508\n",
      "Epoch 00280: val_loss improved from 0.13397 to 0.13032, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1508 - val_loss: 0.1303\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 0.00029943604359532894.\n",
      "Epoch 281/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1382\n",
      "Epoch 00281: val_loss improved from 0.13032 to 0.12717, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1382 - val_loss: 0.1272\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 0.0002994319669605458.\n",
      "Epoch 282/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00282: val_loss improved from 0.12717 to 0.12601, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1232 - val_loss: 0.1260\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 0.00029942787567275946.\n",
      "Epoch 283/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00283: val_loss improved from 0.12601 to 0.12601, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1488 - val_loss: 0.1260\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 0.0002994237697323752.\n",
      "Epoch 284/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1449\n",
      "Epoch 00284: val_loss did not improve from 0.12601\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1449 - val_loss: 0.1263\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 0.00029941964913979984.\n",
      "Epoch 285/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1535\n",
      "Epoch 00285: val_loss improved from 0.12601 to 0.12504, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1535 - val_loss: 0.1250\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 0.0002994155138954415.\n",
      "Epoch 286/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1384\n",
      "Epoch 00286: val_loss improved from 0.12504 to 0.12398, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1384 - val_loss: 0.1240\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 0.00029941136399970963.\n",
      "Epoch 287/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00287: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1289 - val_loss: 0.1256\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 0.0002994071994530154.\n",
      "Epoch 288/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00288: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 1s 173ms/step - loss: 0.1150 - val_loss: 0.1267\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 0.00029940302025577135.\n",
      "Epoch 289/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00289: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1281 - val_loss: 0.1272\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 0.0002993988264083914.\n",
      "Epoch 290/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00290: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1298 - val_loss: 0.1283\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 0.00029939461791129096.\n",
      "Epoch 291/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1385\n",
      "Epoch 00291: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1385 - val_loss: 0.1295\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 0.00029939039476488687.\n",
      "Epoch 292/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00292: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1268 - val_loss: 0.1301\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 0.0002993861569695975.\n",
      "Epoch 293/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1459\n",
      "Epoch 00293: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1459 - val_loss: 0.1303\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 0.00029938190452584256.\n",
      "Epoch 294/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00294: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1252 - val_loss: 0.1310\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 0.0002993776374340433.\n",
      "Epoch 295/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00295: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1169 - val_loss: 0.1324\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 0.0002993733556946223.\n",
      "Epoch 296/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00296: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1326 - val_loss: 0.1331\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 0.00029936905930800383.\n",
      "Epoch 297/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1313\n",
      "Epoch 00297: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1313 - val_loss: 0.1334\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 0.0002993647482746134.\n",
      "Epoch 298/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00298: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1488 - val_loss: 0.1329\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 0.0002993604225948779.\n",
      "Epoch 299/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1368\n",
      "Epoch 00299: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1368 - val_loss: 0.1311\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 0.0002993560822692259.\n",
      "Epoch 300/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1185\n",
      "Epoch 00300: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1185 - val_loss: 0.1298\n",
      "\n",
      "Epoch 00301: LearningRateScheduler reducing learning rate to 0.00029935172729808737.\n",
      "Epoch 301/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00301: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1206 - val_loss: 0.1305\n",
      "\n",
      "Epoch 00302: LearningRateScheduler reducing learning rate to 0.00029934735768189364.\n",
      "Epoch 302/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00302: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1277 - val_loss: 0.1315\n",
      "\n",
      "Epoch 00303: LearningRateScheduler reducing learning rate to 0.0002993429734210775.\n",
      "Epoch 303/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00303: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1251 - val_loss: 0.1312\n",
      "\n",
      "Epoch 00304: LearningRateScheduler reducing learning rate to 0.0002993385745160732.\n",
      "Epoch 304/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00304: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1352 - val_loss: 0.1307\n",
      "\n",
      "Epoch 00305: LearningRateScheduler reducing learning rate to 0.0002993341609673165.\n",
      "Epoch 305/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1448\n",
      "Epoch 00305: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.1448 - val_loss: 0.1298\n",
      "\n",
      "Epoch 00306: LearningRateScheduler reducing learning rate to 0.0002993297327752446.\n",
      "Epoch 306/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1200\n",
      "Epoch 00306: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1200 - val_loss: 0.1290\n",
      "\n",
      "Epoch 00307: LearningRateScheduler reducing learning rate to 0.00029932528994029614.\n",
      "Epoch 307/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00307: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1497 - val_loss: 0.1282\n",
      "\n",
      "Epoch 00308: LearningRateScheduler reducing learning rate to 0.0002993208324629111.\n",
      "Epoch 308/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00308: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1149 - val_loss: 0.1273\n",
      "\n",
      "Epoch 00309: LearningRateScheduler reducing learning rate to 0.00029931636034353105.\n",
      "Epoch 309/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00309: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1362 - val_loss: 0.1271\n",
      "\n",
      "Epoch 00310: LearningRateScheduler reducing learning rate to 0.0002993118735825989.\n",
      "Epoch 310/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1353\n",
      "Epoch 00310: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1353 - val_loss: 0.1256\n",
      "\n",
      "Epoch 00311: LearningRateScheduler reducing learning rate to 0.0002993073721805593.\n",
      "Epoch 311/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1439\n",
      "Epoch 00311: val_loss did not improve from 0.12398\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1439 - val_loss: 0.1242\n",
      "\n",
      "Epoch 00312: LearningRateScheduler reducing learning rate to 0.00029930285613785786.\n",
      "Epoch 312/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1418\n",
      "Epoch 00312: val_loss improved from 0.12398 to 0.12367, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 4s 1s/step - loss: 0.1418 - val_loss: 0.1237\n",
      "\n",
      "Epoch 00313: LearningRateScheduler reducing learning rate to 0.000299298325454942.\n",
      "Epoch 313/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1612\n",
      "Epoch 00313: val_loss improved from 0.12367 to 0.12167, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1612 - val_loss: 0.1217\n",
      "\n",
      "Epoch 00314: LearningRateScheduler reducing learning rate to 0.00029929378013226056.\n",
      "Epoch 314/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1459\n",
      "Epoch 00314: val_loss improved from 0.12167 to 0.12039, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1459 - val_loss: 0.1204\n",
      "\n",
      "Epoch 00315: LearningRateScheduler reducing learning rate to 0.00029928922017026364.\n",
      "Epoch 315/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00315: val_loss improved from 0.12039 to 0.12024, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1230 - val_loss: 0.1202\n",
      "\n",
      "Epoch 00316: LearningRateScheduler reducing learning rate to 0.00029928464556940303.\n",
      "Epoch 316/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00316: val_loss did not improve from 0.12024\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1296 - val_loss: 0.1214\n",
      "\n",
      "Epoch 00317: LearningRateScheduler reducing learning rate to 0.00029928005633013177.\n",
      "Epoch 317/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00317: val_loss did not improve from 0.12024\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1118 - val_loss: 0.1224\n",
      "\n",
      "Epoch 00318: LearningRateScheduler reducing learning rate to 0.00029927545245290457.\n",
      "Epoch 318/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00318: val_loss did not improve from 0.12024\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1114 - val_loss: 0.1217\n",
      "\n",
      "Epoch 00319: LearningRateScheduler reducing learning rate to 0.0002992708339381772.\n",
      "Epoch 319/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00319: val_loss improved from 0.12024 to 0.11965, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1143 - val_loss: 0.1196\n",
      "\n",
      "Epoch 00320: LearningRateScheduler reducing learning rate to 0.00029926620078640743.\n",
      "Epoch 320/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1220\n",
      "Epoch 00320: val_loss improved from 0.11965 to 0.11690, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1220 - val_loss: 0.1169\n",
      "\n",
      "Epoch 00321: LearningRateScheduler reducing learning rate to 0.00029926155299805397.\n",
      "Epoch 321/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00321: val_loss improved from 0.11690 to 0.11350, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1067 - val_loss: 0.1135\n",
      "\n",
      "Epoch 00322: LearningRateScheduler reducing learning rate to 0.0002992568905735773.\n",
      "Epoch 322/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00322: val_loss improved from 0.11350 to 0.11093, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1128 - val_loss: 0.1109\n",
      "\n",
      "Epoch 00323: LearningRateScheduler reducing learning rate to 0.0002992522135134392.\n",
      "Epoch 323/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00323: val_loss improved from 0.11093 to 0.10960, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1291 - val_loss: 0.1096\n",
      "\n",
      "Epoch 00324: LearningRateScheduler reducing learning rate to 0.00029924752181810293.\n",
      "Epoch 324/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00324: val_loss improved from 0.10960 to 0.10862, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1079 - val_loss: 0.1086\n",
      "\n",
      "Epoch 00325: LearningRateScheduler reducing learning rate to 0.0002992428154880333.\n",
      "Epoch 325/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00325: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1231 - val_loss: 0.1101\n",
      "\n",
      "Epoch 00326: LearningRateScheduler reducing learning rate to 0.0002992380945236964.\n",
      "Epoch 326/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00326: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1036 - val_loss: 0.1106\n",
      "\n",
      "Epoch 00327: LearningRateScheduler reducing learning rate to 0.0002992333589255599.\n",
      "Epoch 327/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00327: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1263 - val_loss: 0.1118\n",
      "\n",
      "Epoch 00328: LearningRateScheduler reducing learning rate to 0.0002992286086940928.\n",
      "Epoch 328/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1422\n",
      "Epoch 00328: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1422 - val_loss: 0.1136\n",
      "\n",
      "Epoch 00329: LearningRateScheduler reducing learning rate to 0.0002992238438297657.\n",
      "Epoch 329/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00329: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1104 - val_loss: 0.1144\n",
      "\n",
      "Epoch 00330: LearningRateScheduler reducing learning rate to 0.00029921906433305053.\n",
      "Epoch 330/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00330: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1283 - val_loss: 0.1169\n",
      "\n",
      "Epoch 00331: LearningRateScheduler reducing learning rate to 0.0002992142702044207.\n",
      "Epoch 331/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1220\n",
      "Epoch 00331: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1220 - val_loss: 0.1202\n",
      "\n",
      "Epoch 00332: LearningRateScheduler reducing learning rate to 0.00029920946144435114.\n",
      "Epoch 332/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00332: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1268 - val_loss: 0.1218\n",
      "\n",
      "Epoch 00333: LearningRateScheduler reducing learning rate to 0.0002992046380533181.\n",
      "Epoch 333/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00333: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1352 - val_loss: 0.1227\n",
      "\n",
      "Epoch 00334: LearningRateScheduler reducing learning rate to 0.00029919980003179943.\n",
      "Epoch 334/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00334: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 0.1203 - val_loss: 0.1223\n",
      "\n",
      "Epoch 00335: LearningRateScheduler reducing learning rate to 0.00029919494738027424.\n",
      "Epoch 335/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00335: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1173 - val_loss: 0.1214\n",
      "\n",
      "Epoch 00336: LearningRateScheduler reducing learning rate to 0.0002991900800992232.\n",
      "Epoch 336/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00336: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1288 - val_loss: 0.1196\n",
      "\n",
      "Epoch 00337: LearningRateScheduler reducing learning rate to 0.00029918519818912856.\n",
      "Epoch 337/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00337: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.1188 - val_loss: 0.1172\n",
      "\n",
      "Epoch 00338: LearningRateScheduler reducing learning rate to 0.00029918030165047377.\n",
      "Epoch 338/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00338: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.1100 - val_loss: 0.1147\n",
      "\n",
      "Epoch 00339: LearningRateScheduler reducing learning rate to 0.0002991753904837439.\n",
      "Epoch 339/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1187\n",
      "Epoch 00339: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.1187 - val_loss: 0.1128\n",
      "\n",
      "Epoch 00340: LearningRateScheduler reducing learning rate to 0.00029917046468942536.\n",
      "Epoch 340/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1204\n",
      "Epoch 00340: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1204 - val_loss: 0.1116\n",
      "\n",
      "Epoch 00341: LearningRateScheduler reducing learning rate to 0.00029916552426800607.\n",
      "Epoch 341/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00341: val_loss did not improve from 0.10862\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1135 - val_loss: 0.1102\n",
      "\n",
      "Epoch 00342: LearningRateScheduler reducing learning rate to 0.00029916056921997544.\n",
      "Epoch 342/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00342: val_loss improved from 0.10862 to 0.10795, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1007 - val_loss: 0.1080\n",
      "\n",
      "Epoch 00343: LearningRateScheduler reducing learning rate to 0.00029915559954582423.\n",
      "Epoch 343/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00343: val_loss improved from 0.10795 to 0.10545, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1280 - val_loss: 0.1055\n",
      "\n",
      "Epoch 00344: LearningRateScheduler reducing learning rate to 0.00029915061524604474.\n",
      "Epoch 344/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1176\n",
      "Epoch 00344: val_loss improved from 0.10545 to 0.10335, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1176 - val_loss: 0.1034\n",
      "\n",
      "Epoch 00345: LearningRateScheduler reducing learning rate to 0.00029914561632113064.\n",
      "Epoch 345/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00345: val_loss improved from 0.10335 to 0.10061, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1230 - val_loss: 0.1006\n",
      "\n",
      "Epoch 00346: LearningRateScheduler reducing learning rate to 0.00029914060277157705.\n",
      "Epoch 346/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00346: val_loss improved from 0.10061 to 0.09850, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1158 - val_loss: 0.0985\n",
      "\n",
      "Epoch 00347: LearningRateScheduler reducing learning rate to 0.0002991355745978807.\n",
      "Epoch 347/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1371\n",
      "Epoch 00347: val_loss improved from 0.09850 to 0.09770, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1371 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00348: LearningRateScheduler reducing learning rate to 0.0002991305318005395.\n",
      "Epoch 348/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00348: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1064 - val_loss: 0.0982\n",
      "\n",
      "Epoch 00349: LearningRateScheduler reducing learning rate to 0.00029912547438005305.\n",
      "Epoch 349/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00349: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1222 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00350: LearningRateScheduler reducing learning rate to 0.00029912040233692224.\n",
      "Epoch 350/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1214\n",
      "Epoch 00350: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1214 - val_loss: 0.0998\n",
      "\n",
      "Epoch 00351: LearningRateScheduler reducing learning rate to 0.0002991153156716495.\n",
      "Epoch 351/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00351: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1064 - val_loss: 0.1011\n",
      "\n",
      "Epoch 00352: LearningRateScheduler reducing learning rate to 0.00029911021438473864.\n",
      "Epoch 352/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00352: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1228 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00353: LearningRateScheduler reducing learning rate to 0.000299105098476695.\n",
      "Epoch 353/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00353: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0996 - val_loss: 0.1047\n",
      "\n",
      "Epoch 00354: LearningRateScheduler reducing learning rate to 0.00029909996794802527.\n",
      "Epoch 354/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00354: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1202 - val_loss: 0.1034\n",
      "\n",
      "Epoch 00355: LearningRateScheduler reducing learning rate to 0.0002990948227992377.\n",
      "Epoch 355/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 00355: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.1191 - val_loss: 0.1010\n",
      "\n",
      "Epoch 00356: LearningRateScheduler reducing learning rate to 0.0002990896630308419.\n",
      "Epoch 356/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00356: val_loss did not improve from 0.09770\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.1182 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00357: LearningRateScheduler reducing learning rate to 0.000299084488643349.\n",
      "Epoch 357/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 00357: val_loss improved from 0.09770 to 0.09522, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1055 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00358: LearningRateScheduler reducing learning rate to 0.00029907929963727143.\n",
      "Epoch 358/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00358: val_loss improved from 0.09522 to 0.09372, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1095 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00359: LearningRateScheduler reducing learning rate to 0.00029907409601312326.\n",
      "Epoch 359/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00359: val_loss improved from 0.09372 to 0.09290, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1210 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00360: LearningRateScheduler reducing learning rate to 0.00029906887777141994.\n",
      "Epoch 360/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00360: val_loss improved from 0.09290 to 0.09261, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1139 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00361: LearningRateScheduler reducing learning rate to 0.00029906364491267824.\n",
      "Epoch 361/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00361: val_loss did not improve from 0.09261\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.1291 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00362: LearningRateScheduler reducing learning rate to 0.0002990583974374166.\n",
      "Epoch 362/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00362: val_loss did not improve from 0.09261\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1170 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00363: LearningRateScheduler reducing learning rate to 0.00029905313534615466.\n",
      "Epoch 363/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00363: val_loss did not improve from 0.09261\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1319 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00364: LearningRateScheduler reducing learning rate to 0.00029904785863941384.\n",
      "Epoch 364/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1318\n",
      "Epoch 00364: val_loss did not improve from 0.09261\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1318 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00365: LearningRateScheduler reducing learning rate to 0.00029904256731771664.\n",
      "Epoch 365/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00365: val_loss did not improve from 0.09261\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1153 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00366: LearningRateScheduler reducing learning rate to 0.00029903726138158726.\n",
      "Epoch 366/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 00366: val_loss did not improve from 0.09261\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1303 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00367: LearningRateScheduler reducing learning rate to 0.00029903194083155123.\n",
      "Epoch 367/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00367: val_loss did not improve from 0.09261\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1107 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00368: LearningRateScheduler reducing learning rate to 0.0002990266056681356.\n",
      "Epoch 368/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 00368: val_loss improved from 0.09261 to 0.09246, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1056 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00369: LearningRateScheduler reducing learning rate to 0.0002990212558918688.\n",
      "Epoch 369/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 00369: val_loss improved from 0.09246 to 0.09162, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1063 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00370: LearningRateScheduler reducing learning rate to 0.00029901589150328067.\n",
      "Epoch 370/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00370: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1236 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00371: LearningRateScheduler reducing learning rate to 0.00029901051250290275.\n",
      "Epoch 371/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00371: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1106 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00372: LearningRateScheduler reducing learning rate to 0.00029900511889126763.\n",
      "Epoch 372/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00372: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1021 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00373: LearningRateScheduler reducing learning rate to 0.0002989997106689097.\n",
      "Epoch 373/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1179\n",
      "Epoch 00373: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.1179 - val_loss: 0.1001\n",
      "\n",
      "Epoch 00374: LearningRateScheduler reducing learning rate to 0.0002989942878363647.\n",
      "Epoch 374/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1215\n",
      "Epoch 00374: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1215 - val_loss: 0.1000\n",
      "\n",
      "Epoch 00375: LearningRateScheduler reducing learning rate to 0.00029898885039416956.\n",
      "Epoch 375/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00375: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1175 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00376: LearningRateScheduler reducing learning rate to 0.0002989833983428631.\n",
      "Epoch 376/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00376: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0962 - val_loss: 0.0998\n",
      "\n",
      "Epoch 00377: LearningRateScheduler reducing learning rate to 0.00029897793168298523.\n",
      "Epoch 377/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 00377: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0891 - val_loss: 0.1003\n",
      "\n",
      "Epoch 00378: LearningRateScheduler reducing learning rate to 0.00029897245041507746.\n",
      "Epoch 378/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00378: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1136 - val_loss: 0.1007\n",
      "\n",
      "Epoch 00379: LearningRateScheduler reducing learning rate to 0.0002989669545396827.\n",
      "Epoch 379/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00379: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1166 - val_loss: 0.1014\n",
      "\n",
      "Epoch 00380: LearningRateScheduler reducing learning rate to 0.0002989614440573454.\n",
      "Epoch 380/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00380: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1280 - val_loss: 0.1018\n",
      "\n",
      "Epoch 00381: LearningRateScheduler reducing learning rate to 0.00029895591896861133.\n",
      "Epoch 381/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1212\n",
      "Epoch 00381: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1212 - val_loss: 0.1025\n",
      "\n",
      "Epoch 00382: LearningRateScheduler reducing learning rate to 0.0002989503792740278.\n",
      "Epoch 382/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00382: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1018 - val_loss: 0.1026\n",
      "\n",
      "Epoch 00383: LearningRateScheduler reducing learning rate to 0.00029894482497414346.\n",
      "Epoch 383/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00383: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1069 - val_loss: 0.1030\n",
      "\n",
      "Epoch 00384: LearningRateScheduler reducing learning rate to 0.0002989392560695085.\n",
      "Epoch 384/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00384: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0992 - val_loss: 0.1034\n",
      "\n",
      "Epoch 00385: LearningRateScheduler reducing learning rate to 0.0002989336725606746.\n",
      "Epoch 385/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0893\n",
      "Epoch 00385: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0893 - val_loss: 0.1035\n",
      "\n",
      "Epoch 00386: LearningRateScheduler reducing learning rate to 0.0002989280744481948.\n",
      "Epoch 386/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00386: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1155 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00387: LearningRateScheduler reducing learning rate to 0.00029892246173262357.\n",
      "Epoch 387/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00387: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1297 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00388: LearningRateScheduler reducing learning rate to 0.00029891683441451685.\n",
      "Epoch 388/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00388: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1165 - val_loss: 0.1032\n",
      "\n",
      "Epoch 00389: LearningRateScheduler reducing learning rate to 0.0002989111924944321.\n",
      "Epoch 389/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00389: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1010 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00390: LearningRateScheduler reducing learning rate to 0.00029890553597292807.\n",
      "Epoch 390/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00390: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1080 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00391: LearningRateScheduler reducing learning rate to 0.00029889986485056517.\n",
      "Epoch 391/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1134\n",
      "Epoch 00391: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1134 - val_loss: 0.1024\n",
      "\n",
      "Epoch 00392: LearningRateScheduler reducing learning rate to 0.000298894179127905.\n",
      "Epoch 392/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 00392: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1033 - val_loss: 0.1012\n",
      "\n",
      "Epoch 00393: LearningRateScheduler reducing learning rate to 0.0002988884788055109.\n",
      "Epoch 393/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 00393: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1126 - val_loss: 0.1003\n",
      "\n",
      "Epoch 00394: LearningRateScheduler reducing learning rate to 0.0002988827638839474.\n",
      "Epoch 394/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00394: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0999 - val_loss: 0.0994\n",
      "\n",
      "Epoch 00395: LearningRateScheduler reducing learning rate to 0.00029887703436378056.\n",
      "Epoch 395/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 00395: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1061 - val_loss: 0.1004\n",
      "\n",
      "Epoch 00396: LearningRateScheduler reducing learning rate to 0.00029887129024557793.\n",
      "Epoch 396/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00396: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1227 - val_loss: 0.1020\n",
      "\n",
      "Epoch 00397: LearningRateScheduler reducing learning rate to 0.00029886553152990857.\n",
      "Epoch 397/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00397: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1002 - val_loss: 0.1031\n",
      "\n",
      "Epoch 00398: LearningRateScheduler reducing learning rate to 0.00029885975821734276.\n",
      "Epoch 398/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00398: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1217 - val_loss: 0.1040\n",
      "\n",
      "Epoch 00399: LearningRateScheduler reducing learning rate to 0.0002988539703084524.\n",
      "Epoch 399/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00399: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1067 - val_loss: 0.1051\n",
      "\n",
      "Epoch 00400: LearningRateScheduler reducing learning rate to 0.0002988481678038108.\n",
      "Epoch 400/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00400: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1148 - val_loss: 0.1062\n",
      "\n",
      "Epoch 00401: LearningRateScheduler reducing learning rate to 0.00029884235070399277.\n",
      "Epoch 401/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00401: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1091 - val_loss: 0.1061\n",
      "\n",
      "Epoch 00402: LearningRateScheduler reducing learning rate to 0.00029883651900957444.\n",
      "Epoch 402/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00402: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1279 - val_loss: 0.1049\n",
      "\n",
      "Epoch 00403: LearningRateScheduler reducing learning rate to 0.00029883067272113345.\n",
      "Epoch 403/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 00403: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0941 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00404: LearningRateScheduler reducing learning rate to 0.0002988248118392489.\n",
      "Epoch 404/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00404: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1252 - val_loss: 0.1023\n",
      "\n",
      "Epoch 00405: LearningRateScheduler reducing learning rate to 0.0002988189363645013.\n",
      "Epoch 405/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0947\n",
      "Epoch 00405: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0947 - val_loss: 0.1017\n",
      "\n",
      "Epoch 00406: LearningRateScheduler reducing learning rate to 0.00029881304629747273.\n",
      "Epoch 406/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00406: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1213 - val_loss: 0.1010\n",
      "\n",
      "Epoch 00407: LearningRateScheduler reducing learning rate to 0.0002988071416387465.\n",
      "Epoch 407/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00407: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1236 - val_loss: 0.1009\n",
      "\n",
      "Epoch 00408: LearningRateScheduler reducing learning rate to 0.00029880122238890754.\n",
      "Epoch 408/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00408: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1028 - val_loss: 0.1005\n",
      "\n",
      "Epoch 00409: LearningRateScheduler reducing learning rate to 0.00029879528854854213.\n",
      "Epoch 409/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 00409: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1074 - val_loss: 0.1011\n",
      "\n",
      "Epoch 00410: LearningRateScheduler reducing learning rate to 0.0002987893401182381.\n",
      "Epoch 410/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 00410: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.1061 - val_loss: 0.1011\n",
      "\n",
      "Epoch 00411: LearningRateScheduler reducing learning rate to 0.00029878337709858457.\n",
      "Epoch 411/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00411: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1257 - val_loss: 0.1009\n",
      "\n",
      "Epoch 00412: LearningRateScheduler reducing learning rate to 0.00029877739949017224.\n",
      "Epoch 412/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00412: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1290 - val_loss: 0.1005\n",
      "\n",
      "Epoch 00413: LearningRateScheduler reducing learning rate to 0.0002987714072935932.\n",
      "Epoch 413/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0896\n",
      "Epoch 00413: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0896 - val_loss: 0.1001\n",
      "\n",
      "Epoch 00414: LearningRateScheduler reducing learning rate to 0.00029876540050944103.\n",
      "Epoch 414/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00414: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1143 - val_loss: 0.0995\n",
      "\n",
      "Epoch 00415: LearningRateScheduler reducing learning rate to 0.00029875937913831054.\n",
      "Epoch 415/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00415: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1184 - val_loss: 0.0985\n",
      "\n",
      "Epoch 00416: LearningRateScheduler reducing learning rate to 0.00029875334318079836.\n",
      "Epoch 416/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 00416: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0874 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00417: LearningRateScheduler reducing learning rate to 0.00029874729263750237.\n",
      "Epoch 417/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00417: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1279 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00418: LearningRateScheduler reducing learning rate to 0.0002987412275090217.\n",
      "Epoch 418/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00418: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1238 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00419: LearningRateScheduler reducing learning rate to 0.0002987351477959573.\n",
      "Epoch 419/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00419: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1135 - val_loss: 0.0984\n",
      "\n",
      "Epoch 00420: LearningRateScheduler reducing learning rate to 0.0002987290534989113.\n",
      "Epoch 420/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1146\n",
      "Epoch 00420: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1146 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00421: LearningRateScheduler reducing learning rate to 0.0002987229446184874.\n",
      "Epoch 421/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00421: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1089 - val_loss: 0.0972\n",
      "\n",
      "Epoch 00422: LearningRateScheduler reducing learning rate to 0.0002987168211552906.\n",
      "Epoch 422/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00422: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1228 - val_loss: 0.0968\n",
      "\n",
      "Epoch 00423: LearningRateScheduler reducing learning rate to 0.0002987106831099275.\n",
      "Epoch 423/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00423: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0987 - val_loss: 0.0957\n",
      "\n",
      "Epoch 00424: LearningRateScheduler reducing learning rate to 0.00029870453048300616.\n",
      "Epoch 424/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00424: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1047 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00425: LearningRateScheduler reducing learning rate to 0.0002986983632751359.\n",
      "Epoch 425/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00425: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1256 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00426: LearningRateScheduler reducing learning rate to 0.0002986921814869276.\n",
      "Epoch 426/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1368\n",
      "Epoch 00426: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.1368 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00427: LearningRateScheduler reducing learning rate to 0.0002986859851189937.\n",
      "Epoch 427/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00427: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1082 - val_loss: 0.0930\n",
      "\n",
      "Epoch 00428: LearningRateScheduler reducing learning rate to 0.0002986797741719479.\n",
      "Epoch 428/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00428: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.1042 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00429: LearningRateScheduler reducing learning rate to 0.00029867354864640533.\n",
      "Epoch 429/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0814\n",
      "Epoch 00429: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0814 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00430: LearningRateScheduler reducing learning rate to 0.0002986673085429828.\n",
      "Epoch 430/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00430: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1084 - val_loss: 0.0951\n",
      "\n",
      "Epoch 00431: LearningRateScheduler reducing learning rate to 0.0002986610538622982.\n",
      "Epoch 431/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 00431: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1022 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00432: LearningRateScheduler reducing learning rate to 0.0002986547846049713.\n",
      "Epoch 432/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1518\n",
      "Epoch 00432: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1518 - val_loss: 0.0999\n",
      "\n",
      "Epoch 00433: LearningRateScheduler reducing learning rate to 0.0002986485007716229.\n",
      "Epoch 433/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 00433: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0880 - val_loss: 0.1015\n",
      "\n",
      "Epoch 00434: LearningRateScheduler reducing learning rate to 0.00029864220236287554.\n",
      "Epoch 434/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00434: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1255 - val_loss: 0.1026\n",
      "\n",
      "Epoch 00435: LearningRateScheduler reducing learning rate to 0.000298635889379353.\n",
      "Epoch 435/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00435: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1206 - val_loss: 0.1034\n",
      "\n",
      "Epoch 00436: LearningRateScheduler reducing learning rate to 0.00029862956182168067.\n",
      "Epoch 436/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 00436: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0880 - val_loss: 0.1040\n",
      "\n",
      "Epoch 00437: LearningRateScheduler reducing learning rate to 0.0002986232196904853.\n",
      "Epoch 437/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1196\n",
      "Epoch 00437: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1196 - val_loss: 0.1044\n",
      "\n",
      "Epoch 00438: LearningRateScheduler reducing learning rate to 0.0002986168629863951.\n",
      "Epoch 438/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00438: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1203 - val_loss: 0.1036\n",
      "\n",
      "Epoch 00439: LearningRateScheduler reducing learning rate to 0.00029861049171003964.\n",
      "Epoch 439/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00439: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1159 - val_loss: 0.1017\n",
      "\n",
      "Epoch 00440: LearningRateScheduler reducing learning rate to 0.00029860410586205014.\n",
      "Epoch 440/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00440: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.1027 - val_loss: 0.1001\n",
      "\n",
      "Epoch 00441: LearningRateScheduler reducing learning rate to 0.000298597705443059.\n",
      "Epoch 441/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00441: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1096 - val_loss: 0.0987\n",
      "\n",
      "Epoch 00442: LearningRateScheduler reducing learning rate to 0.0002985912904537003.\n",
      "Epoch 442/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1223\n",
      "Epoch 00442: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1223 - val_loss: 0.0980\n",
      "\n",
      "Epoch 00443: LearningRateScheduler reducing learning rate to 0.0002985848608946093.\n",
      "Epoch 443/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00443: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1073 - val_loss: 0.0971\n",
      "\n",
      "Epoch 00444: LearningRateScheduler reducing learning rate to 0.0002985784167664231.\n",
      "Epoch 444/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 00444: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1066 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00445: LearningRateScheduler reducing learning rate to 0.00029857195806977984.\n",
      "Epoch 445/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00445: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1021 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00446: LearningRateScheduler reducing learning rate to 0.0002985654848053193.\n",
      "Epoch 446/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 00446: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1033 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00447: LearningRateScheduler reducing learning rate to 0.0002985589969736827.\n",
      "Epoch 447/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0947\n",
      "Epoch 00447: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0947 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00448: LearningRateScheduler reducing learning rate to 0.00029855249457551265.\n",
      "Epoch 448/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00448: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1069 - val_loss: 0.0919\n",
      "\n",
      "Epoch 00449: LearningRateScheduler reducing learning rate to 0.0002985459776114532.\n",
      "Epoch 449/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00449: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0973 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00450: LearningRateScheduler reducing learning rate to 0.00029853944608214996.\n",
      "Epoch 450/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00450: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0990 - val_loss: 0.0917\n",
      "\n",
      "Epoch 00451: LearningRateScheduler reducing learning rate to 0.0002985328999882498.\n",
      "Epoch 451/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00451: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1275 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00452: LearningRateScheduler reducing learning rate to 0.0002985263393304012.\n",
      "Epoch 452/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 00452: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1044 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00453: LearningRateScheduler reducing learning rate to 0.0002985197641092539.\n",
      "Epoch 453/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00453: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1237 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00454: LearningRateScheduler reducing learning rate to 0.0002985131743254594.\n",
      "Epoch 454/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00454: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1188 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00455: LearningRateScheduler reducing learning rate to 0.00029850656997967016.\n",
      "Epoch 455/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0870\n",
      "Epoch 00455: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0870 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00456: LearningRateScheduler reducing learning rate to 0.0002984999510725405.\n",
      "Epoch 456/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00456: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1119 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00457: LearningRateScheduler reducing learning rate to 0.00029849331760472615.\n",
      "Epoch 457/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1133\n",
      "Epoch 00457: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1133 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00458: LearningRateScheduler reducing learning rate to 0.00029848666957688394.\n",
      "Epoch 458/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00458: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0972 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00459: LearningRateScheduler reducing learning rate to 0.0002984800069896725.\n",
      "Epoch 459/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00459: val_loss did not improve from 0.09162\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1079 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00460: LearningRateScheduler reducing learning rate to 0.0002984733298437518.\n",
      "Epoch 460/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1192\n",
      "Epoch 00460: val_loss improved from 0.09162 to 0.09099, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1192 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00461: LearningRateScheduler reducing learning rate to 0.0002984666381397832.\n",
      "Epoch 461/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00461: val_loss improved from 0.09099 to 0.08923, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1015 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00462: LearningRateScheduler reducing learning rate to 0.00029845993187842945.\n",
      "Epoch 462/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00462: val_loss improved from 0.08923 to 0.08797, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1175 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00463: LearningRateScheduler reducing learning rate to 0.0002984532110603549.\n",
      "Epoch 463/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0853\n",
      "Epoch 00463: val_loss improved from 0.08797 to 0.08713, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0853 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00464: LearningRateScheduler reducing learning rate to 0.0002984464756862253.\n",
      "Epoch 464/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00464: val_loss did not improve from 0.08713\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1006 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00465: LearningRateScheduler reducing learning rate to 0.0002984397257567077.\n",
      "Epoch 465/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00465: val_loss did not improve from 0.08713\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1096 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00466: LearningRateScheduler reducing learning rate to 0.0002984329612724707.\n",
      "Epoch 466/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 00466: val_loss did not improve from 0.08713\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1053 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00467: LearningRateScheduler reducing learning rate to 0.00029842618223418447.\n",
      "Epoch 467/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 00467: val_loss did not improve from 0.08713\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1063 - val_loss: 0.0882\n",
      "\n",
      "Epoch 00468: LearningRateScheduler reducing learning rate to 0.00029841938864252036.\n",
      "Epoch 468/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00468: val_loss improved from 0.08713 to 0.08689, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1282 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00469: LearningRateScheduler reducing learning rate to 0.00029841258049815135.\n",
      "Epoch 469/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00469: val_loss improved from 0.08689 to 0.08596, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1238 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00470: LearningRateScheduler reducing learning rate to 0.0002984057578017518.\n",
      "Epoch 470/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00470: val_loss improved from 0.08596 to 0.08524, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1014 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00471: LearningRateScheduler reducing learning rate to 0.0002983989205539975.\n",
      "Epoch 471/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00471: val_loss did not improve from 0.08524\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1088 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00472: LearningRateScheduler reducing learning rate to 0.00029839206875556567.\n",
      "Epoch 472/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0931\n",
      "Epoch 00472: val_loss did not improve from 0.08524\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0931 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00473: LearningRateScheduler reducing learning rate to 0.000298385202407135.\n",
      "Epoch 473/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00473: val_loss did not improve from 0.08524\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0961 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00474: LearningRateScheduler reducing learning rate to 0.0002983783215093857.\n",
      "Epoch 474/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00474: val_loss did not improve from 0.08524\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.0966 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00475: LearningRateScheduler reducing learning rate to 0.0002983714260629993.\n",
      "Epoch 475/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00475: val_loss did not improve from 0.08524\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1111 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00476: LearningRateScheduler reducing learning rate to 0.0002983645160686587.\n",
      "Epoch 476/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0865\n",
      "Epoch 00476: val_loss did not improve from 0.08524\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0865 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00477: LearningRateScheduler reducing learning rate to 0.0002983575915270485.\n",
      "Epoch 477/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00477: val_loss did not improve from 0.08524\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0943 - val_loss: 0.0857\n",
      "\n",
      "Epoch 00478: LearningRateScheduler reducing learning rate to 0.00029835065243885453.\n",
      "Epoch 478/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00478: val_loss improved from 0.08524 to 0.08477, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1083 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00479: LearningRateScheduler reducing learning rate to 0.0002983436988047641.\n",
      "Epoch 479/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00479: val_loss improved from 0.08477 to 0.08393, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1166 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00480: LearningRateScheduler reducing learning rate to 0.00029833673062546606.\n",
      "Epoch 480/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00480: val_loss improved from 0.08393 to 0.08367, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1067 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00481: LearningRateScheduler reducing learning rate to 0.0002983297479016505.\n",
      "Epoch 481/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 00481: val_loss did not improve from 0.08367\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1033 - val_loss: 0.0843\n",
      "\n",
      "Epoch 00482: LearningRateScheduler reducing learning rate to 0.0002983227506340092.\n",
      "Epoch 482/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 00482: val_loss did not improve from 0.08367\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0895 - val_loss: 0.0858\n",
      "\n",
      "Epoch 00483: LearningRateScheduler reducing learning rate to 0.0002983157388232352.\n",
      "Epoch 483/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00483: val_loss did not improve from 0.08367\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0946 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00484: LearningRateScheduler reducing learning rate to 0.00029830871247002303.\n",
      "Epoch 484/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 00484: val_loss did not improve from 0.08367\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1044 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00485: LearningRateScheduler reducing learning rate to 0.00029830167157506867.\n",
      "Epoch 485/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00485: val_loss did not improve from 0.08367\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1087 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00486: LearningRateScheduler reducing learning rate to 0.00029829461613906956.\n",
      "Epoch 486/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0951\n",
      "Epoch 00486: val_loss did not improve from 0.08367\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.0951 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00487: LearningRateScheduler reducing learning rate to 0.00029828754616272453.\n",
      "Epoch 487/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00487: val_loss improved from 0.08367 to 0.08327, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0984 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00488: LearningRateScheduler reducing learning rate to 0.00029828046164673386.\n",
      "Epoch 488/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00488: val_loss improved from 0.08327 to 0.08217, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1010 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00489: LearningRateScheduler reducing learning rate to 0.00029827336259179935.\n",
      "Epoch 489/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00489: val_loss improved from 0.08217 to 0.08141, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0993 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00490: LearningRateScheduler reducing learning rate to 0.0002982662489986241.\n",
      "Epoch 490/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 00490: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1056 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00491: LearningRateScheduler reducing learning rate to 0.00029825912086791284.\n",
      "Epoch 491/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0837\n",
      "Epoch 00491: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0837 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00492: LearningRateScheduler reducing learning rate to 0.0002982519782003715.\n",
      "Epoch 492/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00492: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1013 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00493: LearningRateScheduler reducing learning rate to 0.00029824482099670766.\n",
      "Epoch 493/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00493: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1046 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00494: LearningRateScheduler reducing learning rate to 0.00029823764925763027.\n",
      "Epoch 494/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00494: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1158 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00495: LearningRateScheduler reducing learning rate to 0.0002982304629838496.\n",
      "Epoch 495/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00495: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1017 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00496: LearningRateScheduler reducing learning rate to 0.0002982232621760776.\n",
      "Epoch 496/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00496: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1157 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00497: LearningRateScheduler reducing learning rate to 0.0002982160468350274.\n",
      "Epoch 497/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1134\n",
      "Epoch 00497: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1134 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00498: LearningRateScheduler reducing learning rate to 0.00029820881696141383.\n",
      "Epoch 498/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 00498: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1029 - val_loss: 0.0873\n",
      "\n",
      "Epoch 00499: LearningRateScheduler reducing learning rate to 0.0002982015725559529.\n",
      "Epoch 499/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1133\n",
      "Epoch 00499: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1133 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00500: LearningRateScheduler reducing learning rate to 0.00029819431361936225.\n",
      "Epoch 500/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00500: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1087 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00501: LearningRateScheduler reducing learning rate to 0.0002981870401523609.\n",
      "Epoch 501/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00501: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1115 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00502: LearningRateScheduler reducing learning rate to 0.0002981797521556693.\n",
      "Epoch 502/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00502: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1047 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00503: LearningRateScheduler reducing learning rate to 0.00029817244963000936.\n",
      "Epoch 503/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0900\n",
      "Epoch 00503: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0900 - val_loss: 0.0968\n",
      "\n",
      "Epoch 00504: LearningRateScheduler reducing learning rate to 0.0002981651325761043.\n",
      "Epoch 504/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 00504: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0932 - val_loss: 0.0975\n",
      "\n",
      "Epoch 00505: LearningRateScheduler reducing learning rate to 0.00029815780099467907.\n",
      "Epoch 505/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 00505: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0922 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00506: LearningRateScheduler reducing learning rate to 0.00029815045488645973.\n",
      "Epoch 506/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00506: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.0972 - val_loss: 0.0966\n",
      "\n",
      "Epoch 00507: LearningRateScheduler reducing learning rate to 0.000298143094252174.\n",
      "Epoch 507/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00507: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0969 - val_loss: 0.0957\n",
      "\n",
      "Epoch 00508: LearningRateScheduler reducing learning rate to 0.00029813571909255096.\n",
      "Epoch 508/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00508: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1035 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00509: LearningRateScheduler reducing learning rate to 0.00029812832940832114.\n",
      "Epoch 509/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 00509: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1051 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00510: LearningRateScheduler reducing learning rate to 0.00029812092520021643.\n",
      "Epoch 510/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00510: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1117 - val_loss: 0.0972\n",
      "\n",
      "Epoch 00511: LearningRateScheduler reducing learning rate to 0.00029811350646897036.\n",
      "Epoch 511/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00511: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1086 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00512: LearningRateScheduler reducing learning rate to 0.00029810607321531773.\n",
      "Epoch 512/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1292\n",
      "Epoch 00512: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1292 - val_loss: 0.0995\n",
      "\n",
      "Epoch 00513: LearningRateScheduler reducing learning rate to 0.0002980986254399947.\n",
      "Epoch 513/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00513: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0989 - val_loss: 0.0994\n",
      "\n",
      "Epoch 00514: LearningRateScheduler reducing learning rate to 0.0002980911631437392.\n",
      "Epoch 514/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1176\n",
      "Epoch 00514: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1176 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00515: LearningRateScheduler reducing learning rate to 0.00029808368632729023.\n",
      "Epoch 515/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1185\n",
      "Epoch 00515: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1185 - val_loss: 0.0980\n",
      "\n",
      "Epoch 00516: LearningRateScheduler reducing learning rate to 0.00029807619499138847.\n",
      "Epoch 516/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00516: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1013 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00517: LearningRateScheduler reducing learning rate to 0.0002980686891367759.\n",
      "Epoch 517/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00517: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1110 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00518: LearningRateScheduler reducing learning rate to 0.000298061168764196.\n",
      "Epoch 518/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0957\n",
      "Epoch 00518: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0957 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00519: LearningRateScheduler reducing learning rate to 0.0002980536338743937.\n",
      "Epoch 519/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00519: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0994 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00520: LearningRateScheduler reducing learning rate to 0.00029804608446811534.\n",
      "Epoch 520/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0841\n",
      "Epoch 00520: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0841 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00521: LearningRateScheduler reducing learning rate to 0.0002980385205461087.\n",
      "Epoch 521/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00521: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1158 - val_loss: 0.0876\n",
      "\n",
      "Epoch 00522: LearningRateScheduler reducing learning rate to 0.000298030942109123.\n",
      "Epoch 522/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00522: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0989 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00523: LearningRateScheduler reducing learning rate to 0.0002980233491579089.\n",
      "Epoch 523/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00523: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0943 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00524: LearningRateScheduler reducing learning rate to 0.00029801574169321856.\n",
      "Epoch 524/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00524: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1172 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00525: LearningRateScheduler reducing learning rate to 0.00029800811971580546.\n",
      "Epoch 525/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00525: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1011 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00526: LearningRateScheduler reducing learning rate to 0.00029800048322642457.\n",
      "Epoch 526/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 00526: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0948 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00527: LearningRateScheduler reducing learning rate to 0.0002979928322258323.\n",
      "Epoch 527/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00527: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1108 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00528: LearningRateScheduler reducing learning rate to 0.0002979851667147865.\n",
      "Epoch 528/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00528: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1154 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00529: LearningRateScheduler reducing learning rate to 0.0002979774866940465.\n",
      "Epoch 529/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0947\n",
      "Epoch 00529: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0947 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00530: LearningRateScheduler reducing learning rate to 0.000297969792164373.\n",
      "Epoch 530/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00530: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1101 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00531: LearningRateScheduler reducing learning rate to 0.00029796208312652815.\n",
      "Epoch 531/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 00531: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1022 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00532: LearningRateScheduler reducing learning rate to 0.0002979543595812755.\n",
      "Epoch 532/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00532: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1015 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00533: LearningRateScheduler reducing learning rate to 0.0002979466215293802.\n",
      "Epoch 533/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00533: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1113 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00534: LearningRateScheduler reducing learning rate to 0.0002979388689716087.\n",
      "Epoch 534/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00534: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1101 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00535: LearningRateScheduler reducing learning rate to 0.00029793110190872883.\n",
      "Epoch 535/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00535: val_loss did not improve from 0.08141\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0987 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00536: LearningRateScheduler reducing learning rate to 0.00029792332034150995.\n",
      "Epoch 536/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0870\n",
      "Epoch 00536: val_loss improved from 0.08141 to 0.07925, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0870 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00537: LearningRateScheduler reducing learning rate to 0.00029791552427072286.\n",
      "Epoch 537/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00537: val_loss improved from 0.07925 to 0.07728, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0999 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00538: LearningRateScheduler reducing learning rate to 0.00029790771369713983.\n",
      "Epoch 538/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00538: val_loss improved from 0.07728 to 0.07623, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1171 - val_loss: 0.0762\n",
      "\n",
      "Epoch 00539: LearningRateScheduler reducing learning rate to 0.0002978998886215344.\n",
      "Epoch 539/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00539: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1116 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00540: LearningRateScheduler reducing learning rate to 0.0002978920490446818.\n",
      "Epoch 540/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0894\n",
      "Epoch 00540: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0894 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00541: LearningRateScheduler reducing learning rate to 0.00029788419496735844.\n",
      "Epoch 541/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00541: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.1250 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00542: LearningRateScheduler reducing learning rate to 0.0002978763263903424.\n",
      "Epoch 542/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00542: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1203 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00543: LearningRateScheduler reducing learning rate to 0.00029786844331441297.\n",
      "Epoch 543/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00543: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0823 - val_loss: 0.0799\n",
      "\n",
      "Epoch 00544: LearningRateScheduler reducing learning rate to 0.0002978605457403511.\n",
      "Epoch 544/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00544: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0973 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00545: LearningRateScheduler reducing learning rate to 0.0002978526336689389.\n",
      "Epoch 545/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00545: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1295 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00546: LearningRateScheduler reducing learning rate to 0.00029784470710096016.\n",
      "Epoch 546/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 00546: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1016 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00547: LearningRateScheduler reducing learning rate to 0.0002978367660372001.\n",
      "Epoch 547/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00547: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1277 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00548: LearningRateScheduler reducing learning rate to 0.0002978288104784453.\n",
      "Epoch 548/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00548: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.1255 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00549: LearningRateScheduler reducing learning rate to 0.0002978208404254836.\n",
      "Epoch 549/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00549: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.1243 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00550: LearningRateScheduler reducing learning rate to 0.0002978128558791046.\n",
      "Epoch 550/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0845\n",
      "Epoch 00550: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0845 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00551: LearningRateScheduler reducing learning rate to 0.0002978048568400992.\n",
      "Epoch 551/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00551: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1101 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00552: LearningRateScheduler reducing learning rate to 0.0002977968433092597.\n",
      "Epoch 552/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00552: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0990 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00553: LearningRateScheduler reducing learning rate to 0.00029778881528737976.\n",
      "Epoch 553/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00553: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1084 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00554: LearningRateScheduler reducing learning rate to 0.0002977807727752547.\n",
      "Epoch 554/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00554: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1118 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00555: LearningRateScheduler reducing learning rate to 0.0002977727157736811.\n",
      "Epoch 555/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 00555: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0915 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00556: LearningRateScheduler reducing learning rate to 0.000297764644283457.\n",
      "Epoch 556/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1313\n",
      "Epoch 00556: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1313 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00557: LearningRateScheduler reducing learning rate to 0.00029775655830538195.\n",
      "Epoch 557/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 00557: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1038 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00558: LearningRateScheduler reducing learning rate to 0.00029774845784025683.\n",
      "Epoch 558/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1195\n",
      "Epoch 00558: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1195 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00559: LearningRateScheduler reducing learning rate to 0.0002977403428888841.\n",
      "Epoch 559/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00559: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1101 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00560: LearningRateScheduler reducing learning rate to 0.0002977322134520675.\n",
      "Epoch 560/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00560: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1156 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00561: LearningRateScheduler reducing learning rate to 0.0002977240695306122.\n",
      "Epoch 561/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00561: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0979 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00562: LearningRateScheduler reducing learning rate to 0.000297715911125325.\n",
      "Epoch 562/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00562: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0959 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00563: LearningRateScheduler reducing learning rate to 0.00029770773823701403.\n",
      "Epoch 563/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0863\n",
      "Epoch 00563: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0863 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00564: LearningRateScheduler reducing learning rate to 0.0002976995508664886.\n",
      "Epoch 564/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0896\n",
      "Epoch 00564: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 0.0896 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00565: LearningRateScheduler reducing learning rate to 0.00029769134901456.\n",
      "Epoch 565/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0871\n",
      "Epoch 00565: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0871 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00566: LearningRateScheduler reducing learning rate to 0.0002976831326820404.\n",
      "Epoch 566/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 00566: val_loss did not improve from 0.07623\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0917 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00567: LearningRateScheduler reducing learning rate to 0.0002976749018697438.\n",
      "Epoch 567/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 00567: val_loss improved from 0.07623 to 0.07555, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 4s 1s/step - loss: 0.0780 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00568: LearningRateScheduler reducing learning rate to 0.0002976666565784854.\n",
      "Epoch 568/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00568: val_loss improved from 0.07555 to 0.07495, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1018 - val_loss: 0.0750\n",
      "\n",
      "Epoch 00569: LearningRateScheduler reducing learning rate to 0.0002976583968090819.\n",
      "Epoch 569/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 00569: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0934 - val_loss: 0.0752\n",
      "\n",
      "Epoch 00570: LearningRateScheduler reducing learning rate to 0.00029765012256235154.\n",
      "Epoch 570/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00570: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0998 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00571: LearningRateScheduler reducing learning rate to 0.0002976418338391138.\n",
      "Epoch 571/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00571: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1248 - val_loss: 0.0769\n",
      "\n",
      "Epoch 00572: LearningRateScheduler reducing learning rate to 0.0002976335306401898.\n",
      "Epoch 572/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0864\n",
      "Epoch 00572: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0864 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00573: LearningRateScheduler reducing learning rate to 0.0002976252129664019.\n",
      "Epoch 573/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00573: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1021 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00574: LearningRateScheduler reducing learning rate to 0.000297616880818574.\n",
      "Epoch 574/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00574: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1163 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00575: LearningRateScheduler reducing learning rate to 0.00029760853419753147.\n",
      "Epoch 575/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0873\n",
      "Epoch 00575: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0873 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00576: LearningRateScheduler reducing learning rate to 0.000297600173104101.\n",
      "Epoch 576/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00576: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0923 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00577: LearningRateScheduler reducing learning rate to 0.00029759179753911086.\n",
      "Epoch 577/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00577: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1233 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00578: LearningRateScheduler reducing learning rate to 0.00029758340750339054.\n",
      "Epoch 578/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1141\n",
      "Epoch 00578: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1141 - val_loss: 0.0769\n",
      "\n",
      "Epoch 00579: LearningRateScheduler reducing learning rate to 0.00029757500299777126.\n",
      "Epoch 579/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00579: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1085 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00580: LearningRateScheduler reducing learning rate to 0.0002975665840230854.\n",
      "Epoch 580/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00580: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0958 - val_loss: 0.0787\n",
      "\n",
      "Epoch 00581: LearningRateScheduler reducing learning rate to 0.00029755815058016684.\n",
      "Epoch 581/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0830\n",
      "Epoch 00581: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0830 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00582: LearningRateScheduler reducing learning rate to 0.000297549702669851.\n",
      "Epoch 582/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0892\n",
      "Epoch 00582: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0892 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00583: LearningRateScheduler reducing learning rate to 0.00029754124029297466.\n",
      "Epoch 583/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00583: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1272 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00584: LearningRateScheduler reducing learning rate to 0.00029753276345037603.\n",
      "Epoch 584/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00584: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.1171 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00585: LearningRateScheduler reducing learning rate to 0.0002975242721428948.\n",
      "Epoch 585/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 00585: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0897 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00586: LearningRateScheduler reducing learning rate to 0.0002975157663713719.\n",
      "Epoch 586/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1205\n",
      "Epoch 00586: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.1205 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00587: LearningRateScheduler reducing learning rate to 0.0002975072461366501.\n",
      "Epoch 587/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0866\n",
      "Epoch 00587: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0866 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00588: LearningRateScheduler reducing learning rate to 0.0002974987114395731.\n",
      "Epoch 588/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00588: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1025 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00589: LearningRateScheduler reducing learning rate to 0.0002974901622809864.\n",
      "Epoch 589/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00589: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.0991 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00590: LearningRateScheduler reducing learning rate to 0.0002974815986617369.\n",
      "Epoch 590/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00590: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1163 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00591: LearningRateScheduler reducing learning rate to 0.00029747302058267264.\n",
      "Epoch 591/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00591: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1037 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00592: LearningRateScheduler reducing learning rate to 0.0002974644280446434.\n",
      "Epoch 592/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00592: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.0946 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00593: LearningRateScheduler reducing learning rate to 0.00029745582104850036.\n",
      "Epoch 593/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00593: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1077 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00594: LearningRateScheduler reducing learning rate to 0.00029744719959509604.\n",
      "Epoch 594/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00594: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1111 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00595: LearningRateScheduler reducing learning rate to 0.0002974385636852843.\n",
      "Epoch 595/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0902\n",
      "Epoch 00595: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0902 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00596: LearningRateScheduler reducing learning rate to 0.00029742991331992066.\n",
      "Epoch 596/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1423\n",
      "Epoch 00596: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.1423 - val_loss: 0.0889\n",
      "\n",
      "Epoch 00597: LearningRateScheduler reducing learning rate to 0.00029742124849986193.\n",
      "Epoch 597/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0861\n",
      "Epoch 00597: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0861 - val_loss: 0.0899\n",
      "\n",
      "Epoch 00598: LearningRateScheduler reducing learning rate to 0.0002974125692259663.\n",
      "Epoch 598/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00598: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1009 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00599: LearningRateScheduler reducing learning rate to 0.0002974038754990936.\n",
      "Epoch 599/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00599: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1231 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00600: LearningRateScheduler reducing learning rate to 0.0002973951673201049.\n",
      "Epoch 600/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00600: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1166 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00601: LearningRateScheduler reducing learning rate to 0.0002973864446898628.\n",
      "Epoch 601/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00601: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0970 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00602: LearningRateScheduler reducing learning rate to 0.00029737770760923123.\n",
      "Epoch 602/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0908\n",
      "Epoch 00602: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0908 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00603: LearningRateScheduler reducing learning rate to 0.0002973689560790757.\n",
      "Epoch 603/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 00603: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0898 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00604: LearningRateScheduler reducing learning rate to 0.00029736019010026304.\n",
      "Epoch 604/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00604: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0998 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00605: LearningRateScheduler reducing learning rate to 0.0002973514096736615.\n",
      "Epoch 605/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0910\n",
      "Epoch 00605: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0910 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00606: LearningRateScheduler reducing learning rate to 0.0002973426148001408.\n",
      "Epoch 606/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0924\n",
      "Epoch 00606: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0924 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00607: LearningRateScheduler reducing learning rate to 0.00029733380548057216.\n",
      "Epoch 607/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00607: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1015 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00608: LearningRateScheduler reducing learning rate to 0.0002973249817158281.\n",
      "Epoch 608/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0936\n",
      "Epoch 00608: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0936 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00609: LearningRateScheduler reducing learning rate to 0.0002973161435067827.\n",
      "Epoch 609/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0942\n",
      "Epoch 00609: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0942 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00610: LearningRateScheduler reducing learning rate to 0.00029730729085431136.\n",
      "Epoch 610/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 00610: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0917 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00611: LearningRateScheduler reducing learning rate to 0.0002972984237592909.\n",
      "Epoch 611/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00611: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1048 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00612: LearningRateScheduler reducing learning rate to 0.0002972895422225997.\n",
      "Epoch 612/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 00612: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1052 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00613: LearningRateScheduler reducing learning rate to 0.00029728064624511756.\n",
      "Epoch 613/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1189\n",
      "Epoch 00613: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1189 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00614: LearningRateScheduler reducing learning rate to 0.0002972717358277255.\n",
      "Epoch 614/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 00614: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1066 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00615: LearningRateScheduler reducing learning rate to 0.00029726281097130624.\n",
      "Epoch 615/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00615: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1001 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00616: LearningRateScheduler reducing learning rate to 0.0002972538716767437.\n",
      "Epoch 616/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00616: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1288 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00617: LearningRateScheduler reducing learning rate to 0.00029724491794492347.\n",
      "Epoch 617/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00617: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.0983 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00618: LearningRateScheduler reducing learning rate to 0.0002972359497767323.\n",
      "Epoch 618/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0816\n",
      "Epoch 00618: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0816 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00619: LearningRateScheduler reducing learning rate to 0.00029722696717305854.\n",
      "Epoch 619/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00619: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1069 - val_loss: 0.0769\n",
      "\n",
      "Epoch 00620: LearningRateScheduler reducing learning rate to 0.000297217970134792.\n",
      "Epoch 620/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 00620: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0784 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00621: LearningRateScheduler reducing learning rate to 0.00029720895866282385.\n",
      "Epoch 621/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0848\n",
      "Epoch 00621: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0848 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00622: LearningRateScheduler reducing learning rate to 0.00029719993275804664.\n",
      "Epoch 622/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00622: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1049 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00623: LearningRateScheduler reducing learning rate to 0.0002971908924213545.\n",
      "Epoch 623/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 00623: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1012 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00624: LearningRateScheduler reducing learning rate to 0.0002971818376536427.\n",
      "Epoch 624/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 00624: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1038 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00625: LearningRateScheduler reducing learning rate to 0.0002971727684558084.\n",
      "Epoch 625/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0937\n",
      "Epoch 00625: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0937 - val_loss: 0.0799\n",
      "\n",
      "Epoch 00626: LearningRateScheduler reducing learning rate to 0.0002971636848287498.\n",
      "Epoch 626/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00626: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0977 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00627: LearningRateScheduler reducing learning rate to 0.00029715458677336654.\n",
      "Epoch 627/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00627: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0967 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00628: LearningRateScheduler reducing learning rate to 0.00029714547429056.\n",
      "Epoch 628/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00628: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1103 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00629: LearningRateScheduler reducing learning rate to 0.00029713634738123265.\n",
      "Epoch 629/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00629: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1028 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00630: LearningRateScheduler reducing learning rate to 0.00029712720604628865.\n",
      "Epoch 630/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 00630: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0918 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00631: LearningRateScheduler reducing learning rate to 0.0002971180502866334.\n",
      "Epoch 631/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0938\n",
      "Epoch 00631: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0938 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00632: LearningRateScheduler reducing learning rate to 0.00029710888010317375.\n",
      "Epoch 632/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00632: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1161 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00633: LearningRateScheduler reducing learning rate to 0.0002970996954968181.\n",
      "Epoch 633/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 00633: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0898 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00634: LearningRateScheduler reducing learning rate to 0.0002970904964684762.\n",
      "Epoch 634/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 00634: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0783 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00635: LearningRateScheduler reducing learning rate to 0.00029708128301905924.\n",
      "Epoch 635/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 00635: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0941 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00636: LearningRateScheduler reducing learning rate to 0.0002970720551494798.\n",
      "Epoch 636/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00636: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1046 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00637: LearningRateScheduler reducing learning rate to 0.00029706281286065195.\n",
      "Epoch 637/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00637: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1086 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00638: LearningRateScheduler reducing learning rate to 0.0002970535561534912.\n",
      "Epoch 638/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1212\n",
      "Epoch 00638: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1212 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00639: LearningRateScheduler reducing learning rate to 0.0002970442850289143.\n",
      "Epoch 639/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0944\n",
      "Epoch 00639: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0944 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00640: LearningRateScheduler reducing learning rate to 0.0002970349994878397.\n",
      "Epoch 640/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 00640: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0770 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00641: LearningRateScheduler reducing learning rate to 0.0002970256995311872.\n",
      "Epoch 641/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00641: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1168 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00642: LearningRateScheduler reducing learning rate to 0.00029701638515987783.\n",
      "Epoch 642/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00642: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1097 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00643: LearningRateScheduler reducing learning rate to 0.00029700705637483434.\n",
      "Epoch 643/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 00643: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0917 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00644: LearningRateScheduler reducing learning rate to 0.0002969977131769807.\n",
      "Epoch 644/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00644: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0961 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00645: LearningRateScheduler reducing learning rate to 0.0002969883555672424.\n",
      "Epoch 645/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00645: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0986 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00646: LearningRateScheduler reducing learning rate to 0.0002969789835465463.\n",
      "Epoch 646/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0797\n",
      "Epoch 00646: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0797 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00647: LearningRateScheduler reducing learning rate to 0.0002969695971158207.\n",
      "Epoch 647/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00647: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1257 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00648: LearningRateScheduler reducing learning rate to 0.0002969601962759954.\n",
      "Epoch 648/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00648: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1156 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00649: LearningRateScheduler reducing learning rate to 0.0002969507810280016.\n",
      "Epoch 649/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00649: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1149 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00650: LearningRateScheduler reducing learning rate to 0.0002969413513727718.\n",
      "Epoch 650/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00650: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 0.1202 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00651: LearningRateScheduler reducing learning rate to 0.00029693190731124014.\n",
      "Epoch 651/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00651: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0999 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00652: LearningRateScheduler reducing learning rate to 0.00029692244884434204.\n",
      "Epoch 652/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0794\n",
      "Epoch 00652: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0794 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00653: LearningRateScheduler reducing learning rate to 0.00029691297597301435.\n",
      "Epoch 653/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 00653: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0787 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00654: LearningRateScheduler reducing learning rate to 0.0002969034886981954.\n",
      "Epoch 654/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 00654: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0785 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00655: LearningRateScheduler reducing learning rate to 0.00029689398702082494.\n",
      "Epoch 655/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00655: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.1087 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00656: LearningRateScheduler reducing learning rate to 0.0002968844709418441.\n",
      "Epoch 656/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0837\n",
      "Epoch 00656: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0837 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00657: LearningRateScheduler reducing learning rate to 0.0002968749404621955.\n",
      "Epoch 657/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 00657: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1050 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00658: LearningRateScheduler reducing learning rate to 0.0002968653955828231.\n",
      "Epoch 658/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00658: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1078 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00659: LearningRateScheduler reducing learning rate to 0.00029685583630467243.\n",
      "Epoch 659/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0947\n",
      "Epoch 00659: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0947 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00660: LearningRateScheduler reducing learning rate to 0.00029684626262869036.\n",
      "Epoch 660/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 00660: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1076 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00661: LearningRateScheduler reducing learning rate to 0.00029683667455582507.\n",
      "Epoch 661/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00661: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0943 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00662: LearningRateScheduler reducing learning rate to 0.00029682707208702645.\n",
      "Epoch 662/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00662: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1139 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00663: LearningRateScheduler reducing learning rate to 0.00029681745522324544.\n",
      "Epoch 663/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0864\n",
      "Epoch 00663: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0864 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00664: LearningRateScheduler reducing learning rate to 0.0002968078239654348.\n",
      "Epoch 664/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00664: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1049 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00665: LearningRateScheduler reducing learning rate to 0.00029679817831454843.\n",
      "Epoch 665/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 00665: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0922 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00666: LearningRateScheduler reducing learning rate to 0.0002967885182715417.\n",
      "Epoch 666/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0906\n",
      "Epoch 00666: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0906 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00667: LearningRateScheduler reducing learning rate to 0.0002967788438373716.\n",
      "Epoch 667/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1382\n",
      "Epoch 00667: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1382 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00668: LearningRateScheduler reducing learning rate to 0.0002967691550129963.\n",
      "Epoch 668/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0873\n",
      "Epoch 00668: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0873 - val_loss: 0.0868\n",
      "Epoch 00668: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataloader, epochs=Max_epoch, initial_epoch=0, validation_data=val_dataloader, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取得解凍時間點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_epoch = early_stopping.stopped_epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解凍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_body.layers)): \n",
    "    model_body.layers[i].trainable = True\n",
    "\n",
    "model.compile(optimizer = opt, loss={'yolo_loss': lambda y_true, y_pred: y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping  = EarlyStopping(monitor='val_loss', min_delta = 0, patience = 350, verbose = 1)\n",
    "\n",
    "callbacks = [checkpoint_callback, lr_scheduler, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微調訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00669: LearningRateScheduler reducing learning rate to 0.0002967594517993756.\n",
      "Epoch 669/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1344\n",
      "Epoch 00669: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 3s 868ms/step - loss: 0.1344 - val_loss: 0.1139\n",
      "\n",
      "Epoch 00670: LearningRateScheduler reducing learning rate to 0.0002967497341974705.\n",
      "Epoch 670/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 00670: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0920 - val_loss: 0.1102\n",
      "\n",
      "Epoch 00671: LearningRateScheduler reducing learning rate to 0.0002967400022082437.\n",
      "Epoch 671/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00671: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0993 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00672: LearningRateScheduler reducing learning rate to 0.0002967302558326591.\n",
      "Epoch 672/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 00672: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1020 - val_loss: 0.0867\n",
      "\n",
      "Epoch 00673: LearningRateScheduler reducing learning rate to 0.000296720495071682.\n",
      "Epoch 673/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 00673: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0941 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00674: LearningRateScheduler reducing learning rate to 0.00029671071992627934.\n",
      "Epoch 674/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1204\n",
      "Epoch 00674: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1204 - val_loss: 0.1027\n",
      "\n",
      "Epoch 00675: LearningRateScheduler reducing learning rate to 0.0002967009303974194.\n",
      "Epoch 675/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00675: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1182 - val_loss: 0.1121\n",
      "\n",
      "Epoch 00676: LearningRateScheduler reducing learning rate to 0.00029669112648607175.\n",
      "Epoch 676/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00676: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1090 - val_loss: 0.1136\n",
      "\n",
      "Epoch 00677: LearningRateScheduler reducing learning rate to 0.0002966813081932076.\n",
      "Epoch 677/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0907\n",
      "Epoch 00677: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0907 - val_loss: 0.1112\n",
      "\n",
      "Epoch 00678: LearningRateScheduler reducing learning rate to 0.00029667147551979935.\n",
      "Epoch 678/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0856\n",
      "Epoch 00678: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0856 - val_loss: 0.1048\n",
      "\n",
      "Epoch 00679: LearningRateScheduler reducing learning rate to 0.00029666162846682105.\n",
      "Epoch 679/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0892\n",
      "Epoch 00679: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0892 - val_loss: 0.0985\n",
      "\n",
      "Epoch 00680: LearningRateScheduler reducing learning rate to 0.0002966517670352481.\n",
      "Epoch 680/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00680: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.1101 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00681: LearningRateScheduler reducing learning rate to 0.00029664189122605714.\n",
      "Epoch 681/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0949\n",
      "Epoch 00681: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0949 - val_loss: 0.0985\n",
      "\n",
      "Epoch 00682: LearningRateScheduler reducing learning rate to 0.0002966320010402265.\n",
      "Epoch 682/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00682: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1106 - val_loss: 0.1049\n",
      "\n",
      "Epoch 00683: LearningRateScheduler reducing learning rate to 0.00029662209647873594.\n",
      "Epoch 683/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00683: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1064 - val_loss: 0.1066\n",
      "\n",
      "Epoch 00684: LearningRateScheduler reducing learning rate to 0.00029661217754256623.\n",
      "Epoch 684/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00684: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.1111 - val_loss: 0.1192\n",
      "\n",
      "Epoch 00685: LearningRateScheduler reducing learning rate to 0.0002966022442327002.\n",
      "Epoch 685/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1345\n",
      "Epoch 00685: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.1345 - val_loss: 0.1289\n",
      "\n",
      "Epoch 00686: LearningRateScheduler reducing learning rate to 0.0002965922965501215.\n",
      "Epoch 686/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0878\n",
      "Epoch 00686: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0878 - val_loss: 0.1296\n",
      "\n",
      "Epoch 00687: LearningRateScheduler reducing learning rate to 0.00029658233449581564.\n",
      "Epoch 687/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00687: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0971 - val_loss: 0.1303\n",
      "\n",
      "Epoch 00688: LearningRateScheduler reducing learning rate to 0.0002965723580707693.\n",
      "Epoch 688/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00688: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0961 - val_loss: 0.1269\n",
      "\n",
      "Epoch 00689: LearningRateScheduler reducing learning rate to 0.0002965623672759707.\n",
      "Epoch 689/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 00689: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0898 - val_loss: 0.1221\n",
      "\n",
      "Epoch 00690: LearningRateScheduler reducing learning rate to 0.00029655236211240945.\n",
      "Epoch 690/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00690: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1047 - val_loss: 0.1121\n",
      "\n",
      "Epoch 00691: LearningRateScheduler reducing learning rate to 0.0002965423425810765.\n",
      "Epoch 691/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00691: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1106 - val_loss: 0.1015\n",
      "\n",
      "Epoch 00692: LearningRateScheduler reducing learning rate to 0.0002965323086829645.\n",
      "Epoch 692/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00692: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0983 - val_loss: 0.0960\n",
      "\n",
      "Epoch 00693: LearningRateScheduler reducing learning rate to 0.00029652226041906713.\n",
      "Epoch 693/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 00693: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1016 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00694: LearningRateScheduler reducing learning rate to 0.0002965121977903798.\n",
      "Epoch 694/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00694: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1152 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00695: LearningRateScheduler reducing learning rate to 0.00029650212079789927.\n",
      "Epoch 695/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00695: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1206 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00696: LearningRateScheduler reducing learning rate to 0.00029649202944262356.\n",
      "Epoch 696/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00696: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1001 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00697: LearningRateScheduler reducing learning rate to 0.00029648192372555234.\n",
      "Epoch 697/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00697: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1136 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00698: LearningRateScheduler reducing learning rate to 0.0002964718036476866.\n",
      "Epoch 698/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 00698: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0941 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00699: LearningRateScheduler reducing learning rate to 0.00029646166921002873.\n",
      "Epoch 699/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00699: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1073 - val_loss: 0.0918\n",
      "\n",
      "Epoch 00700: LearningRateScheduler reducing learning rate to 0.0002964515204135826.\n",
      "Epoch 700/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1190\n",
      "Epoch 00700: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1190 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00701: LearningRateScheduler reducing learning rate to 0.00029644135725935344.\n",
      "Epoch 701/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00701: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0923 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00702: LearningRateScheduler reducing learning rate to 0.00029643117974834793.\n",
      "Epoch 702/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1308\n",
      "Epoch 00702: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1308 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00703: LearningRateScheduler reducing learning rate to 0.0002964209878815742.\n",
      "Epoch 703/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00703: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1127 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00704: LearningRateScheduler reducing learning rate to 0.00029641078166004175.\n",
      "Epoch 704/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0864\n",
      "Epoch 00704: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0864 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00705: LearningRateScheduler reducing learning rate to 0.00029640056108476155.\n",
      "Epoch 705/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00705: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1167 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00706: LearningRateScheduler reducing learning rate to 0.000296390326156746.\n",
      "Epoch 706/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 00706: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1033 - val_loss: 0.0985\n",
      "\n",
      "Epoch 00707: LearningRateScheduler reducing learning rate to 0.00029638007687700883.\n",
      "Epoch 707/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0744\n",
      "Epoch 00707: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0744 - val_loss: 0.0972\n",
      "\n",
      "Epoch 00708: LearningRateScheduler reducing learning rate to 0.0002963698132465653.\n",
      "Epoch 708/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 00708: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1181 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00709: LearningRateScheduler reducing learning rate to 0.0002963595352664321.\n",
      "Epoch 709/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0825\n",
      "Epoch 00709: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0825 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00710: LearningRateScheduler reducing learning rate to 0.0002963492429376271.\n",
      "Epoch 710/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00710: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.1001 - val_loss: 0.0975\n",
      "\n",
      "Epoch 00711: LearningRateScheduler reducing learning rate to 0.00029633893626116993.\n",
      "Epoch 711/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1307\n",
      "Epoch 00711: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1307 - val_loss: 0.1019\n",
      "\n",
      "Epoch 00712: LearningRateScheduler reducing learning rate to 0.0002963286152380815.\n",
      "Epoch 712/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00712: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0977 - val_loss: 0.1049\n",
      "\n",
      "Epoch 00713: LearningRateScheduler reducing learning rate to 0.0002963182798693841.\n",
      "Epoch 713/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00713: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1013 - val_loss: 0.1016\n",
      "\n",
      "Epoch 00714: LearningRateScheduler reducing learning rate to 0.00029630793015610144.\n",
      "Epoch 714/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0739\n",
      "Epoch 00714: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0739 - val_loss: 0.0997\n",
      "\n",
      "Epoch 00715: LearningRateScheduler reducing learning rate to 0.0002962975660992587.\n",
      "Epoch 715/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00715: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1114 - val_loss: 0.0988\n",
      "\n",
      "Epoch 00716: LearningRateScheduler reducing learning rate to 0.0002962871876998825.\n",
      "Epoch 716/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 00716: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1144 - val_loss: 0.0996\n",
      "\n",
      "Epoch 00717: LearningRateScheduler reducing learning rate to 0.00029627679495900075.\n",
      "Epoch 717/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0940\n",
      "Epoch 00717: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0940 - val_loss: 0.1018\n",
      "\n",
      "Epoch 00718: LearningRateScheduler reducing learning rate to 0.00029626638787764304.\n",
      "Epoch 718/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0863\n",
      "Epoch 00718: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0863 - val_loss: 0.1014\n",
      "\n",
      "Epoch 00719: LearningRateScheduler reducing learning rate to 0.00029625596645684006.\n",
      "Epoch 719/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0868\n",
      "Epoch 00719: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0868 - val_loss: 0.0969\n",
      "\n",
      "Epoch 00720: LearningRateScheduler reducing learning rate to 0.0002962455306976241.\n",
      "Epoch 720/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0876\n",
      "Epoch 00720: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0876 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00721: LearningRateScheduler reducing learning rate to 0.0002962350806010289.\n",
      "Epoch 721/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0883\n",
      "Epoch 00721: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0883 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00722: LearningRateScheduler reducing learning rate to 0.0002962246161680896.\n",
      "Epoch 722/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0850\n",
      "Epoch 00722: val_loss did not improve from 0.07495\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0850 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00723: LearningRateScheduler reducing learning rate to 0.0002962141373998426.\n",
      "Epoch 723/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1211\n",
      "Epoch 00723: val_loss improved from 0.07495 to 0.07336, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.1211 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00724: LearningRateScheduler reducing learning rate to 0.000296203644297326.\n",
      "Epoch 724/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00724: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1127 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00725: LearningRateScheduler reducing learning rate to 0.00029619313686157897.\n",
      "Epoch 725/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00725: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1081 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00726: LearningRateScheduler reducing learning rate to 0.0002961826150936425.\n",
      "Epoch 726/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00726: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0964 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00727: LearningRateScheduler reducing learning rate to 0.0002961720789945586.\n",
      "Epoch 727/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00727: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1021 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00728: LearningRateScheduler reducing learning rate to 0.0002961615285653711.\n",
      "Epoch 728/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00728: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0961 - val_loss: 0.0771\n",
      "\n",
      "Epoch 00729: LearningRateScheduler reducing learning rate to 0.0002961509638071249.\n",
      "Epoch 729/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00729: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1058 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00730: LearningRateScheduler reducing learning rate to 0.0002961403847208665.\n",
      "Epoch 730/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00730: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1090 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00731: LearningRateScheduler reducing learning rate to 0.0002961297913076438.\n",
      "Epoch 731/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0939\n",
      "Epoch 00731: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0939 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00732: LearningRateScheduler reducing learning rate to 0.0002961191835685061.\n",
      "Epoch 732/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0847\n",
      "Epoch 00732: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0847 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00733: LearningRateScheduler reducing learning rate to 0.0002961085615045041.\n",
      "Epoch 733/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0939\n",
      "Epoch 00733: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0939 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00734: LearningRateScheduler reducing learning rate to 0.00029609792511668995.\n",
      "Epoch 734/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 00734: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0927 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00735: LearningRateScheduler reducing learning rate to 0.00029608727440611724.\n",
      "Epoch 735/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0813\n",
      "Epoch 00735: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0813 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00736: LearningRateScheduler reducing learning rate to 0.00029607660937384093.\n",
      "Epoch 736/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0938\n",
      "Epoch 00736: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0938 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00737: LearningRateScheduler reducing learning rate to 0.00029606593002091735.\n",
      "Epoch 737/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00737: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.1083 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00738: LearningRateScheduler reducing learning rate to 0.00029605523634840437.\n",
      "Epoch 738/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00738: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1127 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00739: LearningRateScheduler reducing learning rate to 0.00029604452835736125.\n",
      "Epoch 739/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1199\n",
      "Epoch 00739: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1199 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00740: LearningRateScheduler reducing learning rate to 0.0002960338060488486.\n",
      "Epoch 740/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 00740: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.1054 - val_loss: 0.0893\n",
      "\n",
      "Epoch 00741: LearningRateScheduler reducing learning rate to 0.00029602306942392854.\n",
      "Epoch 741/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0759\n",
      "Epoch 00741: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0759 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00742: LearningRateScheduler reducing learning rate to 0.0002960123184836645.\n",
      "Epoch 742/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00742: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0979 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00743: LearningRateScheduler reducing learning rate to 0.0002960015532291214.\n",
      "Epoch 743/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00743: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0971 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00744: LearningRateScheduler reducing learning rate to 0.00029599077366136554.\n",
      "Epoch 744/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 00744: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1061 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00745: LearningRateScheduler reducing learning rate to 0.0002959799797814647.\n",
      "Epoch 745/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 00745: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1068 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00746: LearningRateScheduler reducing learning rate to 0.0002959691715904881.\n",
      "Epoch 746/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00746: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1178 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00747: LearningRateScheduler reducing learning rate to 0.0002959583490895062.\n",
      "Epoch 747/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 00747: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1023 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00748: LearningRateScheduler reducing learning rate to 0.00029594751227959105.\n",
      "Epoch 748/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 00748: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0897 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00749: LearningRateScheduler reducing learning rate to 0.00029593666116181606.\n",
      "Epoch 749/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00749: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.1039 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00750: LearningRateScheduler reducing learning rate to 0.000295925795737256.\n",
      "Epoch 750/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 00750: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0903 - val_loss: 0.0930\n",
      "\n",
      "Epoch 00751: LearningRateScheduler reducing learning rate to 0.00029591491600698715.\n",
      "Epoch 751/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00751: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0959 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00752: LearningRateScheduler reducing learning rate to 0.0002959040219720872.\n",
      "Epoch 752/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0882\n",
      "Epoch 00752: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0882 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00753: LearningRateScheduler reducing learning rate to 0.0002958931136336353.\n",
      "Epoch 753/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00753: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0984 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00754: LearningRateScheduler reducing learning rate to 0.0002958821909927118.\n",
      "Epoch 754/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00754: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1069 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00755: LearningRateScheduler reducing learning rate to 0.0002958712540503987.\n",
      "Epoch 755/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 00755: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0915 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00756: LearningRateScheduler reducing learning rate to 0.00029586030280777923.\n",
      "Epoch 756/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0851\n",
      "Epoch 00756: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0851 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00757: LearningRateScheduler reducing learning rate to 0.00029584933726593826.\n",
      "Epoch 757/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 00757: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0897 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00758: LearningRateScheduler reducing learning rate to 0.00029583835742596197.\n",
      "Epoch 758/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0839\n",
      "Epoch 00758: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0839 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00759: LearningRateScheduler reducing learning rate to 0.00029582736328893775.\n",
      "Epoch 759/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0808\n",
      "Epoch 00759: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0808 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00760: LearningRateScheduler reducing learning rate to 0.00029581635485595475.\n",
      "Epoch 760/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0881\n",
      "Epoch 00760: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0881 - val_loss: 0.0882\n",
      "\n",
      "Epoch 00761: LearningRateScheduler reducing learning rate to 0.0002958053321281034.\n",
      "Epoch 761/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00761: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1040 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00762: LearningRateScheduler reducing learning rate to 0.00029579429510647544.\n",
      "Epoch 762/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00762: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1067 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00763: LearningRateScheduler reducing learning rate to 0.0002957832437921642.\n",
      "Epoch 763/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00763: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1090 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00764: LearningRateScheduler reducing learning rate to 0.00029577217818626416.\n",
      "Epoch 764/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00764: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1095 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00765: LearningRateScheduler reducing learning rate to 0.0002957610982898716.\n",
      "Epoch 765/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 00765: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0785 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00766: LearningRateScheduler reducing learning rate to 0.00029575000410408394.\n",
      "Epoch 766/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00766: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1234 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00767: LearningRateScheduler reducing learning rate to 0.00029573889563000005.\n",
      "Epoch 767/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 00767: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1044 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00768: LearningRateScheduler reducing learning rate to 0.00029572777286872027.\n",
      "Epoch 768/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0859\n",
      "Epoch 00768: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0859 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00769: LearningRateScheduler reducing learning rate to 0.0002957166358213463.\n",
      "Epoch 769/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0930\n",
      "Epoch 00769: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0930 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00770: LearningRateScheduler reducing learning rate to 0.0002957054844889814.\n",
      "Epoch 770/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 00770: val_loss did not improve from 0.07336\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0774 - val_loss: 0.0747\n",
      "\n",
      "Epoch 00771: LearningRateScheduler reducing learning rate to 0.00029569431887273.\n",
      "Epoch 771/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 00771: val_loss improved from 0.07336 to 0.07231, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0898 - val_loss: 0.0723\n",
      "\n",
      "Epoch 00772: LearningRateScheduler reducing learning rate to 0.00029568313897369814.\n",
      "Epoch 772/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0835\n",
      "Epoch 00772: val_loss improved from 0.07231 to 0.07004, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0835 - val_loss: 0.0700\n",
      "\n",
      "Epoch 00773: LearningRateScheduler reducing learning rate to 0.00029567194479299325.\n",
      "Epoch 773/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00773: val_loss improved from 0.07004 to 0.06832, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.1168 - val_loss: 0.0683\n",
      "\n",
      "Epoch 00774: LearningRateScheduler reducing learning rate to 0.00029566073633172416.\n",
      "Epoch 774/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 00774: val_loss improved from 0.06832 to 0.06728, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0909 - val_loss: 0.0673\n",
      "\n",
      "Epoch 00775: LearningRateScheduler reducing learning rate to 0.000295649513591001.\n",
      "Epoch 775/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0850\n",
      "Epoch 00775: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0850 - val_loss: 0.0677\n",
      "\n",
      "Epoch 00776: LearningRateScheduler reducing learning rate to 0.0002956382765719354.\n",
      "Epoch 776/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00776: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0999 - val_loss: 0.0710\n",
      "\n",
      "Epoch 00777: LearningRateScheduler reducing learning rate to 0.0002956270252756405.\n",
      "Epoch 777/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00777: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1073 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00778: LearningRateScheduler reducing learning rate to 0.0002956157597032307.\n",
      "Epoch 778/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0842\n",
      "Epoch 00778: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0842 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00779: LearningRateScheduler reducing learning rate to 0.00029560447985582195.\n",
      "Epoch 779/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0844\n",
      "Epoch 00779: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0844 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00780: LearningRateScheduler reducing learning rate to 0.00029559318573453146.\n",
      "Epoch 780/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0842\n",
      "Epoch 00780: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0842 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00781: LearningRateScheduler reducing learning rate to 0.000295581877340478.\n",
      "Epoch 781/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00781: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0923 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00782: LearningRateScheduler reducing learning rate to 0.0002955705546747817.\n",
      "Epoch 782/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00782: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.1007 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00783: LearningRateScheduler reducing learning rate to 0.00029555921773856403.\n",
      "Epoch 783/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00783: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0963 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00784: LearningRateScheduler reducing learning rate to 0.00029554786653294805.\n",
      "Epoch 784/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 00784: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1066 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00785: LearningRateScheduler reducing learning rate to 0.000295536501059058.\n",
      "Epoch 785/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00785: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1093 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00786: LearningRateScheduler reducing learning rate to 0.0002955251213180197.\n",
      "Epoch 786/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0833\n",
      "Epoch 00786: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0833 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00787: LearningRateScheduler reducing learning rate to 0.0002955137273109604.\n",
      "Epoch 787/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0872\n",
      "Epoch 00787: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0872 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00788: LearningRateScheduler reducing learning rate to 0.00029550231903900865.\n",
      "Epoch 788/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0872\n",
      "Epoch 00788: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0872 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00789: LearningRateScheduler reducing learning rate to 0.00029549089650329443.\n",
      "Epoch 789/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00789: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0982 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00790: LearningRateScheduler reducing learning rate to 0.00029547945970494926.\n",
      "Epoch 790/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00790: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0986 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00791: LearningRateScheduler reducing learning rate to 0.00029546800864510586.\n",
      "Epoch 791/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0925\n",
      "Epoch 00791: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0925 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00792: LearningRateScheduler reducing learning rate to 0.0002954565433248986.\n",
      "Epoch 792/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0819\n",
      "Epoch 00792: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0819 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00793: LearningRateScheduler reducing learning rate to 0.0002954450637454631.\n",
      "Epoch 793/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 00793: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0884 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00794: LearningRateScheduler reducing learning rate to 0.00029543356990793644.\n",
      "Epoch 794/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0859\n",
      "Epoch 00794: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0859 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00795: LearningRateScheduler reducing learning rate to 0.00029542206181345707.\n",
      "Epoch 795/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 00795: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0909 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00796: LearningRateScheduler reducing learning rate to 0.00029541053946316497.\n",
      "Epoch 796/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00796: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0958 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00797: LearningRateScheduler reducing learning rate to 0.0002953990028582014.\n",
      "Epoch 797/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00797: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 0.0960 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00798: LearningRateScheduler reducing learning rate to 0.00029538745199970913.\n",
      "Epoch 798/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00798: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0943 - val_loss: 0.0738\n",
      "\n",
      "Epoch 00799: LearningRateScheduler reducing learning rate to 0.00029537588688883224.\n",
      "Epoch 799/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 00799: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.1055 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00800: LearningRateScheduler reducing learning rate to 0.0002953643075267164.\n",
      "Epoch 800/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0881\n",
      "Epoch 00800: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0881 - val_loss: 0.0731\n",
      "\n",
      "Epoch 00801: LearningRateScheduler reducing learning rate to 0.0002953527139145084.\n",
      "Epoch 801/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0841\n",
      "Epoch 00801: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0841 - val_loss: 0.0729\n",
      "\n",
      "Epoch 00802: LearningRateScheduler reducing learning rate to 0.0002953411060533567.\n",
      "Epoch 802/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 00802: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1052 - val_loss: 0.0732\n",
      "\n",
      "Epoch 00803: LearningRateScheduler reducing learning rate to 0.0002953294839444112.\n",
      "Epoch 803/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 00803: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0917 - val_loss: 0.0732\n",
      "\n",
      "Epoch 00804: LearningRateScheduler reducing learning rate to 0.0002953178475888228.\n",
      "Epoch 804/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0789\n",
      "Epoch 00804: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0789 - val_loss: 0.0733\n",
      "\n",
      "Epoch 00805: LearningRateScheduler reducing learning rate to 0.0002953061969877445.\n",
      "Epoch 805/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0810\n",
      "Epoch 00805: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0810 - val_loss: 0.0742\n",
      "\n",
      "Epoch 00806: LearningRateScheduler reducing learning rate to 0.00029529453214232997.\n",
      "Epoch 806/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 00806: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1020 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00807: LearningRateScheduler reducing learning rate to 0.0002952828530537348.\n",
      "Epoch 807/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0758\n",
      "Epoch 00807: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0758 - val_loss: 0.0735\n",
      "\n",
      "Epoch 00808: LearningRateScheduler reducing learning rate to 0.00029527115972311586.\n",
      "Epoch 808/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0830\n",
      "Epoch 00808: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0830 - val_loss: 0.0729\n",
      "\n",
      "Epoch 00809: LearningRateScheduler reducing learning rate to 0.0002952594521516313.\n",
      "Epoch 809/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0954\n",
      "Epoch 00809: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0954 - val_loss: 0.0714\n",
      "\n",
      "Epoch 00810: LearningRateScheduler reducing learning rate to 0.00029524773034044085.\n",
      "Epoch 810/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00810: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0993 - val_loss: 0.0696\n",
      "\n",
      "Epoch 00811: LearningRateScheduler reducing learning rate to 0.0002952359942907056.\n",
      "Epoch 811/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0800\n",
      "Epoch 00811: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0800 - val_loss: 0.0689\n",
      "\n",
      "Epoch 00812: LearningRateScheduler reducing learning rate to 0.00029522424400358797.\n",
      "Epoch 812/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0698\n",
      "Epoch 00812: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0698 - val_loss: 0.0693\n",
      "\n",
      "Epoch 00813: LearningRateScheduler reducing learning rate to 0.00029521247948025194.\n",
      "Epoch 813/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00813: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1008 - val_loss: 0.0697\n",
      "\n",
      "Epoch 00814: LearningRateScheduler reducing learning rate to 0.0002952007007218627.\n",
      "Epoch 814/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0860\n",
      "Epoch 00814: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0860 - val_loss: 0.0707\n",
      "\n",
      "Epoch 00815: LearningRateScheduler reducing learning rate to 0.000295188907729587.\n",
      "Epoch 815/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0931\n",
      "Epoch 00815: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0931 - val_loss: 0.0728\n",
      "\n",
      "Epoch 00816: LearningRateScheduler reducing learning rate to 0.0002951771005045931.\n",
      "Epoch 816/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 00816: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0899 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00817: LearningRateScheduler reducing learning rate to 0.0002951652790480503.\n",
      "Epoch 817/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0856\n",
      "Epoch 00817: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0856 - val_loss: 0.0750\n",
      "\n",
      "Epoch 00818: LearningRateScheduler reducing learning rate to 0.00029515344336112964.\n",
      "Epoch 818/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0803\n",
      "Epoch 00818: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0803 - val_loss: 0.0756\n",
      "\n",
      "Epoch 00819: LearningRateScheduler reducing learning rate to 0.0002951415934450036.\n",
      "Epoch 819/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00819: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0958 - val_loss: 0.0770\n",
      "\n",
      "Epoch 00820: LearningRateScheduler reducing learning rate to 0.0002951297293008458.\n",
      "Epoch 820/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0749\n",
      "Epoch 00820: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0749 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00821: LearningRateScheduler reducing learning rate to 0.0002951178509298314.\n",
      "Epoch 821/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 00821: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0918 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00822: LearningRateScheduler reducing learning rate to 0.00029510595833313706.\n",
      "Epoch 822/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00822: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1001 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00823: LearningRateScheduler reducing learning rate to 0.00029509405151194074.\n",
      "Epoch 823/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 00823: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1051 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00824: LearningRateScheduler reducing learning rate to 0.0002950821304674218.\n",
      "Epoch 824/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0861\n",
      "Epoch 00824: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0861 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00825: LearningRateScheduler reducing learning rate to 0.0002950701952007611.\n",
      "Epoch 825/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0835\n",
      "Epoch 00825: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0835 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00826: LearningRateScheduler reducing learning rate to 0.0002950582457131408.\n",
      "Epoch 826/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00826: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0923 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00827: LearningRateScheduler reducing learning rate to 0.0002950462820057446.\n",
      "Epoch 827/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0751\n",
      "Epoch 00827: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0751 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00828: LearningRateScheduler reducing learning rate to 0.0002950343040797575.\n",
      "Epoch 828/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 00828: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1034 - val_loss: 0.0741\n",
      "\n",
      "Epoch 00829: LearningRateScheduler reducing learning rate to 0.0002950223119363659.\n",
      "Epoch 829/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00829: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1067 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00830: LearningRateScheduler reducing learning rate to 0.0002950103055767577.\n",
      "Epoch 830/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0879\n",
      "Epoch 00830: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0879 - val_loss: 0.0722\n",
      "\n",
      "Epoch 00831: LearningRateScheduler reducing learning rate to 0.0002949982850021221.\n",
      "Epoch 831/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00831: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1111 - val_loss: 0.0717\n",
      "\n",
      "Epoch 00832: LearningRateScheduler reducing learning rate to 0.00029498625021364983.\n",
      "Epoch 832/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0857\n",
      "Epoch 00832: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0857 - val_loss: 0.0717\n",
      "\n",
      "Epoch 00833: LearningRateScheduler reducing learning rate to 0.00029497420121253295.\n",
      "Epoch 833/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0680\n",
      "Epoch 00833: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0680 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00834: LearningRateScheduler reducing learning rate to 0.00029496213799996493.\n",
      "Epoch 834/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0794\n",
      "Epoch 00834: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0794 - val_loss: 0.0717\n",
      "\n",
      "Epoch 00835: LearningRateScheduler reducing learning rate to 0.0002949500605771407.\n",
      "Epoch 835/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0759\n",
      "Epoch 00835: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0759 - val_loss: 0.0707\n",
      "\n",
      "Epoch 00836: LearningRateScheduler reducing learning rate to 0.00029493796894525644.\n",
      "Epoch 836/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 00836: val_loss did not improve from 0.06728\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0770 - val_loss: 0.0691\n",
      "\n",
      "Epoch 00837: LearningRateScheduler reducing learning rate to 0.00029492586310551.\n",
      "Epoch 837/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0816\n",
      "Epoch 00837: val_loss improved from 0.06728 to 0.06566, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0816 - val_loss: 0.0657\n",
      "\n",
      "Epoch 00838: LearningRateScheduler reducing learning rate to 0.00029491374305910036.\n",
      "Epoch 838/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0940\n",
      "Epoch 00838: val_loss improved from 0.06566 to 0.06356, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0940 - val_loss: 0.0636\n",
      "\n",
      "Epoch 00839: LearningRateScheduler reducing learning rate to 0.0002949016088072281.\n",
      "Epoch 839/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 00839: val_loss improved from 0.06356 to 0.06279, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0903 - val_loss: 0.0628\n",
      "\n",
      "Epoch 00840: LearningRateScheduler reducing learning rate to 0.0002948894603510952.\n",
      "Epoch 840/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00840: val_loss improved from 0.06279 to 0.06215, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0823 - val_loss: 0.0622\n",
      "\n",
      "Epoch 00841: LearningRateScheduler reducing learning rate to 0.0002948772976919049.\n",
      "Epoch 841/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0871\n",
      "Epoch 00841: val_loss improved from 0.06215 to 0.06118, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0871 - val_loss: 0.0612\n",
      "\n",
      "Epoch 00842: LearningRateScheduler reducing learning rate to 0.00029486512083086203.\n",
      "Epoch 842/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0872\n",
      "Epoch 00842: val_loss improved from 0.06118 to 0.06118, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0872 - val_loss: 0.0612\n",
      "\n",
      "Epoch 00843: LearningRateScheduler reducing learning rate to 0.00029485292976917264.\n",
      "Epoch 843/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00843: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.1007 - val_loss: 0.0636\n",
      "\n",
      "Epoch 00844: LearningRateScheduler reducing learning rate to 0.0002948407245080443.\n",
      "Epoch 844/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00844: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0961 - val_loss: 0.0659\n",
      "\n",
      "Epoch 00845: LearningRateScheduler reducing learning rate to 0.000294828505048686.\n",
      "Epoch 845/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0799\n",
      "Epoch 00845: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0799 - val_loss: 0.0682\n",
      "\n",
      "Epoch 00846: LearningRateScheduler reducing learning rate to 0.00029481627139230814.\n",
      "Epoch 846/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0771\n",
      "Epoch 00846: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0771 - val_loss: 0.0696\n",
      "\n",
      "Epoch 00847: LearningRateScheduler reducing learning rate to 0.0002948040235401225.\n",
      "Epoch 847/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0676\n",
      "Epoch 00847: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0676 - val_loss: 0.0706\n",
      "\n",
      "Epoch 00848: LearningRateScheduler reducing learning rate to 0.0002947917614933421.\n",
      "Epoch 848/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0929\n",
      "Epoch 00848: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0929 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00849: LearningRateScheduler reducing learning rate to 0.00029477948525318166.\n",
      "Epoch 849/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0858\n",
      "Epoch 00849: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0858 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00850: LearningRateScheduler reducing learning rate to 0.00029476719482085723.\n",
      "Epoch 850/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00850: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0985 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00851: LearningRateScheduler reducing learning rate to 0.0002947548901975861.\n",
      "Epoch 851/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 00851: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0885 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00852: LearningRateScheduler reducing learning rate to 0.000294742571384587.\n",
      "Epoch 852/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0832\n",
      "Epoch 00852: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0832 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00853: LearningRateScheduler reducing learning rate to 0.0002947302383830803.\n",
      "Epoch 853/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00853: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.1004 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00854: LearningRateScheduler reducing learning rate to 0.0002947178911942876.\n",
      "Epoch 854/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 00854: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0934 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00855: LearningRateScheduler reducing learning rate to 0.00029470552981943174.\n",
      "Epoch 855/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00855: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0962 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00856: LearningRateScheduler reducing learning rate to 0.0002946931542597373.\n",
      "Epoch 856/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 00856: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0773 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00857: LearningRateScheduler reducing learning rate to 0.0002946807645164301.\n",
      "Epoch 857/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00857: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1024 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00858: LearningRateScheduler reducing learning rate to 0.0002946683605907374.\n",
      "Epoch 858/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0911\n",
      "Epoch 00858: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0911 - val_loss: 0.0733\n",
      "\n",
      "Epoch 00859: LearningRateScheduler reducing learning rate to 0.00029465594248388765.\n",
      "Epoch 859/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0908\n",
      "Epoch 00859: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0908 - val_loss: 0.0708\n",
      "\n",
      "Epoch 00860: LearningRateScheduler reducing learning rate to 0.0002946435101971111.\n",
      "Epoch 860/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 00860: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0898 - val_loss: 0.0702\n",
      "\n",
      "Epoch 00861: LearningRateScheduler reducing learning rate to 0.0002946310637316391.\n",
      "Epoch 861/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0877\n",
      "Epoch 00861: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0877 - val_loss: 0.0708\n",
      "\n",
      "Epoch 00862: LearningRateScheduler reducing learning rate to 0.00029461860308870454.\n",
      "Epoch 862/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0743\n",
      "Epoch 00862: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0743 - val_loss: 0.0728\n",
      "\n",
      "Epoch 00863: LearningRateScheduler reducing learning rate to 0.0002946061282695416.\n",
      "Epoch 863/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0861\n",
      "Epoch 00863: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0861 - val_loss: 0.0743\n",
      "\n",
      "Epoch 00864: LearningRateScheduler reducing learning rate to 0.0002945936392753861.\n",
      "Epoch 864/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 00864: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0885 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00865: LearningRateScheduler reducing learning rate to 0.000294581136107475.\n",
      "Epoch 865/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0796\n",
      "Epoch 00865: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0796 - val_loss: 0.0770\n",
      "\n",
      "Epoch 00866: LearningRateScheduler reducing learning rate to 0.00029456861876704665.\n",
      "Epoch 866/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00866: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.1092 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00867: LearningRateScheduler reducing learning rate to 0.0002945560872553411.\n",
      "Epoch 867/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0850\n",
      "Epoch 00867: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0850 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00868: LearningRateScheduler reducing learning rate to 0.0002945435415735996.\n",
      "Epoch 868/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0837\n",
      "Epoch 00868: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0837 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00869: LearningRateScheduler reducing learning rate to 0.0002945309817230647.\n",
      "Epoch 869/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0863\n",
      "Epoch 00869: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0863 - val_loss: 0.0802\n",
      "\n",
      "Epoch 00870: LearningRateScheduler reducing learning rate to 0.00029451840770498063.\n",
      "Epoch 870/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 00870: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0768 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00871: LearningRateScheduler reducing learning rate to 0.00029450581952059285.\n",
      "Epoch 871/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0833\n",
      "Epoch 00871: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0833 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00872: LearningRateScheduler reducing learning rate to 0.0002944932171711482.\n",
      "Epoch 872/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0813\n",
      "Epoch 00872: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0813 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00873: LearningRateScheduler reducing learning rate to 0.0002944806006578949.\n",
      "Epoch 873/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0931\n",
      "Epoch 00873: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0931 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00874: LearningRateScheduler reducing learning rate to 0.00029446796998208285.\n",
      "Epoch 874/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 00874: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.1076 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00875: LearningRateScheduler reducing learning rate to 0.00029445532514496304.\n",
      "Epoch 875/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0817\n",
      "Epoch 00875: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0817 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00876: LearningRateScheduler reducing learning rate to 0.00029444266614778793.\n",
      "Epoch 876/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0914\n",
      "Epoch 00876: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0914 - val_loss: 0.0737\n",
      "\n",
      "Epoch 00877: LearningRateScheduler reducing learning rate to 0.0002944299929918114.\n",
      "Epoch 877/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0799\n",
      "Epoch 00877: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0799 - val_loss: 0.0715\n",
      "\n",
      "Epoch 00878: LearningRateScheduler reducing learning rate to 0.0002944173056782889.\n",
      "Epoch 878/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 00878: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0909 - val_loss: 0.0696\n",
      "\n",
      "Epoch 00879: LearningRateScheduler reducing learning rate to 0.000294404604208477.\n",
      "Epoch 879/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0862\n",
      "Epoch 00879: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0862 - val_loss: 0.0681\n",
      "\n",
      "Epoch 00880: LearningRateScheduler reducing learning rate to 0.0002943918885836339.\n",
      "Epoch 880/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0894\n",
      "Epoch 00880: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0894 - val_loss: 0.0675\n",
      "\n",
      "Epoch 00881: LearningRateScheduler reducing learning rate to 0.00029437915880501904.\n",
      "Epoch 881/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00881: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.1010 - val_loss: 0.0687\n",
      "\n",
      "Epoch 00882: LearningRateScheduler reducing learning rate to 0.00029436641487389333.\n",
      "Epoch 882/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00882: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0988 - val_loss: 0.0713\n",
      "\n",
      "Epoch 00883: LearningRateScheduler reducing learning rate to 0.0002943536567915192.\n",
      "Epoch 883/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0931\n",
      "Epoch 00883: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0931 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00884: LearningRateScheduler reducing learning rate to 0.0002943408845591602.\n",
      "Epoch 884/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0858\n",
      "Epoch 00884: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0858 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00885: LearningRateScheduler reducing learning rate to 0.00029432809817808166.\n",
      "Epoch 885/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0912\n",
      "Epoch 00885: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0912 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00886: LearningRateScheduler reducing learning rate to 0.0002943152976495499.\n",
      "Epoch 886/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00886: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0972 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00887: LearningRateScheduler reducing learning rate to 0.0002943024829748329.\n",
      "Epoch 887/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 00887: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0785 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00888: LearningRateScheduler reducing learning rate to 0.00029428965415520005.\n",
      "Epoch 888/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 00888: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0768 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00889: LearningRateScheduler reducing learning rate to 0.000294276811191922.\n",
      "Epoch 889/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 00889: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0948 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00890: LearningRateScheduler reducing learning rate to 0.00029426395408627094.\n",
      "Epoch 890/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0833\n",
      "Epoch 00890: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0833 - val_loss: 0.0742\n",
      "\n",
      "Epoch 00891: LearningRateScheduler reducing learning rate to 0.00029425108283952033.\n",
      "Epoch 891/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0810\n",
      "Epoch 00891: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0810 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00892: LearningRateScheduler reducing learning rate to 0.0002942381974529451.\n",
      "Epoch 892/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00892: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0823 - val_loss: 0.0714\n",
      "\n",
      "Epoch 00893: LearningRateScheduler reducing learning rate to 0.00029422529792782164.\n",
      "Epoch 893/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0870\n",
      "Epoch 00893: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0870 - val_loss: 0.0710\n",
      "\n",
      "Epoch 00894: LearningRateScheduler reducing learning rate to 0.00029421238426542766.\n",
      "Epoch 894/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0766\n",
      "Epoch 00894: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0766 - val_loss: 0.0703\n",
      "\n",
      "Epoch 00895: LearningRateScheduler reducing learning rate to 0.00029419945646704226.\n",
      "Epoch 895/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0900\n",
      "Epoch 00895: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0900 - val_loss: 0.0692\n",
      "\n",
      "Epoch 00896: LearningRateScheduler reducing learning rate to 0.00029418651453394593.\n",
      "Epoch 896/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 00896: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0941 - val_loss: 0.0696\n",
      "\n",
      "Epoch 00897: LearningRateScheduler reducing learning rate to 0.0002941735584674207.\n",
      "Epoch 897/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00897: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0823 - val_loss: 0.0701\n",
      "\n",
      "Epoch 00898: LearningRateScheduler reducing learning rate to 0.0002941605882687498.\n",
      "Epoch 898/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0864\n",
      "Epoch 00898: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0864 - val_loss: 0.0716\n",
      "\n",
      "Epoch 00899: LearningRateScheduler reducing learning rate to 0.00029414760393921805.\n",
      "Epoch 899/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0761\n",
      "Epoch 00899: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0761 - val_loss: 0.0710\n",
      "\n",
      "Epoch 00900: LearningRateScheduler reducing learning rate to 0.0002941346054801115.\n",
      "Epoch 900/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00900: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0961 - val_loss: 0.0696\n",
      "\n",
      "Epoch 00901: LearningRateScheduler reducing learning rate to 0.0002941215928927177.\n",
      "Epoch 901/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 00901: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0774 - val_loss: 0.0695\n",
      "\n",
      "Epoch 00902: LearningRateScheduler reducing learning rate to 0.0002941085661783256.\n",
      "Epoch 902/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 00902: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0888 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00903: LearningRateScheduler reducing learning rate to 0.0002940955253382255.\n",
      "Epoch 903/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0713\n",
      "Epoch 00903: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0713 - val_loss: 0.0733\n",
      "\n",
      "Epoch 00904: LearningRateScheduler reducing learning rate to 0.00029408247037370915.\n",
      "Epoch 904/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0847\n",
      "Epoch 00904: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0847 - val_loss: 0.0750\n",
      "\n",
      "Epoch 00905: LearningRateScheduler reducing learning rate to 0.0002940694012860697.\n",
      "Epoch 905/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00905: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0983 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00906: LearningRateScheduler reducing learning rate to 0.0002940563180766016.\n",
      "Epoch 906/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0674\n",
      "Epoch 00906: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0674 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00907: LearningRateScheduler reducing learning rate to 0.00029404322074660076.\n",
      "Epoch 907/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0862\n",
      "Epoch 00907: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0862 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00908: LearningRateScheduler reducing learning rate to 0.0002940301092973646.\n",
      "Epoch 908/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0924\n",
      "Epoch 00908: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0924 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00909: LearningRateScheduler reducing learning rate to 0.0002940169837301918.\n",
      "Epoch 909/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0803\n",
      "Epoch 00909: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0803 - val_loss: 0.0875\n",
      "\n",
      "Epoch 00910: LearningRateScheduler reducing learning rate to 0.0002940038440463824.\n",
      "Epoch 910/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 00910: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0785 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00911: LearningRateScheduler reducing learning rate to 0.00029399069024723805.\n",
      "Epoch 911/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0901\n",
      "Epoch 00911: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0901 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00912: LearningRateScheduler reducing learning rate to 0.00029397752233406154.\n",
      "Epoch 912/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0856\n",
      "Epoch 00912: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0856 - val_loss: 0.0763\n",
      "\n",
      "Epoch 00913: LearningRateScheduler reducing learning rate to 0.0002939643403081572.\n",
      "Epoch 913/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0879\n",
      "Epoch 00913: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0879 - val_loss: 0.0742\n",
      "\n",
      "Epoch 00914: LearningRateScheduler reducing learning rate to 0.00029395114417083085.\n",
      "Epoch 914/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0834\n",
      "Epoch 00914: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0834 - val_loss: 0.0766\n",
      "\n",
      "Epoch 00915: LearningRateScheduler reducing learning rate to 0.0002939379339233895.\n",
      "Epoch 915/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0820\n",
      "Epoch 00915: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0820 - val_loss: 0.0800\n",
      "\n",
      "Epoch 00916: LearningRateScheduler reducing learning rate to 0.00029392470956714174.\n",
      "Epoch 916/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00916: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.1028 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00917: LearningRateScheduler reducing learning rate to 0.00029391147110339734.\n",
      "Epoch 917/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0838\n",
      "Epoch 00917: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0838 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00918: LearningRateScheduler reducing learning rate to 0.0002938982185334677.\n",
      "Epoch 918/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0763\n",
      "Epoch 00918: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0763 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00919: LearningRateScheduler reducing learning rate to 0.00029388495185866546.\n",
      "Epoch 919/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0736\n",
      "Epoch 00919: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0736 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00920: LearningRateScheduler reducing learning rate to 0.0002938716710803048.\n",
      "Epoch 920/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0824\n",
      "Epoch 00920: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0824 - val_loss: 0.0669\n",
      "\n",
      "Epoch 00921: LearningRateScheduler reducing learning rate to 0.00029385837619970114.\n",
      "Epoch 921/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 00921: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0774 - val_loss: 0.0644\n",
      "\n",
      "Epoch 00922: LearningRateScheduler reducing learning rate to 0.00029384506721817134.\n",
      "Epoch 922/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0799\n",
      "Epoch 00922: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0799 - val_loss: 0.0637\n",
      "\n",
      "Epoch 00923: LearningRateScheduler reducing learning rate to 0.0002938317441370338.\n",
      "Epoch 923/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00923: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0993 - val_loss: 0.0636\n",
      "\n",
      "Epoch 00924: LearningRateScheduler reducing learning rate to 0.00029381840695760807.\n",
      "Epoch 924/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0844\n",
      "Epoch 00924: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0844 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00925: LearningRateScheduler reducing learning rate to 0.0002938050556812153.\n",
      "Epoch 925/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0906\n",
      "Epoch 00925: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0906 - val_loss: 0.0662\n",
      "\n",
      "Epoch 00926: LearningRateScheduler reducing learning rate to 0.000293791690309178.\n",
      "Epoch 926/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0881\n",
      "Epoch 00926: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0881 - val_loss: 0.0677\n",
      "\n",
      "Epoch 00927: LearningRateScheduler reducing learning rate to 0.00029377831084282.\n",
      "Epoch 927/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0816\n",
      "Epoch 00927: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0816 - val_loss: 0.0678\n",
      "\n",
      "Epoch 00928: LearningRateScheduler reducing learning rate to 0.00029376491728346654.\n",
      "Epoch 928/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0653\n",
      "Epoch 00928: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0653 - val_loss: 0.0679\n",
      "\n",
      "Epoch 00929: LearningRateScheduler reducing learning rate to 0.0002937515096324443.\n",
      "Epoch 929/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0765\n",
      "Epoch 00929: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0765 - val_loss: 0.0671\n",
      "\n",
      "Epoch 00930: LearningRateScheduler reducing learning rate to 0.00029373808789108133.\n",
      "Epoch 930/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0643\n",
      "Epoch 00930: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0643 - val_loss: 0.0666\n",
      "\n",
      "Epoch 00931: LearningRateScheduler reducing learning rate to 0.0002937246520607071.\n",
      "Epoch 931/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0669\n",
      "Epoch 00931: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0669 - val_loss: 0.0655\n",
      "\n",
      "Epoch 00932: LearningRateScheduler reducing learning rate to 0.0002937112021426525.\n",
      "Epoch 932/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0653\n",
      "Epoch 00932: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0653 - val_loss: 0.0657\n",
      "\n",
      "Epoch 00933: LearningRateScheduler reducing learning rate to 0.0002936977381382498.\n",
      "Epoch 933/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0833\n",
      "Epoch 00933: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0833 - val_loss: 0.0676\n",
      "\n",
      "Epoch 00934: LearningRateScheduler reducing learning rate to 0.00029368426004883246.\n",
      "Epoch 934/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0846\n",
      "Epoch 00934: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0846 - val_loss: 0.0697\n",
      "\n",
      "Epoch 00935: LearningRateScheduler reducing learning rate to 0.00029367076787573565.\n",
      "Epoch 935/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0892\n",
      "Epoch 00935: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0892 - val_loss: 0.0723\n",
      "\n",
      "Epoch 00936: LearningRateScheduler reducing learning rate to 0.00029365726162029586.\n",
      "Epoch 936/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0791\n",
      "Epoch 00936: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0791 - val_loss: 0.0730\n",
      "\n",
      "Epoch 00937: LearningRateScheduler reducing learning rate to 0.00029364374128385077.\n",
      "Epoch 937/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0835\n",
      "Epoch 00937: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0835 - val_loss: 0.0722\n",
      "\n",
      "Epoch 00938: LearningRateScheduler reducing learning rate to 0.0002936302068677397.\n",
      "Epoch 938/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0750\n",
      "Epoch 00938: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0750 - val_loss: 0.0722\n",
      "\n",
      "Epoch 00939: LearningRateScheduler reducing learning rate to 0.0002936166583733032.\n",
      "Epoch 939/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0910\n",
      "Epoch 00939: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0910 - val_loss: 0.0740\n",
      "\n",
      "Epoch 00940: LearningRateScheduler reducing learning rate to 0.00029360309580188333.\n",
      "Epoch 940/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00940: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0972 - val_loss: 0.0761\n",
      "\n",
      "Epoch 00941: LearningRateScheduler reducing learning rate to 0.0002935895191548235.\n",
      "Epoch 941/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0741\n",
      "Epoch 00941: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0741 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00942: LearningRateScheduler reducing learning rate to 0.0002935759284334685.\n",
      "Epoch 942/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00942: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0958 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00943: LearningRateScheduler reducing learning rate to 0.0002935623236391645.\n",
      "Epoch 943/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0836\n",
      "Epoch 00943: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0836 - val_loss: 0.0683\n",
      "\n",
      "Epoch 00944: LearningRateScheduler reducing learning rate to 0.0002935487047732591.\n",
      "Epoch 944/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0916\n",
      "Epoch 00944: val_loss did not improve from 0.06118\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0916 - val_loss: 0.0624\n",
      "\n",
      "Epoch 00945: LearningRateScheduler reducing learning rate to 0.0002935350718371012.\n",
      "Epoch 945/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0668\n",
      "Epoch 00945: val_loss improved from 0.06118 to 0.05908, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0668 - val_loss: 0.0591\n",
      "\n",
      "Epoch 00946: LearningRateScheduler reducing learning rate to 0.00029352142483204133.\n",
      "Epoch 946/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0818\n",
      "Epoch 00946: val_loss improved from 0.05908 to 0.05685, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0818 - val_loss: 0.0569\n",
      "\n",
      "Epoch 00947: LearningRateScheduler reducing learning rate to 0.00029350776375943117.\n",
      "Epoch 947/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0847\n",
      "Epoch 00947: val_loss improved from 0.05685 to 0.05521, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0847 - val_loss: 0.0552\n",
      "\n",
      "Epoch 00948: LearningRateScheduler reducing learning rate to 0.0002934940886206239.\n",
      "Epoch 948/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0715\n",
      "Epoch 00948: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0715 - val_loss: 0.0595\n",
      "\n",
      "Epoch 00949: LearningRateScheduler reducing learning rate to 0.00029348039941697404.\n",
      "Epoch 949/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 00949: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0802 - val_loss: 0.0628\n",
      "\n",
      "Epoch 00950: LearningRateScheduler reducing learning rate to 0.0002934666961498375.\n",
      "Epoch 950/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0797\n",
      "Epoch 00950: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0797 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00951: LearningRateScheduler reducing learning rate to 0.0002934529788205718.\n",
      "Epoch 951/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 00951: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0997 - val_loss: 0.0639\n",
      "\n",
      "Epoch 00952: LearningRateScheduler reducing learning rate to 0.0002934392474305355.\n",
      "Epoch 952/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0928\n",
      "Epoch 00952: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0928 - val_loss: 0.0635\n",
      "\n",
      "Epoch 00953: LearningRateScheduler reducing learning rate to 0.0002934255019810888.\n",
      "Epoch 953/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00953: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0965 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00954: LearningRateScheduler reducing learning rate to 0.0002934117424735931.\n",
      "Epoch 954/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0845\n",
      "Epoch 00954: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0845 - val_loss: 0.0670\n",
      "\n",
      "Epoch 00955: LearningRateScheduler reducing learning rate to 0.0002933979689094115.\n",
      "Epoch 955/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0858\n",
      "Epoch 00955: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0858 - val_loss: 0.0686\n",
      "\n",
      "Epoch 00956: LearningRateScheduler reducing learning rate to 0.0002933841812899082.\n",
      "Epoch 956/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0660\n",
      "Epoch 00956: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0660 - val_loss: 0.0690\n",
      "\n",
      "Epoch 00957: LearningRateScheduler reducing learning rate to 0.0002933703796164489.\n",
      "Epoch 957/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0555\n",
      "Epoch 00957: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0555 - val_loss: 0.0661\n",
      "\n",
      "Epoch 00958: LearningRateScheduler reducing learning rate to 0.00029335656389040067.\n",
      "Epoch 958/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0695\n",
      "Epoch 00958: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0695 - val_loss: 0.0625\n",
      "\n",
      "Epoch 00959: LearningRateScheduler reducing learning rate to 0.0002933427341131321.\n",
      "Epoch 959/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0626\n",
      "Epoch 00959: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0626 - val_loss: 0.0632\n",
      "\n",
      "Epoch 00960: LearningRateScheduler reducing learning rate to 0.0002933288902860129.\n",
      "Epoch 960/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0722\n",
      "Epoch 00960: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0722 - val_loss: 0.0629\n",
      "\n",
      "Epoch 00961: LearningRateScheduler reducing learning rate to 0.00029331503241041446.\n",
      "Epoch 961/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0680\n",
      "Epoch 00961: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0680 - val_loss: 0.0625\n",
      "\n",
      "Epoch 00962: LearningRateScheduler reducing learning rate to 0.0002933011604877093.\n",
      "Epoch 962/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0714\n",
      "Epoch 00962: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0714 - val_loss: 0.0593\n",
      "\n",
      "Epoch 00963: LearningRateScheduler reducing learning rate to 0.0002932872745192716.\n",
      "Epoch 963/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0814\n",
      "Epoch 00963: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0814 - val_loss: 0.0572\n",
      "\n",
      "Epoch 00964: LearningRateScheduler reducing learning rate to 0.0002932733745064768.\n",
      "Epoch 964/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0805\n",
      "Epoch 00964: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0805 - val_loss: 0.0586\n",
      "\n",
      "Epoch 00965: LearningRateScheduler reducing learning rate to 0.0002932594604507016.\n",
      "Epoch 965/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0719\n",
      "Epoch 00965: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0719 - val_loss: 0.0607\n",
      "\n",
      "Epoch 00966: LearningRateScheduler reducing learning rate to 0.0002932455323533243.\n",
      "Epoch 966/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0804\n",
      "Epoch 00966: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0804 - val_loss: 0.0633\n",
      "\n",
      "Epoch 00967: LearningRateScheduler reducing learning rate to 0.0002932315902157245.\n",
      "Epoch 967/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0854\n",
      "Epoch 00967: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0854 - val_loss: 0.0633\n",
      "\n",
      "Epoch 00968: LearningRateScheduler reducing learning rate to 0.0002932176340392833.\n",
      "Epoch 968/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00968: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0923 - val_loss: 0.0626\n",
      "\n",
      "Epoch 00969: LearningRateScheduler reducing learning rate to 0.0002932036638253829.\n",
      "Epoch 969/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0650\n",
      "Epoch 00969: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0650 - val_loss: 0.0609\n",
      "\n",
      "Epoch 00970: LearningRateScheduler reducing learning rate to 0.00029318967957540725.\n",
      "Epoch 970/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 00970: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0770 - val_loss: 0.0604\n",
      "\n",
      "Epoch 00971: LearningRateScheduler reducing learning rate to 0.0002931756812907414.\n",
      "Epoch 971/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0630\n",
      "Epoch 00971: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0630 - val_loss: 0.0597\n",
      "\n",
      "Epoch 00972: LearningRateScheduler reducing learning rate to 0.000293161668972772.\n",
      "Epoch 972/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 00972: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0773 - val_loss: 0.0575\n",
      "\n",
      "Epoch 00973: LearningRateScheduler reducing learning rate to 0.00029314764262288693.\n",
      "Epoch 973/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0813\n",
      "Epoch 00973: val_loss did not improve from 0.05521\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0813 - val_loss: 0.0556\n",
      "\n",
      "Epoch 00974: LearningRateScheduler reducing learning rate to 0.00029313360224247556.\n",
      "Epoch 974/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0849\n",
      "Epoch 00974: val_loss improved from 0.05521 to 0.05514, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0849 - val_loss: 0.0551\n",
      "\n",
      "Epoch 00975: LearningRateScheduler reducing learning rate to 0.00029311954783292865.\n",
      "Epoch 975/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 00975: val_loss improved from 0.05514 to 0.05274, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.1054 - val_loss: 0.0527\n",
      "\n",
      "Epoch 00976: LearningRateScheduler reducing learning rate to 0.0002931054793956383.\n",
      "Epoch 976/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0795\n",
      "Epoch 00976: val_loss improved from 0.05274 to 0.05197, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0795 - val_loss: 0.0520\n",
      "\n",
      "Epoch 00977: LearningRateScheduler reducing learning rate to 0.00029309139693199806.\n",
      "Epoch 977/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 00977: val_loss improved from 0.05197 to 0.05138, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0917 - val_loss: 0.0514\n",
      "\n",
      "Epoch 00978: LearningRateScheduler reducing learning rate to 0.0002930773004434027.\n",
      "Epoch 978/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0756\n",
      "Epoch 00978: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0756 - val_loss: 0.0518\n",
      "\n",
      "Epoch 00979: LearningRateScheduler reducing learning rate to 0.0002930631899312487.\n",
      "Epoch 979/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0727\n",
      "Epoch 00979: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0727 - val_loss: 0.0532\n",
      "\n",
      "Epoch 00980: LearningRateScheduler reducing learning rate to 0.00029304906539693367.\n",
      "Epoch 980/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0749\n",
      "Epoch 00980: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0749 - val_loss: 0.0545\n",
      "\n",
      "Epoch 00981: LearningRateScheduler reducing learning rate to 0.0002930349268418566.\n",
      "Epoch 981/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 00981: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0772 - val_loss: 0.0552\n",
      "\n",
      "Epoch 00982: LearningRateScheduler reducing learning rate to 0.000293020774267418.\n",
      "Epoch 982/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0569\n",
      "Epoch 00982: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0569 - val_loss: 0.0562\n",
      "\n",
      "Epoch 00983: LearningRateScheduler reducing learning rate to 0.0002930066076750197.\n",
      "Epoch 983/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0843\n",
      "Epoch 00983: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0843 - val_loss: 0.0561\n",
      "\n",
      "Epoch 00984: LearningRateScheduler reducing learning rate to 0.0002929924270660649.\n",
      "Epoch 984/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0806\n",
      "Epoch 00984: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0806 - val_loss: 0.0557\n",
      "\n",
      "Epoch 00985: LearningRateScheduler reducing learning rate to 0.00029297823244195837.\n",
      "Epoch 985/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0714\n",
      "Epoch 00985: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0714 - val_loss: 0.0543\n",
      "\n",
      "Epoch 00986: LearningRateScheduler reducing learning rate to 0.000292964023804106.\n",
      "Epoch 986/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0657\n",
      "Epoch 00986: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 0.0657 - val_loss: 0.0539\n",
      "\n",
      "Epoch 00987: LearningRateScheduler reducing learning rate to 0.0002929498011539151.\n",
      "Epoch 987/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0726\n",
      "Epoch 00987: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0726 - val_loss: 0.0533\n",
      "\n",
      "Epoch 00988: LearningRateScheduler reducing learning rate to 0.00029293556449279466.\n",
      "Epoch 988/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0861\n",
      "Epoch 00988: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0861 - val_loss: 0.0535\n",
      "\n",
      "Epoch 00989: LearningRateScheduler reducing learning rate to 0.00029292131382215476.\n",
      "Epoch 989/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 00989: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0909 - val_loss: 0.0527\n",
      "\n",
      "Epoch 00990: LearningRateScheduler reducing learning rate to 0.0002929070491434069.\n",
      "Epoch 990/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0721\n",
      "Epoch 00990: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0721 - val_loss: 0.0514\n",
      "\n",
      "Epoch 00991: LearningRateScheduler reducing learning rate to 0.0002928927704579642.\n",
      "Epoch 991/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0717\n",
      "Epoch 00991: val_loss did not improve from 0.05138\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0717 - val_loss: 0.0514\n",
      "\n",
      "Epoch 00992: LearningRateScheduler reducing learning rate to 0.0002928784777672408.\n",
      "Epoch 992/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0709\n",
      "Epoch 00992: val_loss improved from 0.05138 to 0.05075, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0709 - val_loss: 0.0508\n",
      "\n",
      "Epoch 00993: LearningRateScheduler reducing learning rate to 0.0002928641710726525.\n",
      "Epoch 993/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0725\n",
      "Epoch 00993: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0725 - val_loss: 0.0510\n",
      "\n",
      "Epoch 00994: LearningRateScheduler reducing learning rate to 0.00029284985037561647.\n",
      "Epoch 994/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0853\n",
      "Epoch 00994: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0853 - val_loss: 0.0516\n",
      "\n",
      "Epoch 00995: LearningRateScheduler reducing learning rate to 0.00029283551567755114.\n",
      "Epoch 995/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0663\n",
      "Epoch 00995: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0663 - val_loss: 0.0521\n",
      "\n",
      "Epoch 00996: LearningRateScheduler reducing learning rate to 0.0002928211669798764.\n",
      "Epoch 996/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0871\n",
      "Epoch 00996: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0871 - val_loss: 0.0547\n",
      "\n",
      "Epoch 00997: LearningRateScheduler reducing learning rate to 0.0002928068042840136.\n",
      "Epoch 997/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0871\n",
      "Epoch 00997: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0871 - val_loss: 0.0580\n",
      "\n",
      "Epoch 00998: LearningRateScheduler reducing learning rate to 0.0002927924275913852.\n",
      "Epoch 998/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0744\n",
      "Epoch 00998: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0744 - val_loss: 0.0577\n",
      "\n",
      "Epoch 00999: LearningRateScheduler reducing learning rate to 0.0002927780369034155.\n",
      "Epoch 999/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0750\n",
      "Epoch 00999: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0750 - val_loss: 0.0566\n",
      "\n",
      "Epoch 01000: LearningRateScheduler reducing learning rate to 0.00029276363222152977.\n",
      "Epoch 1000/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0638\n",
      "Epoch 01000: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0638 - val_loss: 0.0561\n",
      "\n",
      "Epoch 01001: LearningRateScheduler reducing learning rate to 0.00029274921354715487.\n",
      "Epoch 1001/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0699\n",
      "Epoch 01001: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0699 - val_loss: 0.0583\n",
      "\n",
      "Epoch 01002: LearningRateScheduler reducing learning rate to 0.000292734780881719.\n",
      "Epoch 1002/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0817\n",
      "Epoch 01002: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0817 - val_loss: 0.0594\n",
      "\n",
      "Epoch 01003: LearningRateScheduler reducing learning rate to 0.00029272033422665173.\n",
      "Epoch 1003/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0707\n",
      "Epoch 01003: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0707 - val_loss: 0.0578\n",
      "\n",
      "Epoch 01004: LearningRateScheduler reducing learning rate to 0.00029270587358338405.\n",
      "Epoch 1004/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 01004: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0948 - val_loss: 0.0562\n",
      "\n",
      "Epoch 01005: LearningRateScheduler reducing learning rate to 0.00029269139895334836.\n",
      "Epoch 1005/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0755\n",
      "Epoch 01005: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0755 - val_loss: 0.0555\n",
      "\n",
      "Epoch 01006: LearningRateScheduler reducing learning rate to 0.00029267691033797835.\n",
      "Epoch 1006/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0790\n",
      "Epoch 01006: val_loss did not improve from 0.05075\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0790 - val_loss: 0.0541\n",
      "\n",
      "Epoch 01007: LearningRateScheduler reducing learning rate to 0.0002926624077387092.\n",
      "Epoch 1007/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0534\n",
      "Epoch 01007: val_loss improved from 0.05075 to 0.05031, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0534 - val_loss: 0.0503\n",
      "\n",
      "Epoch 01008: LearningRateScheduler reducing learning rate to 0.00029264789115697736.\n",
      "Epoch 1008/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0713\n",
      "Epoch 01008: val_loss improved from 0.05031 to 0.04558, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0713 - val_loss: 0.0456\n",
      "\n",
      "Epoch 01009: LearningRateScheduler reducing learning rate to 0.0002926333605942208.\n",
      "Epoch 1009/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0751\n",
      "Epoch 01009: val_loss improved from 0.04558 to 0.04237, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0751 - val_loss: 0.0424\n",
      "\n",
      "Epoch 01010: LearningRateScheduler reducing learning rate to 0.0002926188160518787.\n",
      "Epoch 1010/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0662\n",
      "Epoch 01010: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0662 - val_loss: 0.0427\n",
      "\n",
      "Epoch 01011: LearningRateScheduler reducing learning rate to 0.00029260425753139185.\n",
      "Epoch 1011/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0635\n",
      "Epoch 01011: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0635 - val_loss: 0.0431\n",
      "\n",
      "Epoch 01012: LearningRateScheduler reducing learning rate to 0.00029258968503420227.\n",
      "Epoch 1012/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0769\n",
      "Epoch 01012: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0769 - val_loss: 0.0428\n",
      "\n",
      "Epoch 01013: LearningRateScheduler reducing learning rate to 0.0002925750985617534.\n",
      "Epoch 1013/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0756\n",
      "Epoch 01013: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0756 - val_loss: 0.0438\n",
      "\n",
      "Epoch 01014: LearningRateScheduler reducing learning rate to 0.00029256049811549.\n",
      "Epoch 1014/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0843\n",
      "Epoch 01014: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0843 - val_loss: 0.0471\n",
      "\n",
      "Epoch 01015: LearningRateScheduler reducing learning rate to 0.0002925458836968584.\n",
      "Epoch 1015/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0762\n",
      "Epoch 01015: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0762 - val_loss: 0.0505\n",
      "\n",
      "Epoch 01016: LearningRateScheduler reducing learning rate to 0.00029253125530730607.\n",
      "Epoch 1016/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0674\n",
      "Epoch 01016: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0674 - val_loss: 0.0509\n",
      "\n",
      "Epoch 01017: LearningRateScheduler reducing learning rate to 0.000292516612948282.\n",
      "Epoch 1017/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0604\n",
      "Epoch 01017: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0604 - val_loss: 0.0492\n",
      "\n",
      "Epoch 01018: LearningRateScheduler reducing learning rate to 0.0002925019566212367.\n",
      "Epoch 1018/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 01018: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0920 - val_loss: 0.0486\n",
      "\n",
      "Epoch 01019: LearningRateScheduler reducing learning rate to 0.00029248728632762173.\n",
      "Epoch 1019/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0698\n",
      "Epoch 01019: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0698 - val_loss: 0.0486\n",
      "\n",
      "Epoch 01020: LearningRateScheduler reducing learning rate to 0.00029247260206889024.\n",
      "Epoch 1020/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0624\n",
      "Epoch 01020: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0624 - val_loss: 0.0478\n",
      "\n",
      "Epoch 01021: LearningRateScheduler reducing learning rate to 0.00029245790384649687.\n",
      "Epoch 1021/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0622\n",
      "Epoch 01021: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0622 - val_loss: 0.0456\n",
      "\n",
      "Epoch 01022: LearningRateScheduler reducing learning rate to 0.0002924431916618974.\n",
      "Epoch 1022/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0686\n",
      "Epoch 01022: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0686 - val_loss: 0.0437\n",
      "\n",
      "Epoch 01023: LearningRateScheduler reducing learning rate to 0.0002924284655165491.\n",
      "Epoch 1023/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0724\n",
      "Epoch 01023: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0724 - val_loss: 0.0435\n",
      "\n",
      "Epoch 01024: LearningRateScheduler reducing learning rate to 0.0002924137254119107.\n",
      "Epoch 1024/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0838\n",
      "Epoch 01024: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0838 - val_loss: 0.0441\n",
      "\n",
      "Epoch 01025: LearningRateScheduler reducing learning rate to 0.00029239897134944215.\n",
      "Epoch 1025/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0805\n",
      "Epoch 01025: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0805 - val_loss: 0.0454\n",
      "\n",
      "Epoch 01026: LearningRateScheduler reducing learning rate to 0.00029238420333060504.\n",
      "Epoch 1026/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0630\n",
      "Epoch 01026: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0630 - val_loss: 0.0452\n",
      "\n",
      "Epoch 01027: LearningRateScheduler reducing learning rate to 0.00029236942135686195.\n",
      "Epoch 1027/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0531\n",
      "Epoch 01027: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0531 - val_loss: 0.0449\n",
      "\n",
      "Epoch 01028: LearningRateScheduler reducing learning rate to 0.00029235462542967727.\n",
      "Epoch 1028/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0692\n",
      "Epoch 01028: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0692 - val_loss: 0.0450\n",
      "\n",
      "Epoch 01029: LearningRateScheduler reducing learning rate to 0.0002923398155505164.\n",
      "Epoch 1029/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 01029: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0982 - val_loss: 0.0457\n",
      "\n",
      "Epoch 01030: LearningRateScheduler reducing learning rate to 0.00029232499172084645.\n",
      "Epoch 1030/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0752\n",
      "Epoch 01030: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0752 - val_loss: 0.0464\n",
      "\n",
      "Epoch 01031: LearningRateScheduler reducing learning rate to 0.0002923101539421356.\n",
      "Epoch 1031/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0592\n",
      "Epoch 01031: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0592 - val_loss: 0.0458\n",
      "\n",
      "Epoch 01032: LearningRateScheduler reducing learning rate to 0.0002922953022158538.\n",
      "Epoch 1032/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0566\n",
      "Epoch 01032: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0566 - val_loss: 0.0471\n",
      "\n",
      "Epoch 01033: LearningRateScheduler reducing learning rate to 0.00029228043654347185.\n",
      "Epoch 1033/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0602\n",
      "Epoch 01033: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0602 - val_loss: 0.0490\n",
      "\n",
      "Epoch 01034: LearningRateScheduler reducing learning rate to 0.00029226555692646246.\n",
      "Epoch 1034/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0670\n",
      "Epoch 01034: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0670 - val_loss: 0.0500\n",
      "\n",
      "Epoch 01035: LearningRateScheduler reducing learning rate to 0.00029225066336629937.\n",
      "Epoch 1035/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0655\n",
      "Epoch 01035: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0655 - val_loss: 0.0500\n",
      "\n",
      "Epoch 01036: LearningRateScheduler reducing learning rate to 0.00029223575586445784.\n",
      "Epoch 1036/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0727\n",
      "Epoch 01036: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0727 - val_loss: 0.0472\n",
      "\n",
      "Epoch 01037: LearningRateScheduler reducing learning rate to 0.00029222083442241456.\n",
      "Epoch 1037/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0751\n",
      "Epoch 01037: val_loss did not improve from 0.04237\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0751 - val_loss: 0.0426\n",
      "\n",
      "Epoch 01038: LearningRateScheduler reducing learning rate to 0.0002922058990416474.\n",
      "Epoch 1038/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482\n",
      "Epoch 01038: val_loss improved from 0.04237 to 0.04117, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0482 - val_loss: 0.0412\n",
      "\n",
      "Epoch 01039: LearningRateScheduler reducing learning rate to 0.00029219094972363595.\n",
      "Epoch 1039/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 01039: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0785 - val_loss: 0.0425\n",
      "\n",
      "Epoch 01040: LearningRateScheduler reducing learning rate to 0.00029217598646986075.\n",
      "Epoch 1040/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0690\n",
      "Epoch 01040: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0690 - val_loss: 0.0454\n",
      "\n",
      "Epoch 01041: LearningRateScheduler reducing learning rate to 0.00029216100928180404.\n",
      "Epoch 1041/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0954\n",
      "Epoch 01041: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0954 - val_loss: 0.0478\n",
      "\n",
      "Epoch 01042: LearningRateScheduler reducing learning rate to 0.00029214601816094943.\n",
      "Epoch 1042/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0672\n",
      "Epoch 01042: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0672 - val_loss: 0.0472\n",
      "\n",
      "Epoch 01043: LearningRateScheduler reducing learning rate to 0.0002921310131087817.\n",
      "Epoch 1043/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0677\n",
      "Epoch 01043: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0677 - val_loss: 0.0445\n",
      "\n",
      "Epoch 01044: LearningRateScheduler reducing learning rate to 0.0002921159941267872.\n",
      "Epoch 1044/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0574\n",
      "Epoch 01044: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0574 - val_loss: 0.0440\n",
      "\n",
      "Epoch 01045: LearningRateScheduler reducing learning rate to 0.00029210096121645354.\n",
      "Epoch 1045/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0534\n",
      "Epoch 01045: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0534 - val_loss: 0.0468\n",
      "\n",
      "Epoch 01046: LearningRateScheduler reducing learning rate to 0.0002920859143792698.\n",
      "Epoch 1046/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0601\n",
      "Epoch 01046: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0601 - val_loss: 0.0508\n",
      "\n",
      "Epoch 01047: LearningRateScheduler reducing learning rate to 0.00029207085361672647.\n",
      "Epoch 1047/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0552\n",
      "Epoch 01047: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0552 - val_loss: 0.0540\n",
      "\n",
      "Epoch 01048: LearningRateScheduler reducing learning rate to 0.00029205577893031526.\n",
      "Epoch 1048/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0801\n",
      "Epoch 01048: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0801 - val_loss: 0.0558\n",
      "\n",
      "Epoch 01049: LearningRateScheduler reducing learning rate to 0.00029204069032152935.\n",
      "Epoch 1049/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0683\n",
      "Epoch 01049: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0683 - val_loss: 0.0539\n",
      "\n",
      "Epoch 01050: LearningRateScheduler reducing learning rate to 0.00029202558779186337.\n",
      "Epoch 1050/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0801\n",
      "Epoch 01050: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0801 - val_loss: 0.0510\n",
      "\n",
      "Epoch 01051: LearningRateScheduler reducing learning rate to 0.0002920104713428132.\n",
      "Epoch 1051/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0551\n",
      "Epoch 01051: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0551 - val_loss: 0.0509\n",
      "\n",
      "Epoch 01052: LearningRateScheduler reducing learning rate to 0.00029199534097587614.\n",
      "Epoch 1052/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0663\n",
      "Epoch 01052: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0663 - val_loss: 0.0513\n",
      "\n",
      "Epoch 01053: LearningRateScheduler reducing learning rate to 0.000291980196692551.\n",
      "Epoch 1053/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0728\n",
      "Epoch 01053: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0728 - val_loss: 0.0487\n",
      "\n",
      "Epoch 01054: LearningRateScheduler reducing learning rate to 0.0002919650384943378.\n",
      "Epoch 1054/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0732\n",
      "Epoch 01054: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0732 - val_loss: 0.0444\n",
      "\n",
      "Epoch 01055: LearningRateScheduler reducing learning rate to 0.0002919498663827379.\n",
      "Epoch 1055/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0644\n",
      "Epoch 01055: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0644 - val_loss: 0.0438\n",
      "\n",
      "Epoch 01056: LearningRateScheduler reducing learning rate to 0.0002919346803592543.\n",
      "Epoch 1056/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0632\n",
      "Epoch 01056: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0632 - val_loss: 0.0432\n",
      "\n",
      "Epoch 01057: LearningRateScheduler reducing learning rate to 0.00029191948042539106.\n",
      "Epoch 1057/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0649\n",
      "Epoch 01057: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0649 - val_loss: 0.0437\n",
      "\n",
      "Epoch 01058: LearningRateScheduler reducing learning rate to 0.00029190426658265385.\n",
      "Epoch 1058/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0524\n",
      "Epoch 01058: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0524 - val_loss: 0.0431\n",
      "\n",
      "Epoch 01059: LearningRateScheduler reducing learning rate to 0.0002918890388325496.\n",
      "Epoch 1059/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0696\n",
      "Epoch 01059: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0696 - val_loss: 0.0436\n",
      "\n",
      "Epoch 01060: LearningRateScheduler reducing learning rate to 0.00029187379717658673.\n",
      "Epoch 1060/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 01060: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0779 - val_loss: 0.0441\n",
      "\n",
      "Epoch 01061: LearningRateScheduler reducing learning rate to 0.00029185854161627487.\n",
      "Epoch 1061/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0592\n",
      "Epoch 01061: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0592 - val_loss: 0.0449\n",
      "\n",
      "Epoch 01062: LearningRateScheduler reducing learning rate to 0.0002918432721531252.\n",
      "Epoch 1062/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0739\n",
      "Epoch 01062: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0739 - val_loss: 0.0456\n",
      "\n",
      "Epoch 01063: LearningRateScheduler reducing learning rate to 0.00029182798878865006.\n",
      "Epoch 1063/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0639\n",
      "Epoch 01063: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0639 - val_loss: 0.0472\n",
      "\n",
      "Epoch 01064: LearningRateScheduler reducing learning rate to 0.0002918126915243634.\n",
      "Epoch 1064/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0606\n",
      "Epoch 01064: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0606 - val_loss: 0.0480\n",
      "\n",
      "Epoch 01065: LearningRateScheduler reducing learning rate to 0.0002917973803617805.\n",
      "Epoch 1065/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0624\n",
      "Epoch 01065: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0624 - val_loss: 0.0476\n",
      "\n",
      "Epoch 01066: LearningRateScheduler reducing learning rate to 0.0002917820553024179.\n",
      "Epoch 1066/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0506\n",
      "Epoch 01066: val_loss did not improve from 0.04117\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0506 - val_loss: 0.0435\n",
      "\n",
      "Epoch 01067: LearningRateScheduler reducing learning rate to 0.00029176671634779353.\n",
      "Epoch 1067/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0817\n",
      "Epoch 01067: val_loss improved from 0.04117 to 0.03997, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0817 - val_loss: 0.0400\n",
      "\n",
      "Epoch 01068: LearningRateScheduler reducing learning rate to 0.0002917513634994268.\n",
      "Epoch 1068/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0723\n",
      "Epoch 01068: val_loss improved from 0.03997 to 0.03991, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0723 - val_loss: 0.0399\n",
      "\n",
      "Epoch 01069: LearningRateScheduler reducing learning rate to 0.00029173599675883855.\n",
      "Epoch 1069/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0568\n",
      "Epoch 01069: val_loss improved from 0.03991 to 0.03870, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0568 - val_loss: 0.0387\n",
      "\n",
      "Epoch 01070: LearningRateScheduler reducing learning rate to 0.0002917206161275507.\n",
      "Epoch 1070/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0550\n",
      "Epoch 01070: val_loss improved from 0.03870 to 0.03761, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0550 - val_loss: 0.0376\n",
      "\n",
      "Epoch 01071: LearningRateScheduler reducing learning rate to 0.0002917052216070869.\n",
      "Epoch 1071/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0803\n",
      "Epoch 01071: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0803 - val_loss: 0.0382\n",
      "\n",
      "Epoch 01072: LearningRateScheduler reducing learning rate to 0.00029168981319897195.\n",
      "Epoch 1072/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0567\n",
      "Epoch 01072: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0567 - val_loss: 0.0396\n",
      "\n",
      "Epoch 01073: LearningRateScheduler reducing learning rate to 0.00029167439090473205.\n",
      "Epoch 1073/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0546\n",
      "Epoch 01073: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0546 - val_loss: 0.0402\n",
      "\n",
      "Epoch 01074: LearningRateScheduler reducing learning rate to 0.00029165895472589484.\n",
      "Epoch 1074/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0702\n",
      "Epoch 01074: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0702 - val_loss: 0.0424\n",
      "\n",
      "Epoch 01075: LearningRateScheduler reducing learning rate to 0.00029164350466398927.\n",
      "Epoch 1075/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0549\n",
      "Epoch 01075: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0549 - val_loss: 0.0455\n",
      "\n",
      "Epoch 01076: LearningRateScheduler reducing learning rate to 0.00029162804072054585.\n",
      "Epoch 1076/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0687\n",
      "Epoch 01076: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0687 - val_loss: 0.0461\n",
      "\n",
      "Epoch 01077: LearningRateScheduler reducing learning rate to 0.00029161256289709613.\n",
      "Epoch 1077/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0547\n",
      "Epoch 01077: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0547 - val_loss: 0.0482\n",
      "\n",
      "Epoch 01078: LearningRateScheduler reducing learning rate to 0.0002915970711951734.\n",
      "Epoch 1078/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0631\n",
      "Epoch 01078: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0631 - val_loss: 0.0478\n",
      "\n",
      "Epoch 01079: LearningRateScheduler reducing learning rate to 0.000291581565616312.\n",
      "Epoch 1079/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0563\n",
      "Epoch 01079: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0563 - val_loss: 0.0464\n",
      "\n",
      "Epoch 01080: LearningRateScheduler reducing learning rate to 0.00029156604616204786.\n",
      "Epoch 1080/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0639\n",
      "Epoch 01080: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0639 - val_loss: 0.0446\n",
      "\n",
      "Epoch 01081: LearningRateScheduler reducing learning rate to 0.0002915505128339182.\n",
      "Epoch 1081/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0701\n",
      "Epoch 01081: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0701 - val_loss: 0.0465\n",
      "\n",
      "Epoch 01082: LearningRateScheduler reducing learning rate to 0.0002915349656334617.\n",
      "Epoch 1082/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0601\n",
      "Epoch 01082: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0601 - val_loss: 0.0463\n",
      "\n",
      "Epoch 01083: LearningRateScheduler reducing learning rate to 0.0002915194045622182.\n",
      "Epoch 1083/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0828\n",
      "Epoch 01083: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0828 - val_loss: 0.0455\n",
      "\n",
      "Epoch 01084: LearningRateScheduler reducing learning rate to 0.00029150382962172926.\n",
      "Epoch 1084/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0503\n",
      "Epoch 01084: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0503 - val_loss: 0.0444\n",
      "\n",
      "Epoch 01085: LearningRateScheduler reducing learning rate to 0.0002914882408135374.\n",
      "Epoch 1085/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0603\n",
      "Epoch 01085: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0603 - val_loss: 0.0434\n",
      "\n",
      "Epoch 01086: LearningRateScheduler reducing learning rate to 0.0002914726381391869.\n",
      "Epoch 1086/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0597\n",
      "Epoch 01086: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0597 - val_loss: 0.0453\n",
      "\n",
      "Epoch 01087: LearningRateScheduler reducing learning rate to 0.0002914570216002232.\n",
      "Epoch 1087/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0827\n",
      "Epoch 01087: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0827 - val_loss: 0.0441\n",
      "\n",
      "Epoch 01088: LearningRateScheduler reducing learning rate to 0.0002914413911981931.\n",
      "Epoch 1088/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0664\n",
      "Epoch 01088: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0664 - val_loss: 0.0410\n",
      "\n",
      "Epoch 01089: LearningRateScheduler reducing learning rate to 0.00029142574693464483.\n",
      "Epoch 1089/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0598\n",
      "Epoch 01089: val_loss did not improve from 0.03761\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0598 - val_loss: 0.0386\n",
      "\n",
      "Epoch 01090: LearningRateScheduler reducing learning rate to 0.000291410088811128.\n",
      "Epoch 1090/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0551\n",
      "Epoch 01090: val_loss improved from 0.03761 to 0.03733, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0551 - val_loss: 0.0373\n",
      "\n",
      "Epoch 01091: LearningRateScheduler reducing learning rate to 0.00029139441682919366.\n",
      "Epoch 1091/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0593\n",
      "Epoch 01091: val_loss did not improve from 0.03733\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0593 - val_loss: 0.0376\n",
      "\n",
      "Epoch 01092: LearningRateScheduler reducing learning rate to 0.0002913787309903941.\n",
      "Epoch 1092/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0697\n",
      "Epoch 01092: val_loss did not improve from 0.03733\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0697 - val_loss: 0.0381\n",
      "\n",
      "Epoch 01093: LearningRateScheduler reducing learning rate to 0.00029136303129628294.\n",
      "Epoch 1093/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0713\n",
      "Epoch 01093: val_loss did not improve from 0.03733\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0713 - val_loss: 0.0374\n",
      "\n",
      "Epoch 01094: LearningRateScheduler reducing learning rate to 0.00029134731774841543.\n",
      "Epoch 1094/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0529\n",
      "Epoch 01094: val_loss did not improve from 0.03733\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0529 - val_loss: 0.0381\n",
      "\n",
      "Epoch 01095: LearningRateScheduler reducing learning rate to 0.00029133159034834793.\n",
      "Epoch 1095/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0546\n",
      "Epoch 01095: val_loss did not improve from 0.03733\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0546 - val_loss: 0.0379\n",
      "\n",
      "Epoch 01096: LearningRateScheduler reducing learning rate to 0.0002913158490976384.\n",
      "Epoch 1096/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0695\n",
      "Epoch 01096: val_loss did not improve from 0.03733\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0695 - val_loss: 0.0383\n",
      "\n",
      "Epoch 01097: LearningRateScheduler reducing learning rate to 0.0002913000939978459.\n",
      "Epoch 1097/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0608\n",
      "Epoch 01097: val_loss improved from 0.03733 to 0.03601, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0608 - val_loss: 0.0360\n",
      "\n",
      "Epoch 01098: LearningRateScheduler reducing learning rate to 0.0002912843250505311.\n",
      "Epoch 1098/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0640\n",
      "Epoch 01098: val_loss improved from 0.03601 to 0.03438, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0640 - val_loss: 0.0344\n",
      "\n",
      "Epoch 01099: LearningRateScheduler reducing learning rate to 0.0002912685422572559.\n",
      "Epoch 1099/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0599\n",
      "Epoch 01099: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0599 - val_loss: 0.0379\n",
      "\n",
      "Epoch 01100: LearningRateScheduler reducing learning rate to 0.0002912527456195836.\n",
      "Epoch 1100/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0683\n",
      "Epoch 01100: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0683 - val_loss: 0.0417\n",
      "\n",
      "Epoch 01101: LearningRateScheduler reducing learning rate to 0.00029123693513907897.\n",
      "Epoch 1101/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0560\n",
      "Epoch 01101: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0560 - val_loss: 0.0444\n",
      "\n",
      "Epoch 01102: LearningRateScheduler reducing learning rate to 0.000291221110817308.\n",
      "Epoch 1102/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0610\n",
      "Epoch 01102: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0610 - val_loss: 0.0487\n",
      "\n",
      "Epoch 01103: LearningRateScheduler reducing learning rate to 0.00029120527265583823.\n",
      "Epoch 1103/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0477\n",
      "Epoch 01103: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0477 - val_loss: 0.0511\n",
      "\n",
      "Epoch 01104: LearningRateScheduler reducing learning rate to 0.0002911894206562384.\n",
      "Epoch 1104/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0604\n",
      "Epoch 01104: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0604 - val_loss: 0.0471\n",
      "\n",
      "Epoch 01105: LearningRateScheduler reducing learning rate to 0.0002911735548200787.\n",
      "Epoch 1105/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0568\n",
      "Epoch 01105: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0568 - val_loss: 0.0426\n",
      "\n",
      "Epoch 01106: LearningRateScheduler reducing learning rate to 0.00029115767514893064.\n",
      "Epoch 1106/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0650\n",
      "Epoch 01106: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0650 - val_loss: 0.0418\n",
      "\n",
      "Epoch 01107: LearningRateScheduler reducing learning rate to 0.0002911417816443671.\n",
      "Epoch 1107/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0441\n",
      "Epoch 01107: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0441 - val_loss: 0.0415\n",
      "\n",
      "Epoch 01108: LearningRateScheduler reducing learning rate to 0.00029112587430796255.\n",
      "Epoch 1108/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0566\n",
      "Epoch 01108: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0566 - val_loss: 0.0413\n",
      "\n",
      "Epoch 01109: LearningRateScheduler reducing learning rate to 0.00029110995314129246.\n",
      "Epoch 1109/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0503\n",
      "Epoch 01109: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0503 - val_loss: 0.0403\n",
      "\n",
      "Epoch 01110: LearningRateScheduler reducing learning rate to 0.00029109401814593396.\n",
      "Epoch 1110/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0576\n",
      "Epoch 01110: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0576 - val_loss: 0.0386\n",
      "\n",
      "Epoch 01111: LearningRateScheduler reducing learning rate to 0.0002910780693234654.\n",
      "Epoch 1111/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469\n",
      "Epoch 01111: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0469 - val_loss: 0.0388\n",
      "\n",
      "Epoch 01112: LearningRateScheduler reducing learning rate to 0.00029106210667546656.\n",
      "Epoch 1112/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463\n",
      "Epoch 01112: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0463 - val_loss: 0.0401\n",
      "\n",
      "Epoch 01113: LearningRateScheduler reducing learning rate to 0.0002910461302035186.\n",
      "Epoch 1113/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0549\n",
      "Epoch 01113: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0549 - val_loss: 0.0418\n",
      "\n",
      "Epoch 01114: LearningRateScheduler reducing learning rate to 0.000291030139909204.\n",
      "Epoch 1114/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447\n",
      "Epoch 01114: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0447 - val_loss: 0.0432\n",
      "\n",
      "Epoch 01115: LearningRateScheduler reducing learning rate to 0.0002910141357941067.\n",
      "Epoch 1115/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0458\n",
      "Epoch 01115: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0458 - val_loss: 0.0424\n",
      "\n",
      "Epoch 01116: LearningRateScheduler reducing learning rate to 0.00029099811785981186.\n",
      "Epoch 1116/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0496\n",
      "Epoch 01116: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0496 - val_loss: 0.0413\n",
      "\n",
      "Epoch 01117: LearningRateScheduler reducing learning rate to 0.0002909820861079061.\n",
      "Epoch 1117/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0517\n",
      "Epoch 01117: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0517 - val_loss: 0.0423\n",
      "\n",
      "Epoch 01118: LearningRateScheduler reducing learning rate to 0.0002909660405399774.\n",
      "Epoch 1118/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0680\n",
      "Epoch 01118: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0680 - val_loss: 0.0425\n",
      "\n",
      "Epoch 01119: LearningRateScheduler reducing learning rate to 0.0002909499811576152.\n",
      "Epoch 1119/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0464\n",
      "Epoch 01119: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0464 - val_loss: 0.0432\n",
      "\n",
      "Epoch 01120: LearningRateScheduler reducing learning rate to 0.0002909339079624101.\n",
      "Epoch 1120/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461\n",
      "Epoch 01120: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0461 - val_loss: 0.0455\n",
      "\n",
      "Epoch 01121: LearningRateScheduler reducing learning rate to 0.0002909178209559543.\n",
      "Epoch 1121/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495\n",
      "Epoch 01121: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0495 - val_loss: 0.0465\n",
      "\n",
      "Epoch 01122: LearningRateScheduler reducing learning rate to 0.0002909017201398412.\n",
      "Epoch 1122/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0454\n",
      "Epoch 01122: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0454 - val_loss: 0.0485\n",
      "\n",
      "Epoch 01123: LearningRateScheduler reducing learning rate to 0.0002908856055156655.\n",
      "Epoch 1123/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509\n",
      "Epoch 01123: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0509 - val_loss: 0.0502\n",
      "\n",
      "Epoch 01124: LearningRateScheduler reducing learning rate to 0.00029086947708502364.\n",
      "Epoch 1124/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0373\n",
      "Epoch 01124: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0373 - val_loss: 0.0484\n",
      "\n",
      "Epoch 01125: LearningRateScheduler reducing learning rate to 0.000290853334849513.\n",
      "Epoch 1125/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0663\n",
      "Epoch 01125: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0663 - val_loss: 0.0449\n",
      "\n",
      "Epoch 01126: LearningRateScheduler reducing learning rate to 0.00029083717881073256.\n",
      "Epoch 1126/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0609\n",
      "Epoch 01126: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0609 - val_loss: 0.0426\n",
      "\n",
      "Epoch 01127: LearningRateScheduler reducing learning rate to 0.0002908210089702826.\n",
      "Epoch 1127/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0687\n",
      "Epoch 01127: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0687 - val_loss: 0.0408\n",
      "\n",
      "Epoch 01128: LearningRateScheduler reducing learning rate to 0.0002908048253297648.\n",
      "Epoch 1128/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538\n",
      "Epoch 01128: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0538 - val_loss: 0.0399\n",
      "\n",
      "Epoch 01129: LearningRateScheduler reducing learning rate to 0.0002907886278907822.\n",
      "Epoch 1129/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476\n",
      "Epoch 01129: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0476 - val_loss: 0.0430\n",
      "\n",
      "Epoch 01130: LearningRateScheduler reducing learning rate to 0.0002907724166549392.\n",
      "Epoch 1130/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0595\n",
      "Epoch 01130: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0595 - val_loss: 0.0444\n",
      "\n",
      "Epoch 01131: LearningRateScheduler reducing learning rate to 0.00029075619162384145.\n",
      "Epoch 1131/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0534\n",
      "Epoch 01131: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0534 - val_loss: 0.0407\n",
      "\n",
      "Epoch 01132: LearningRateScheduler reducing learning rate to 0.00029073995279909617.\n",
      "Epoch 1132/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0450\n",
      "Epoch 01132: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0450 - val_loss: 0.0371\n",
      "\n",
      "Epoch 01133: LearningRateScheduler reducing learning rate to 0.0002907237001823118.\n",
      "Epoch 1133/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509\n",
      "Epoch 01133: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0509 - val_loss: 0.0351\n",
      "\n",
      "Epoch 01134: LearningRateScheduler reducing learning rate to 0.0002907074337750983.\n",
      "Epoch 1134/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0511\n",
      "Epoch 01134: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0511 - val_loss: 0.0346\n",
      "\n",
      "Epoch 01135: LearningRateScheduler reducing learning rate to 0.0002906911535790669.\n",
      "Epoch 1135/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0371\n",
      "Epoch 01135: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0371 - val_loss: 0.0357\n",
      "\n",
      "Epoch 01136: LearningRateScheduler reducing learning rate to 0.00029067485959583.\n",
      "Epoch 1136/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0537\n",
      "Epoch 01136: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0537 - val_loss: 0.0373\n",
      "\n",
      "Epoch 01137: LearningRateScheduler reducing learning rate to 0.00029065855182700174.\n",
      "Epoch 1137/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0583\n",
      "Epoch 01137: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0583 - val_loss: 0.0360\n",
      "\n",
      "Epoch 01138: LearningRateScheduler reducing learning rate to 0.00029064223027419736.\n",
      "Epoch 1138/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0636\n",
      "Epoch 01138: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0636 - val_loss: 0.0351\n",
      "\n",
      "Epoch 01139: LearningRateScheduler reducing learning rate to 0.0002906258949390336.\n",
      "Epoch 1139/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0445\n",
      "Epoch 01139: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0445 - val_loss: 0.0389\n",
      "\n",
      "Epoch 01140: LearningRateScheduler reducing learning rate to 0.0002906095458231285.\n",
      "Epoch 1140/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516\n",
      "Epoch 01140: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0516 - val_loss: 0.0420\n",
      "\n",
      "Epoch 01141: LearningRateScheduler reducing learning rate to 0.0002905931829281014.\n",
      "Epoch 1141/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510\n",
      "Epoch 01141: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0510 - val_loss: 0.0428\n",
      "\n",
      "Epoch 01142: LearningRateScheduler reducing learning rate to 0.0002905768062555732.\n",
      "Epoch 1142/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0564\n",
      "Epoch 01142: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0564 - val_loss: 0.0396\n",
      "\n",
      "Epoch 01143: LearningRateScheduler reducing learning rate to 0.0002905604158071659.\n",
      "Epoch 1143/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0448\n",
      "Epoch 01143: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0448 - val_loss: 0.0346\n",
      "\n",
      "Epoch 01144: LearningRateScheduler reducing learning rate to 0.0002905440115845032.\n",
      "Epoch 1144/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0501\n",
      "Epoch 01144: val_loss did not improve from 0.03438\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0501 - val_loss: 0.0347\n",
      "\n",
      "Epoch 01145: LearningRateScheduler reducing learning rate to 0.00029052759358920985.\n",
      "Epoch 1145/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 01145: val_loss improved from 0.03438 to 0.03274, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0772 - val_loss: 0.0327\n",
      "\n",
      "Epoch 01146: LearningRateScheduler reducing learning rate to 0.00029051116182291214.\n",
      "Epoch 1146/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0578\n",
      "Epoch 01146: val_loss improved from 0.03274 to 0.03042, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0578 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01147: LearningRateScheduler reducing learning rate to 0.0002904947162872376.\n",
      "Epoch 1147/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0587\n",
      "Epoch 01147: val_loss improved from 0.03042 to 0.02740, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0587 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01148: LearningRateScheduler reducing learning rate to 0.0002904782569838153.\n",
      "Epoch 1148/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01148: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0407 - val_loss: 0.0303\n",
      "\n",
      "Epoch 01149: LearningRateScheduler reducing learning rate to 0.00029046178391427556.\n",
      "Epoch 1149/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459\n",
      "Epoch 01149: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0459 - val_loss: 0.0351\n",
      "\n",
      "Epoch 01150: LearningRateScheduler reducing learning rate to 0.00029044529708025.\n",
      "Epoch 1150/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0382\n",
      "Epoch 01150: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0382 - val_loss: 0.0361\n",
      "\n",
      "Epoch 01151: LearningRateScheduler reducing learning rate to 0.0002904287964833717.\n",
      "Epoch 1151/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01151: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0402 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01152: LearningRateScheduler reducing learning rate to 0.0002904122821252751.\n",
      "Epoch 1152/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0528\n",
      "Epoch 01152: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0528 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01153: LearningRateScheduler reducing learning rate to 0.000290395754007596.\n",
      "Epoch 1153/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0418\n",
      "Epoch 01153: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0418 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01154: LearningRateScheduler reducing learning rate to 0.00029037921213197155.\n",
      "Epoch 1154/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0624\n",
      "Epoch 01154: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0624 - val_loss: 0.0340\n",
      "\n",
      "Epoch 01155: LearningRateScheduler reducing learning rate to 0.00029036265650004027.\n",
      "Epoch 1155/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 01155: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0345 - val_loss: 0.0361\n",
      "\n",
      "Epoch 01156: LearningRateScheduler reducing learning rate to 0.0002903460871134419.\n",
      "Epoch 1156/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0428\n",
      "Epoch 01156: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0428 - val_loss: 0.0376\n",
      "\n",
      "Epoch 01157: LearningRateScheduler reducing learning rate to 0.0002903295039738179.\n",
      "Epoch 1157/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0532\n",
      "Epoch 01157: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0532 - val_loss: 0.0362\n",
      "\n",
      "Epoch 01158: LearningRateScheduler reducing learning rate to 0.0002903129070828107.\n",
      "Epoch 1158/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0590\n",
      "Epoch 01158: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0590 - val_loss: 0.0344\n",
      "\n",
      "Epoch 01159: LearningRateScheduler reducing learning rate to 0.0002902962964420643.\n",
      "Epoch 1159/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0723\n",
      "Epoch 01159: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0723 - val_loss: 0.0386\n",
      "\n",
      "Epoch 01160: LearningRateScheduler reducing learning rate to 0.00029027967205322406.\n",
      "Epoch 1160/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0638\n",
      "Epoch 01160: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0638 - val_loss: 0.0387\n",
      "\n",
      "Epoch 01161: LearningRateScheduler reducing learning rate to 0.0002902630339179367.\n",
      "Epoch 1161/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0585\n",
      "Epoch 01161: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0585 - val_loss: 0.0377\n",
      "\n",
      "Epoch 01162: LearningRateScheduler reducing learning rate to 0.00029024638203785017.\n",
      "Epoch 1162/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490\n",
      "Epoch 01162: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0490 - val_loss: 0.0356\n",
      "\n",
      "Epoch 01163: LearningRateScheduler reducing learning rate to 0.00029022971641461386.\n",
      "Epoch 1163/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0381\n",
      "Epoch 01163: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0381 - val_loss: 0.0345\n",
      "\n",
      "Epoch 01164: LearningRateScheduler reducing learning rate to 0.0002902130370498786.\n",
      "Epoch 1164/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 01164: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0363 - val_loss: 0.0316\n",
      "\n",
      "Epoch 01165: LearningRateScheduler reducing learning rate to 0.00029019634394529656.\n",
      "Epoch 1165/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0493\n",
      "Epoch 01165: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0493 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01166: LearningRateScheduler reducing learning rate to 0.00029017963710252113.\n",
      "Epoch 1166/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0428\n",
      "Epoch 01166: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0428 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01167: LearningRateScheduler reducing learning rate to 0.00029016291652320727.\n",
      "Epoch 1167/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486\n",
      "Epoch 01167: val_loss did not improve from 0.02740\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0486 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01168: LearningRateScheduler reducing learning rate to 0.0002901461822090111.\n",
      "Epoch 1168/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0403\n",
      "Epoch 01168: val_loss improved from 0.02740 to 0.02714, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0403 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01169: LearningRateScheduler reducing learning rate to 0.0002901294341615902.\n",
      "Epoch 1169/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478\n",
      "Epoch 01169: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0478 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01170: LearningRateScheduler reducing learning rate to 0.0002901126723826036.\n",
      "Epoch 1170/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0558\n",
      "Epoch 01170: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0558 - val_loss: 0.0305\n",
      "\n",
      "Epoch 01171: LearningRateScheduler reducing learning rate to 0.0002900958968737115.\n",
      "Epoch 1171/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0396\n",
      "Epoch 01171: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0396 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01172: LearningRateScheduler reducing learning rate to 0.0002900791076365756.\n",
      "Epoch 1172/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01172: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0413 - val_loss: 0.0319\n",
      "\n",
      "Epoch 01173: LearningRateScheduler reducing learning rate to 0.0002900623046728589.\n",
      "Epoch 1173/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494\n",
      "Epoch 01173: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0494 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01174: LearningRateScheduler reducing learning rate to 0.0002900454879842257.\n",
      "Epoch 1174/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0533\n",
      "Epoch 01174: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0533 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01175: LearningRateScheduler reducing learning rate to 0.0002900286575723418.\n",
      "Epoch 1175/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480\n",
      "Epoch 01175: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0480 - val_loss: 0.0354\n",
      "\n",
      "Epoch 01176: LearningRateScheduler reducing learning rate to 0.0002900118134388743.\n",
      "Epoch 1176/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495\n",
      "Epoch 01176: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0495 - val_loss: 0.0373\n",
      "\n",
      "Epoch 01177: LearningRateScheduler reducing learning rate to 0.00028999495558549166.\n",
      "Epoch 1177/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481\n",
      "Epoch 01177: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0481 - val_loss: 0.0383\n",
      "\n",
      "Epoch 01178: LearningRateScheduler reducing learning rate to 0.0002899780840138636.\n",
      "Epoch 1178/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 01178: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0370 - val_loss: 0.0382\n",
      "\n",
      "Epoch 01179: LearningRateScheduler reducing learning rate to 0.00028996119872566145.\n",
      "Epoch 1179/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0565\n",
      "Epoch 01179: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0565 - val_loss: 0.0381\n",
      "\n",
      "Epoch 01180: LearningRateScheduler reducing learning rate to 0.0002899442997225576.\n",
      "Epoch 1180/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487\n",
      "Epoch 01180: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0487 - val_loss: 0.0383\n",
      "\n",
      "Epoch 01181: LearningRateScheduler reducing learning rate to 0.00028992738700622594.\n",
      "Epoch 1181/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0522\n",
      "Epoch 01181: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0522 - val_loss: 0.0386\n",
      "\n",
      "Epoch 01182: LearningRateScheduler reducing learning rate to 0.00028991046057834174.\n",
      "Epoch 1182/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0421\n",
      "Epoch 01182: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0421 - val_loss: 0.0382\n",
      "\n",
      "Epoch 01183: LearningRateScheduler reducing learning rate to 0.0002898935204405817.\n",
      "Epoch 1183/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0537\n",
      "Epoch 01183: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0537 - val_loss: 0.0384\n",
      "\n",
      "Epoch 01184: LearningRateScheduler reducing learning rate to 0.0002898765665946236.\n",
      "Epoch 1184/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0395\n",
      "Epoch 01184: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0395 - val_loss: 0.0388\n",
      "\n",
      "Epoch 01185: LearningRateScheduler reducing learning rate to 0.0002898595990421469.\n",
      "Epoch 1185/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0565\n",
      "Epoch 01185: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0565 - val_loss: 0.0399\n",
      "\n",
      "Epoch 01186: LearningRateScheduler reducing learning rate to 0.0002898426177848322.\n",
      "Epoch 1186/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 01186: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0325 - val_loss: 0.0428\n",
      "\n",
      "Epoch 01187: LearningRateScheduler reducing learning rate to 0.00028982562282436154.\n",
      "Epoch 1187/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0499\n",
      "Epoch 01187: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0499 - val_loss: 0.0433\n",
      "\n",
      "Epoch 01188: LearningRateScheduler reducing learning rate to 0.00028980861416241836.\n",
      "Epoch 1188/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494\n",
      "Epoch 01188: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0494 - val_loss: 0.0431\n",
      "\n",
      "Epoch 01189: LearningRateScheduler reducing learning rate to 0.00028979159180068736.\n",
      "Epoch 1189/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0399\n",
      "Epoch 01189: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0399 - val_loss: 0.0386\n",
      "\n",
      "Epoch 01190: LearningRateScheduler reducing learning rate to 0.0002897745557408546.\n",
      "Epoch 1190/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485\n",
      "Epoch 01190: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0485 - val_loss: 0.0347\n",
      "\n",
      "Epoch 01191: LearningRateScheduler reducing learning rate to 0.0002897575059846077.\n",
      "Epoch 1191/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0441\n",
      "Epoch 01191: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0441 - val_loss: 0.0349\n",
      "\n",
      "Epoch 01192: LearningRateScheduler reducing learning rate to 0.0002897404425336353.\n",
      "Epoch 1192/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0411\n",
      "Epoch 01192: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0411 - val_loss: 0.0389\n",
      "\n",
      "Epoch 01193: LearningRateScheduler reducing learning rate to 0.0002897233653896277.\n",
      "Epoch 1193/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 01193: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0316 - val_loss: 0.0411\n",
      "\n",
      "Epoch 01194: LearningRateScheduler reducing learning rate to 0.0002897062745542764.\n",
      "Epoch 1194/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0364\n",
      "Epoch 01194: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0364 - val_loss: 0.0431\n",
      "\n",
      "Epoch 01195: LearningRateScheduler reducing learning rate to 0.00028968917002927417.\n",
      "Epoch 1195/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 01195: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0387 - val_loss: 0.0454\n",
      "\n",
      "Epoch 01196: LearningRateScheduler reducing learning rate to 0.00028967205181631544.\n",
      "Epoch 1196/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0706\n",
      "Epoch 01196: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0706 - val_loss: 0.0465\n",
      "\n",
      "Epoch 01197: LearningRateScheduler reducing learning rate to 0.00028965491991709565.\n",
      "Epoch 1197/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0578\n",
      "Epoch 01197: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0578 - val_loss: 0.0481\n",
      "\n",
      "Epoch 01198: LearningRateScheduler reducing learning rate to 0.0002896377743333119.\n",
      "Epoch 1198/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01198: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0348 - val_loss: 0.0500\n",
      "\n",
      "Epoch 01199: LearningRateScheduler reducing learning rate to 0.0002896206150666624.\n",
      "Epoch 1199/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0532\n",
      "Epoch 01199: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0532 - val_loss: 0.0511\n",
      "\n",
      "Epoch 01200: LearningRateScheduler reducing learning rate to 0.00028960344211884683.\n",
      "Epoch 1200/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0389\n",
      "Epoch 01200: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0389 - val_loss: 0.0471\n",
      "\n",
      "Epoch 01201: LearningRateScheduler reducing learning rate to 0.00028958625549156626.\n",
      "Epoch 1201/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01201: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0300 - val_loss: 0.0440\n",
      "\n",
      "Epoch 01202: LearningRateScheduler reducing learning rate to 0.00028956905518652293.\n",
      "Epoch 1202/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 01202: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0318 - val_loss: 0.0420\n",
      "\n",
      "Epoch 01203: LearningRateScheduler reducing learning rate to 0.00028955184120542073.\n",
      "Epoch 1203/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0398\n",
      "Epoch 01203: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0398 - val_loss: 0.0393\n",
      "\n",
      "Epoch 01204: LearningRateScheduler reducing learning rate to 0.00028953461354996467.\n",
      "Epoch 1204/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0378\n",
      "Epoch 01204: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0378 - val_loss: 0.0381\n",
      "\n",
      "Epoch 01205: LearningRateScheduler reducing learning rate to 0.0002895173722218612.\n",
      "Epoch 1205/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459\n",
      "Epoch 01205: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0459 - val_loss: 0.0381\n",
      "\n",
      "Epoch 01206: LearningRateScheduler reducing learning rate to 0.0002895001172228181.\n",
      "Epoch 1206/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0455\n",
      "Epoch 01206: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0455 - val_loss: 0.0348\n",
      "\n",
      "Epoch 01207: LearningRateScheduler reducing learning rate to 0.0002894828485545445.\n",
      "Epoch 1207/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0464\n",
      "Epoch 01207: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0464 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01208: LearningRateScheduler reducing learning rate to 0.00028946556621875096.\n",
      "Epoch 1208/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0451\n",
      "Epoch 01208: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0451 - val_loss: 0.0319\n",
      "\n",
      "Epoch 01209: LearningRateScheduler reducing learning rate to 0.0002894482702171493.\n",
      "Epoch 1209/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0403\n",
      "Epoch 01209: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0403 - val_loss: 0.0309\n",
      "\n",
      "Epoch 01210: LearningRateScheduler reducing learning rate to 0.00028943096055145275.\n",
      "Epoch 1210/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01210: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0315 - val_loss: 0.0303\n",
      "\n",
      "Epoch 01211: LearningRateScheduler reducing learning rate to 0.0002894136372233758.\n",
      "Epoch 1211/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01211: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0332 - val_loss: 0.0320\n",
      "\n",
      "Epoch 01212: LearningRateScheduler reducing learning rate to 0.0002893963002346345.\n",
      "Epoch 1212/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478\n",
      "Epoch 01212: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0478 - val_loss: 0.0326\n",
      "\n",
      "Epoch 01213: LearningRateScheduler reducing learning rate to 0.000289378949586946.\n",
      "Epoch 1213/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01213: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0348 - val_loss: 0.0336\n",
      "\n",
      "Epoch 01214: LearningRateScheduler reducing learning rate to 0.000289361585282029.\n",
      "Epoch 1214/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 01214: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0296 - val_loss: 0.0344\n",
      "\n",
      "Epoch 01215: LearningRateScheduler reducing learning rate to 0.0002893442073216034.\n",
      "Epoch 1215/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0427\n",
      "Epoch 01215: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0427 - val_loss: 0.0341\n",
      "\n",
      "Epoch 01216: LearningRateScheduler reducing learning rate to 0.00028932681570739054.\n",
      "Epoch 1216/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01216: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0310 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01217: LearningRateScheduler reducing learning rate to 0.00028930941044111314.\n",
      "Epoch 1217/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479\n",
      "Epoch 01217: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0479 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01218: LearningRateScheduler reducing learning rate to 0.0002892919915244952.\n",
      "Epoch 1218/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0344\n",
      "Epoch 01218: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0344 - val_loss: 0.0306\n",
      "\n",
      "Epoch 01219: LearningRateScheduler reducing learning rate to 0.0002892745589592622.\n",
      "Epoch 1219/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0379\n",
      "Epoch 01219: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0379 - val_loss: 0.0323\n",
      "\n",
      "Epoch 01220: LearningRateScheduler reducing learning rate to 0.0002892571127471407.\n",
      "Epoch 1220/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0405\n",
      "Epoch 01220: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0405 - val_loss: 0.0326\n",
      "\n",
      "Epoch 01221: LearningRateScheduler reducing learning rate to 0.00028923965288985893.\n",
      "Epoch 1221/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0456\n",
      "Epoch 01221: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0456 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01222: LearningRateScheduler reducing learning rate to 0.0002892221793891463.\n",
      "Epoch 1222/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01222: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0420 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01223: LearningRateScheduler reducing learning rate to 0.0002892046922467335.\n",
      "Epoch 1223/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01223: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0300 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01224: LearningRateScheduler reducing learning rate to 0.00028918719146435283.\n",
      "Epoch 1224/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0379\n",
      "Epoch 01224: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0379 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01225: LearningRateScheduler reducing learning rate to 0.0002891696770437377.\n",
      "Epoch 1225/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0659\n",
      "Epoch 01225: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0659 - val_loss: 0.0313\n",
      "\n",
      "Epoch 01226: LearningRateScheduler reducing learning rate to 0.000289152148986623.\n",
      "Epoch 1226/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 01226: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0318 - val_loss: 0.0334\n",
      "\n",
      "Epoch 01227: LearningRateScheduler reducing learning rate to 0.0002891346072947448.\n",
      "Epoch 1227/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 01227: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0327 - val_loss: 0.0375\n",
      "\n",
      "Epoch 01228: LearningRateScheduler reducing learning rate to 0.00028911705196984073.\n",
      "Epoch 1228/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0378\n",
      "Epoch 01228: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0378 - val_loss: 0.0398\n",
      "\n",
      "Epoch 01229: LearningRateScheduler reducing learning rate to 0.00028909948301364977.\n",
      "Epoch 1229/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0376\n",
      "Epoch 01229: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0376 - val_loss: 0.0363\n",
      "\n",
      "Epoch 01230: LearningRateScheduler reducing learning rate to 0.00028908190042791203.\n",
      "Epoch 1230/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01230: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0326 - val_loss: 0.0375\n",
      "\n",
      "Epoch 01231: LearningRateScheduler reducing learning rate to 0.0002890643042143692.\n",
      "Epoch 1231/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0408\n",
      "Epoch 01231: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0408 - val_loss: 0.0448\n",
      "\n",
      "Epoch 01232: LearningRateScheduler reducing learning rate to 0.0002890466943747641.\n",
      "Epoch 1232/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01232: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0413 - val_loss: 0.0466\n",
      "\n",
      "Epoch 01233: LearningRateScheduler reducing learning rate to 0.0002890290709108412.\n",
      "Epoch 1233/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0446\n",
      "Epoch 01233: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0446 - val_loss: 0.0466\n",
      "\n",
      "Epoch 01234: LearningRateScheduler reducing learning rate to 0.000289011433824346.\n",
      "Epoch 1234/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01234: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0391 - val_loss: 0.0478\n",
      "\n",
      "Epoch 01235: LearningRateScheduler reducing learning rate to 0.0002889937831170255.\n",
      "Epoch 1235/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 01235: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0368 - val_loss: 0.0495\n",
      "\n",
      "Epoch 01236: LearningRateScheduler reducing learning rate to 0.00028897611879062825.\n",
      "Epoch 1236/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01236: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0338 - val_loss: 0.0474\n",
      "\n",
      "Epoch 01237: LearningRateScheduler reducing learning rate to 0.00028895844084690365.\n",
      "Epoch 1237/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0570\n",
      "Epoch 01237: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0570 - val_loss: 0.0466\n",
      "\n",
      "Epoch 01238: LearningRateScheduler reducing learning rate to 0.00028894074928760295.\n",
      "Epoch 1238/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0555\n",
      "Epoch 01238: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0555 - val_loss: 0.0461\n",
      "\n",
      "Epoch 01239: LearningRateScheduler reducing learning rate to 0.00028892304411447836.\n",
      "Epoch 1239/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0455\n",
      "Epoch 01239: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0455 - val_loss: 0.0449\n",
      "\n",
      "Epoch 01240: LearningRateScheduler reducing learning rate to 0.0002889053253292838.\n",
      "Epoch 1240/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01240: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0391 - val_loss: 0.0422\n",
      "\n",
      "Epoch 01241: LearningRateScheduler reducing learning rate to 0.0002888875929337743.\n",
      "Epoch 1241/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0546\n",
      "Epoch 01241: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0546 - val_loss: 0.0357\n",
      "\n",
      "Epoch 01242: LearningRateScheduler reducing learning rate to 0.0002888698469297062.\n",
      "Epoch 1242/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0468\n",
      "Epoch 01242: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0468 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01243: LearningRateScheduler reducing learning rate to 0.0002888520873188374.\n",
      "Epoch 1243/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01243: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0317 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01244: LearningRateScheduler reducing learning rate to 0.000288834314102927.\n",
      "Epoch 1244/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 01244: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0333 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01245: LearningRateScheduler reducing learning rate to 0.00028881652728373545.\n",
      "Epoch 1245/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01245: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0335 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01246: LearningRateScheduler reducing learning rate to 0.00028879872686302457.\n",
      "Epoch 1246/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01246: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0340 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01247: LearningRateScheduler reducing learning rate to 0.00028878091284255755.\n",
      "Epoch 1247/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0444\n",
      "Epoch 01247: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0444 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01248: LearningRateScheduler reducing learning rate to 0.000288763085224099.\n",
      "Epoch 1248/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01248: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0380 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01249: LearningRateScheduler reducing learning rate to 0.00028874524400941464.\n",
      "Epoch 1249/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01249: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0287 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01250: LearningRateScheduler reducing learning rate to 0.0002887273892002717.\n",
      "Epoch 1250/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0529\n",
      "Epoch 01250: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0529 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01251: LearningRateScheduler reducing learning rate to 0.00028870952079843883.\n",
      "Epoch 1251/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0589\n",
      "Epoch 01251: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0589 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01252: LearningRateScheduler reducing learning rate to 0.0002886916388056859.\n",
      "Epoch 1252/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01252: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0353 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01253: LearningRateScheduler reducing learning rate to 0.00028867374322378414.\n",
      "Epoch 1253/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0444\n",
      "Epoch 01253: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0444 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01254: LearningRateScheduler reducing learning rate to 0.0002886558340545062.\n",
      "Epoch 1254/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 01254: val_loss did not improve from 0.02714\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0232 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01255: LearningRateScheduler reducing learning rate to 0.00028863791129962594.\n",
      "Epoch 1255/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01255: val_loss improved from 0.02714 to 0.02549, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0310 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01256: LearningRateScheduler reducing learning rate to 0.0002886199749609187.\n",
      "Epoch 1256/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0312\n",
      "Epoch 01256: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0312 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01257: LearningRateScheduler reducing learning rate to 0.00028860202504016113.\n",
      "Epoch 1257/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0548\n",
      "Epoch 01257: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0548 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01258: LearningRateScheduler reducing learning rate to 0.00028858406153913115.\n",
      "Epoch 1258/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01258: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0258 - val_loss: 0.0320\n",
      "\n",
      "Epoch 01259: LearningRateScheduler reducing learning rate to 0.0002885660844596082.\n",
      "Epoch 1259/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 01259: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0306 - val_loss: 0.0326\n",
      "\n",
      "Epoch 01260: LearningRateScheduler reducing learning rate to 0.00028854809380337285.\n",
      "Epoch 1260/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0426\n",
      "Epoch 01260: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0426 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01261: LearningRateScheduler reducing learning rate to 0.00028853008957220713.\n",
      "Epoch 1261/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01261: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0413 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01262: LearningRateScheduler reducing learning rate to 0.00028851207176789446.\n",
      "Epoch 1262/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0286\n",
      "Epoch 01262: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0286 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01263: LearningRateScheduler reducing learning rate to 0.00028849404039221944.\n",
      "Epoch 1263/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0394\n",
      "Epoch 01263: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0394 - val_loss: 0.0312\n",
      "\n",
      "Epoch 01264: LearningRateScheduler reducing learning rate to 0.00028847599544696824.\n",
      "Epoch 1264/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0376\n",
      "Epoch 01264: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0376 - val_loss: 0.0364\n",
      "\n",
      "Epoch 01265: LearningRateScheduler reducing learning rate to 0.00028845793693392817.\n",
      "Epoch 1265/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0432\n",
      "Epoch 01265: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0432 - val_loss: 0.0386\n",
      "\n",
      "Epoch 01266: LearningRateScheduler reducing learning rate to 0.000288439864854888.\n",
      "Epoch 1266/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513\n",
      "Epoch 01266: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0513 - val_loss: 0.0390\n",
      "\n",
      "Epoch 01267: LearningRateScheduler reducing learning rate to 0.0002884217792116378.\n",
      "Epoch 1267/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01267: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0274 - val_loss: 0.0419\n",
      "\n",
      "Epoch 01268: LearningRateScheduler reducing learning rate to 0.000288403680005969.\n",
      "Epoch 1268/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01268: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0338 - val_loss: 0.0440\n",
      "\n",
      "Epoch 01269: LearningRateScheduler reducing learning rate to 0.0002883855672396744.\n",
      "Epoch 1269/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0440\n",
      "Epoch 01269: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0440 - val_loss: 0.0442\n",
      "\n",
      "Epoch 01270: LearningRateScheduler reducing learning rate to 0.000288367440914548.\n",
      "Epoch 1270/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01270: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0322 - val_loss: 0.0421\n",
      "\n",
      "Epoch 01271: LearningRateScheduler reducing learning rate to 0.0002883493010323854.\n",
      "Epoch 1271/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 01271: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0313 - val_loss: 0.0403\n",
      "\n",
      "Epoch 01272: LearningRateScheduler reducing learning rate to 0.0002883311475949833.\n",
      "Epoch 1272/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476\n",
      "Epoch 01272: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0476 - val_loss: 0.0398\n",
      "\n",
      "Epoch 01273: LearningRateScheduler reducing learning rate to 0.00028831298060413987.\n",
      "Epoch 1273/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0436\n",
      "Epoch 01273: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0436 - val_loss: 0.0329\n",
      "\n",
      "Epoch 01274: LearningRateScheduler reducing learning rate to 0.00028829480006165465.\n",
      "Epoch 1274/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0452\n",
      "Epoch 01274: val_loss did not improve from 0.02549\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0452 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01275: LearningRateScheduler reducing learning rate to 0.00028827660596932833.\n",
      "Epoch 1275/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0465\n",
      "Epoch 01275: val_loss improved from 0.02549 to 0.02452, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0465 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01276: LearningRateScheduler reducing learning rate to 0.00028825839832896324.\n",
      "Epoch 1276/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0362\n",
      "Epoch 01276: val_loss improved from 0.02452 to 0.02104, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0362 - val_loss: 0.0210\n",
      "\n",
      "Epoch 01277: LearningRateScheduler reducing learning rate to 0.00028824017714236273.\n",
      "Epoch 1277/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0543\n",
      "Epoch 01277: val_loss did not improve from 0.02104\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0543 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01278: LearningRateScheduler reducing learning rate to 0.00028822194241133177.\n",
      "Epoch 1278/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01278: val_loss did not improve from 0.02104\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0354 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01279: LearningRateScheduler reducing learning rate to 0.00028820369413767643.\n",
      "Epoch 1279/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0355\n",
      "Epoch 01279: val_loss did not improve from 0.02104\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0355 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01280: LearningRateScheduler reducing learning rate to 0.0002881854323232044.\n",
      "Epoch 1280/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0568\n",
      "Epoch 01280: val_loss did not improve from 0.02104\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0568 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01281: LearningRateScheduler reducing learning rate to 0.00028816715696972447.\n",
      "Epoch 1281/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 01281: val_loss did not improve from 0.02104\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0368 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01282: LearningRateScheduler reducing learning rate to 0.0002881488680790468.\n",
      "Epoch 1282/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0593\n",
      "Epoch 01282: val_loss improved from 0.02104 to 0.02098, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0593 - val_loss: 0.0210\n",
      "\n",
      "Epoch 01283: LearningRateScheduler reducing learning rate to 0.00028813056565298305.\n",
      "Epoch 1283/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0427\n",
      "Epoch 01283: val_loss did not improve from 0.02098\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0427 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01284: LearningRateScheduler reducing learning rate to 0.0002881122496933461.\n",
      "Epoch 1284/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 01284: val_loss did not improve from 0.02098\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0387 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01285: LearningRateScheduler reducing learning rate to 0.00028809392020195013.\n",
      "Epoch 1285/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0414\n",
      "Epoch 01285: val_loss did not improve from 0.02098\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0414 - val_loss: 0.0219\n",
      "\n",
      "Epoch 01286: LearningRateScheduler reducing learning rate to 0.0002880755771806108.\n",
      "Epoch 1286/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505\n",
      "Epoch 01286: val_loss improved from 0.02098 to 0.02006, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0505 - val_loss: 0.0201\n",
      "\n",
      "Epoch 01287: LearningRateScheduler reducing learning rate to 0.0002880572206311449.\n",
      "Epoch 1287/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01287: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0348 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01288: LearningRateScheduler reducing learning rate to 0.00028803885055537083.\n",
      "Epoch 1288/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0417\n",
      "Epoch 01288: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0417 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01289: LearningRateScheduler reducing learning rate to 0.0002880204669551081.\n",
      "Epoch 1289/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486\n",
      "Epoch 01289: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0486 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01290: LearningRateScheduler reducing learning rate to 0.0002880020698321777.\n",
      "Epoch 1290/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0312\n",
      "Epoch 01290: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0312 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01291: LearningRateScheduler reducing learning rate to 0.00028798365918840185.\n",
      "Epoch 1291/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0517\n",
      "Epoch 01291: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0517 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01292: LearningRateScheduler reducing learning rate to 0.00028796523502560423.\n",
      "Epoch 1292/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481\n",
      "Epoch 01292: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0481 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01293: LearningRateScheduler reducing learning rate to 0.00028794679734560976.\n",
      "Epoch 1293/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01293: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0303 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01294: LearningRateScheduler reducing learning rate to 0.0002879283461502447.\n",
      "Epoch 1294/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01294: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0348 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01295: LearningRateScheduler reducing learning rate to 0.0002879098814413368.\n",
      "Epoch 1295/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0583\n",
      "Epoch 01295: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0583 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01296: LearningRateScheduler reducing learning rate to 0.0002878914032207149.\n",
      "Epoch 1296/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 01296: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0343 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01297: LearningRateScheduler reducing learning rate to 0.00028787291149020937.\n",
      "Epoch 1297/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0409\n",
      "Epoch 01297: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0409 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01298: LearningRateScheduler reducing learning rate to 0.0002878544062516519.\n",
      "Epoch 1298/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 01298: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0383 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01299: LearningRateScheduler reducing learning rate to 0.0002878358875068754.\n",
      "Epoch 1299/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0560\n",
      "Epoch 01299: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0560 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01300: LearningRateScheduler reducing learning rate to 0.0002878173552577142.\n",
      "Epoch 1300/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01300: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0348 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01301: LearningRateScheduler reducing learning rate to 0.00028779880950600403.\n",
      "Epoch 1301/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01301: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0420 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01302: LearningRateScheduler reducing learning rate to 0.0002877802502535818.\n",
      "Epoch 1302/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0583\n",
      "Epoch 01302: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0583 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01303: LearningRateScheduler reducing learning rate to 0.00028776167750228603.\n",
      "Epoch 1303/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0448\n",
      "Epoch 01303: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0448 - val_loss: 0.0337\n",
      "\n",
      "Epoch 01304: LearningRateScheduler reducing learning rate to 0.00028774309125395613.\n",
      "Epoch 1304/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 01304: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0375 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01305: LearningRateScheduler reducing learning rate to 0.00028772449151043326.\n",
      "Epoch 1305/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0321\n",
      "Epoch 01305: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0321 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01306: LearningRateScheduler reducing learning rate to 0.00028770587827355975.\n",
      "Epoch 1306/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01306: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0294 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01307: LearningRateScheduler reducing learning rate to 0.0002876872515451793.\n",
      "Epoch 1307/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0432\n",
      "Epoch 01307: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0432 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01308: LearningRateScheduler reducing learning rate to 0.0002876686113271369.\n",
      "Epoch 1308/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01308: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0334 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01309: LearningRateScheduler reducing learning rate to 0.0002876499576212789.\n",
      "Epoch 1309/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0282\n",
      "Epoch 01309: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0282 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01310: LearningRateScheduler reducing learning rate to 0.00028763129042945305.\n",
      "Epoch 1310/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0417\n",
      "Epoch 01310: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0417 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01311: LearningRateScheduler reducing learning rate to 0.00028761260975350834.\n",
      "Epoch 1311/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0453\n",
      "Epoch 01311: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0453 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01312: LearningRateScheduler reducing learning rate to 0.0002875939155952951.\n",
      "Epoch 1312/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0347\n",
      "Epoch 01312: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0347 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01313: LearningRateScheduler reducing learning rate to 0.0002875752079566651.\n",
      "Epoch 1313/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0356\n",
      "Epoch 01313: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0356 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01314: LearningRateScheduler reducing learning rate to 0.0002875564868394713.\n",
      "Epoch 1314/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463\n",
      "Epoch 01314: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0463 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01315: LearningRateScheduler reducing learning rate to 0.0002875377522455681.\n",
      "Epoch 1315/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0422\n",
      "Epoch 01315: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0422 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01316: LearningRateScheduler reducing learning rate to 0.0002875190041768113.\n",
      "Epoch 1316/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 01316: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 0.0370 - val_loss: 0.0296\n",
      "\n",
      "Epoch 01317: LearningRateScheduler reducing learning rate to 0.00028750024263505777.\n",
      "Epoch 1317/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0491\n",
      "Epoch 01317: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0491 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01318: LearningRateScheduler reducing learning rate to 0.000287481467622166.\n",
      "Epoch 1318/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01318: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0240 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01319: LearningRateScheduler reducing learning rate to 0.00028746267913999564.\n",
      "Epoch 1319/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0465\n",
      "Epoch 01319: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0465 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01320: LearningRateScheduler reducing learning rate to 0.0002874438771904078.\n",
      "Epoch 1320/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01320: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0420 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01321: LearningRateScheduler reducing learning rate to 0.00028742506177526476.\n",
      "Epoch 1321/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0305\n",
      "Epoch 01321: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0305 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01322: LearningRateScheduler reducing learning rate to 0.0002874062328964303.\n",
      "Epoch 1322/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0440\n",
      "Epoch 01322: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0440 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01323: LearningRateScheduler reducing learning rate to 0.0002873873905557695.\n",
      "Epoch 1323/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0377\n",
      "Epoch 01323: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0377 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01324: LearningRateScheduler reducing learning rate to 0.0002873685347551486.\n",
      "Epoch 1324/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01324: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0288 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01325: LearningRateScheduler reducing learning rate to 0.00028734966549643547.\n",
      "Epoch 1325/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0444\n",
      "Epoch 01325: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0444 - val_loss: 0.0311\n",
      "\n",
      "Epoch 01326: LearningRateScheduler reducing learning rate to 0.00028733078278149905.\n",
      "Epoch 1326/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457\n",
      "Epoch 01326: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0457 - val_loss: 0.0326\n",
      "\n",
      "Epoch 01327: LearningRateScheduler reducing learning rate to 0.00028731188661220976.\n",
      "Epoch 1327/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0404\n",
      "Epoch 01327: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0404 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01328: LearningRateScheduler reducing learning rate to 0.0002872929769904393.\n",
      "Epoch 1328/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0438\n",
      "Epoch 01328: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0438 - val_loss: 0.0322\n",
      "\n",
      "Epoch 01329: LearningRateScheduler reducing learning rate to 0.00028727405391806073.\n",
      "Epoch 1329/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0491\n",
      "Epoch 01329: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0491 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01330: LearningRateScheduler reducing learning rate to 0.0002872551173969483.\n",
      "Epoch 1330/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01330: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0334 - val_loss: 0.0321\n",
      "\n",
      "Epoch 01331: LearningRateScheduler reducing learning rate to 0.0002872361674289779.\n",
      "Epoch 1331/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0501\n",
      "Epoch 01331: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0501 - val_loss: 0.0296\n",
      "\n",
      "Epoch 01332: LearningRateScheduler reducing learning rate to 0.0002872172040160265.\n",
      "Epoch 1332/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0406\n",
      "Epoch 01332: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0406 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01333: LearningRateScheduler reducing learning rate to 0.00028719822715997243.\n",
      "Epoch 1333/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01333: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0369 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01334: LearningRateScheduler reducing learning rate to 0.0002871792368626954.\n",
      "Epoch 1334/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01334: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0291 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01335: LearningRateScheduler reducing learning rate to 0.0002871602331260765.\n",
      "Epoch 1335/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0356\n",
      "Epoch 01335: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01336: LearningRateScheduler reducing learning rate to 0.000287141215951998.\n",
      "Epoch 1336/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0367\n",
      "Epoch 01336: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0367 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01337: LearningRateScheduler reducing learning rate to 0.0002871221853423437.\n",
      "Epoch 1337/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0307\n",
      "Epoch 01337: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0307 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01338: LearningRateScheduler reducing learning rate to 0.0002871031412989986.\n",
      "Epoch 1338/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01338: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0340 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01339: LearningRateScheduler reducing learning rate to 0.00028708408382384896.\n",
      "Epoch 1339/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01339: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0329 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01340: LearningRateScheduler reducing learning rate to 0.00028706501291878256.\n",
      "Epoch 1340/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01340: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0407 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01341: LearningRateScheduler reducing learning rate to 0.0002870459285856884.\n",
      "Epoch 1341/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 01341: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0333 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01342: LearningRateScheduler reducing learning rate to 0.00028702683082645685.\n",
      "Epoch 1342/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 01342: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0336 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01343: LearningRateScheduler reducing learning rate to 0.0002870077196429795.\n",
      "Epoch 1343/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01343: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0354 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01344: LearningRateScheduler reducing learning rate to 0.0002869885950371495.\n",
      "Epoch 1344/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0371\n",
      "Epoch 01344: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0371 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01345: LearningRateScheduler reducing learning rate to 0.000286969457010861.\n",
      "Epoch 1345/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0405\n",
      "Epoch 01345: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0405 - val_loss: 0.0296\n",
      "\n",
      "Epoch 01346: LearningRateScheduler reducing learning rate to 0.00028695030556600984.\n",
      "Epoch 1346/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 01346: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0349 - val_loss: 0.0314\n",
      "\n",
      "Epoch 01347: LearningRateScheduler reducing learning rate to 0.00028693114070449296.\n",
      "Epoch 1347/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01347: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0402 - val_loss: 0.0330\n",
      "\n",
      "Epoch 01348: LearningRateScheduler reducing learning rate to 0.0002869119624282086.\n",
      "Epoch 1348/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01348: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0299 - val_loss: 0.0317\n",
      "\n",
      "Epoch 01349: LearningRateScheduler reducing learning rate to 0.0002868927707390565.\n",
      "Epoch 1349/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0366\n",
      "Epoch 01349: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0366 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01350: LearningRateScheduler reducing learning rate to 0.0002868735656389377.\n",
      "Epoch 1350/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0444\n",
      "Epoch 01350: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0444 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01351: LearningRateScheduler reducing learning rate to 0.0002868543471297543.\n",
      "Epoch 1351/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01351: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0369 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01352: LearningRateScheduler reducing learning rate to 0.0002868351152134102.\n",
      "Epoch 1352/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01352: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0335 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01353: LearningRateScheduler reducing learning rate to 0.00028681586989181017.\n",
      "Epoch 1353/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0347\n",
      "Epoch 01353: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0347 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01354: LearningRateScheduler reducing learning rate to 0.0002867966111668606.\n",
      "Epoch 1354/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0377\n",
      "Epoch 01354: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0377 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01355: LearningRateScheduler reducing learning rate to 0.00028677733904046903.\n",
      "Epoch 1355/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0359\n",
      "Epoch 01355: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0359 - val_loss: 0.0311\n",
      "\n",
      "Epoch 01356: LearningRateScheduler reducing learning rate to 0.0002867580535145445.\n",
      "Epoch 1356/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01356: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0353 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01357: LearningRateScheduler reducing learning rate to 0.0002867387545909972.\n",
      "Epoch 1357/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471\n",
      "Epoch 01357: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0471 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01358: LearningRateScheduler reducing learning rate to 0.0002867194422717388.\n",
      "Epoch 1358/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01358: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0283 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01359: LearningRateScheduler reducing learning rate to 0.0002867001165586822.\n",
      "Epoch 1359/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01359: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0348 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01360: LearningRateScheduler reducing learning rate to 0.0002866807774537417.\n",
      "Epoch 1360/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0389\n",
      "Epoch 01360: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0389 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01361: LearningRateScheduler reducing learning rate to 0.00028666142495883284.\n",
      "Epoch 1361/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0355\n",
      "Epoch 01361: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0355 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01362: LearningRateScheduler reducing learning rate to 0.0002866420590758725.\n",
      "Epoch 1362/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480\n",
      "Epoch 01362: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0480 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01363: LearningRateScheduler reducing learning rate to 0.00028662267980677895.\n",
      "Epoch 1363/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0429\n",
      "Epoch 01363: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0429 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01364: LearningRateScheduler reducing learning rate to 0.0002866032871534718.\n",
      "Epoch 1364/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0422\n",
      "Epoch 01364: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0422 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01365: LearningRateScheduler reducing learning rate to 0.00028658388111787184.\n",
      "Epoch 1365/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0392\n",
      "Epoch 01365: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0392 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01366: LearningRateScheduler reducing learning rate to 0.0002865644617019014.\n",
      "Epoch 1366/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 01366: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0320 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01367: LearningRateScheduler reducing learning rate to 0.00028654502890748386.\n",
      "Epoch 1367/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01367: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0280 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01368: LearningRateScheduler reducing learning rate to 0.0002865255827365442.\n",
      "Epoch 1368/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463\n",
      "Epoch 01368: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0463 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01369: LearningRateScheduler reducing learning rate to 0.0002865061231910087.\n",
      "Epoch 1369/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01369: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0329 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01370: LearningRateScheduler reducing learning rate to 0.00028648665027280454.\n",
      "Epoch 1370/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01370: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0288 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01371: LearningRateScheduler reducing learning rate to 0.00028646716398386093.\n",
      "Epoch 1371/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01371: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0369 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01372: LearningRateScheduler reducing learning rate to 0.0002864476643261078.\n",
      "Epoch 1372/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01372: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0298 - val_loss: 0.0337\n",
      "\n",
      "Epoch 01373: LearningRateScheduler reducing learning rate to 0.00028642815130147675.\n",
      "Epoch 1373/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0371\n",
      "Epoch 01373: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0371 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01374: LearningRateScheduler reducing learning rate to 0.0002864086249119006.\n",
      "Epoch 1374/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 01374: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0349 - val_loss: 0.0316\n",
      "\n",
      "Epoch 01375: LearningRateScheduler reducing learning rate to 0.00028638908515931337.\n",
      "Epoch 1375/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0412\n",
      "Epoch 01375: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0412 - val_loss: 0.0290\n",
      "\n",
      "Epoch 01376: LearningRateScheduler reducing learning rate to 0.00028636953204565056.\n",
      "Epoch 1376/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01376: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0289 - val_loss: 0.0290\n",
      "\n",
      "Epoch 01377: LearningRateScheduler reducing learning rate to 0.00028634996557284903.\n",
      "Epoch 1377/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01377: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0354 - val_loss: 0.0331\n",
      "\n",
      "Epoch 01378: LearningRateScheduler reducing learning rate to 0.0002863303857428468.\n",
      "Epoch 1378/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489\n",
      "Epoch 01378: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0489 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01379: LearningRateScheduler reducing learning rate to 0.0002863107925575833.\n",
      "Epoch 1379/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01379: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0265 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01380: LearningRateScheduler reducing learning rate to 0.00028629118601899934.\n",
      "Epoch 1380/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 01380: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0330 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01381: LearningRateScheduler reducing learning rate to 0.00028627156612903693.\n",
      "Epoch 1381/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 01381: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0387 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01382: LearningRateScheduler reducing learning rate to 0.0002862519328896395.\n",
      "Epoch 1382/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466\n",
      "Epoch 01382: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0466 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01383: LearningRateScheduler reducing learning rate to 0.00028623228630275183.\n",
      "Epoch 1383/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0406\n",
      "Epoch 01383: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0406 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01384: LearningRateScheduler reducing learning rate to 0.0002862126263703198.\n",
      "Epoch 1384/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0563\n",
      "Epoch 01384: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0563 - val_loss: 0.0205\n",
      "\n",
      "Epoch 01385: LearningRateScheduler reducing learning rate to 0.0002861929530942909.\n",
      "Epoch 1385/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0388\n",
      "Epoch 01385: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0388 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01386: LearningRateScheduler reducing learning rate to 0.0002861732664766138.\n",
      "Epoch 1386/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0405\n",
      "Epoch 01386: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0405 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01387: LearningRateScheduler reducing learning rate to 0.00028615356651923846.\n",
      "Epoch 1387/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0385\n",
      "Epoch 01387: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0385 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01388: LearningRateScheduler reducing learning rate to 0.00028613385322411624.\n",
      "Epoch 1388/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01388: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0329 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01389: LearningRateScheduler reducing learning rate to 0.0002861141265931997.\n",
      "Epoch 1389/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01389: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0391 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01390: LearningRateScheduler reducing learning rate to 0.00028609438662844294.\n",
      "Epoch 1390/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0359\n",
      "Epoch 01390: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0359 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01391: LearningRateScheduler reducing learning rate to 0.0002860746333318012.\n",
      "Epoch 1391/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0360\n",
      "Epoch 01391: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0360 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01392: LearningRateScheduler reducing learning rate to 0.0002860548667052311.\n",
      "Epoch 1392/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 01392: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0325 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01393: LearningRateScheduler reducing learning rate to 0.00028603508675069053.\n",
      "Epoch 1393/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0305\n",
      "Epoch 01393: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0305 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01394: LearningRateScheduler reducing learning rate to 0.0002860152934701388.\n",
      "Epoch 1394/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01394: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0283 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01395: LearningRateScheduler reducing learning rate to 0.0002859954868655364.\n",
      "Epoch 1395/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0266\n",
      "Epoch 01395: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0266 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01396: LearningRateScheduler reducing learning rate to 0.00028597566693884524.\n",
      "Epoch 1396/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0311\n",
      "Epoch 01396: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0311 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01397: LearningRateScheduler reducing learning rate to 0.00028595583369202856.\n",
      "Epoch 1397/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0366\n",
      "Epoch 01397: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0366 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01398: LearningRateScheduler reducing learning rate to 0.0002859359871270509.\n",
      "Epoch 1398/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01398: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0413 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01399: LearningRateScheduler reducing learning rate to 0.0002859161272458781.\n",
      "Epoch 1399/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 01399: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0306 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01400: LearningRateScheduler reducing learning rate to 0.0002858962540504773.\n",
      "Epoch 1400/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 01400: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0241 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01401: LearningRateScheduler reducing learning rate to 0.00028587636754281705.\n",
      "Epoch 1401/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01401: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0298 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01402: LearningRateScheduler reducing learning rate to 0.00028585646772486705.\n",
      "Epoch 1402/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 01402: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0260 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01403: LearningRateScheduler reducing learning rate to 0.0002858365545985985.\n",
      "Epoch 1403/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01403: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0299 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01404: LearningRateScheduler reducing learning rate to 0.00028581662816598384.\n",
      "Epoch 1404/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01404: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0369 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01405: LearningRateScheduler reducing learning rate to 0.0002857966884289968.\n",
      "Epoch 1405/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0406\n",
      "Epoch 01405: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0406 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01406: LearningRateScheduler reducing learning rate to 0.00028577673538961244.\n",
      "Epoch 1406/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01406: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0380 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01407: LearningRateScheduler reducing learning rate to 0.00028575676904980715.\n",
      "Epoch 1407/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490\n",
      "Epoch 01407: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0490 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01408: LearningRateScheduler reducing learning rate to 0.0002857367894115588.\n",
      "Epoch 1408/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0360\n",
      "Epoch 01408: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0360 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01409: LearningRateScheduler reducing learning rate to 0.00028571679647684616.\n",
      "Epoch 1409/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01409: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0348 - val_loss: 0.0366\n",
      "\n",
      "Epoch 01410: LearningRateScheduler reducing learning rate to 0.00028569679024764976.\n",
      "Epoch 1410/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0362\n",
      "Epoch 01410: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0362 - val_loss: 0.0345\n",
      "\n",
      "Epoch 01411: LearningRateScheduler reducing learning rate to 0.0002856767707259513.\n",
      "Epoch 1411/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 01411: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0331 - val_loss: 0.0318\n",
      "\n",
      "Epoch 01412: LearningRateScheduler reducing learning rate to 0.00028565673791373355.\n",
      "Epoch 1412/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0545\n",
      "Epoch 01412: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0545 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01413: LearningRateScheduler reducing learning rate to 0.00028563669181298095.\n",
      "Epoch 1413/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01413: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0287 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01414: LearningRateScheduler reducing learning rate to 0.0002856166324256791.\n",
      "Epoch 1414/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01414: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0348 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01415: LearningRateScheduler reducing learning rate to 0.00028559655975381496.\n",
      "Epoch 1415/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0433\n",
      "Epoch 01415: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0433 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01416: LearningRateScheduler reducing learning rate to 0.0002855764737993767.\n",
      "Epoch 1416/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0415\n",
      "Epoch 01416: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0415 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01417: LearningRateScheduler reducing learning rate to 0.00028555637456435394.\n",
      "Epoch 1417/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01417: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0402 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01418: LearningRateScheduler reducing learning rate to 0.00028553626205073747.\n",
      "Epoch 1418/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0358\n",
      "Epoch 01418: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0358 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01419: LearningRateScheduler reducing learning rate to 0.0002855161362605196.\n",
      "Epoch 1419/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0424\n",
      "Epoch 01419: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0424 - val_loss: 0.0254\n",
      "\n",
      "Epoch 01420: LearningRateScheduler reducing learning rate to 0.0002854959971956938.\n",
      "Epoch 1420/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01420: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0393 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01421: LearningRateScheduler reducing learning rate to 0.0002854758448582548.\n",
      "Epoch 1421/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 01421: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0316 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01422: LearningRateScheduler reducing learning rate to 0.00028545567925019884.\n",
      "Epoch 1422/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01422: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0299 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01423: LearningRateScheduler reducing learning rate to 0.00028543550037352333.\n",
      "Epoch 1423/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01423: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0295 - val_loss: 0.0344\n",
      "\n",
      "Epoch 01424: LearningRateScheduler reducing learning rate to 0.000285415308230227.\n",
      "Epoch 1424/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0400\n",
      "Epoch 01424: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0400 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01425: LearningRateScheduler reducing learning rate to 0.00028539510282231005.\n",
      "Epoch 1425/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01425: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0393 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01426: LearningRateScheduler reducing learning rate to 0.00028537488415177374.\n",
      "Epoch 1426/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0231\n",
      "Epoch 01426: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0231 - val_loss: 0.0210\n",
      "\n",
      "Epoch 01427: LearningRateScheduler reducing learning rate to 0.0002853546522206208.\n",
      "Epoch 1427/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 01427: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0208 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01428: LearningRateScheduler reducing learning rate to 0.0002853344070308553.\n",
      "Epoch 1428/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01428: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0300 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01429: LearningRateScheduler reducing learning rate to 0.0002853141485844826.\n",
      "Epoch 1429/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01429: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0303 - val_loss: 0.0254\n",
      "\n",
      "Epoch 01430: LearningRateScheduler reducing learning rate to 0.0002852938768835092.\n",
      "Epoch 1430/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01430: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0326 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01431: LearningRateScheduler reducing learning rate to 0.0002852735919299432.\n",
      "Epoch 1431/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01431: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0236 - val_loss: 0.0289\n",
      "\n",
      "Epoch 01432: LearningRateScheduler reducing learning rate to 0.0002852532937257938.\n",
      "Epoch 1432/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01432: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0291 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01433: LearningRateScheduler reducing learning rate to 0.00028523298227307165.\n",
      "Epoch 1433/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01433: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0289 - val_loss: 0.0283\n",
      "\n",
      "Epoch 01434: LearningRateScheduler reducing learning rate to 0.0002852126575737886.\n",
      "Epoch 1434/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 01434: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0235 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01435: LearningRateScheduler reducing learning rate to 0.00028519231962995784.\n",
      "Epoch 1435/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0404\n",
      "Epoch 01435: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0404 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01436: LearningRateScheduler reducing learning rate to 0.00028517196844359395.\n",
      "Epoch 1436/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 01436: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0202 - val_loss: 0.0294\n",
      "\n",
      "Epoch 01437: LearningRateScheduler reducing learning rate to 0.0002851516040167127.\n",
      "Epoch 1437/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01437: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0295 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01438: LearningRateScheduler reducing learning rate to 0.0002851312263513312.\n",
      "Epoch 1438/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0379\n",
      "Epoch 01438: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0379 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01439: LearningRateScheduler reducing learning rate to 0.00028511083544946806.\n",
      "Epoch 1439/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0410\n",
      "Epoch 01439: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0410 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01440: LearningRateScheduler reducing learning rate to 0.0002850904313131429.\n",
      "Epoch 1440/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 01440: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0284 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01441: LearningRateScheduler reducing learning rate to 0.00028507001394437686.\n",
      "Epoch 1441/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01441: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0337 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01442: LearningRateScheduler reducing learning rate to 0.0002850495833451923.\n",
      "Epoch 1442/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 01442: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0306 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01443: LearningRateScheduler reducing learning rate to 0.00028502913951761297.\n",
      "Epoch 1443/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 01443: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0363 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01444: LearningRateScheduler reducing learning rate to 0.0002850086824636638.\n",
      "Epoch 1444/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0671\n",
      "Epoch 01444: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0671 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01445: LearningRateScheduler reducing learning rate to 0.0002849882121853711.\n",
      "Epoch 1445/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 01445: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0296 - val_loss: 0.0254\n",
      "\n",
      "Epoch 01446: LearningRateScheduler reducing learning rate to 0.0002849677286847627.\n",
      "Epoch 1446/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01446: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0332 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01447: LearningRateScheduler reducing learning rate to 0.00028494723196386724.\n",
      "Epoch 1447/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 01447: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0327 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01448: LearningRateScheduler reducing learning rate to 0.00028492672202471515.\n",
      "Epoch 1448/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01448: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0213 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01449: LearningRateScheduler reducing learning rate to 0.000284906198869338.\n",
      "Epoch 1449/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01449: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0329 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01450: LearningRateScheduler reducing learning rate to 0.00028488566249976855.\n",
      "Epoch 1450/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0388\n",
      "Epoch 01450: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0388 - val_loss: 0.0292\n",
      "\n",
      "Epoch 01451: LearningRateScheduler reducing learning rate to 0.00028486511291804105.\n",
      "Epoch 1451/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0361\n",
      "Epoch 01451: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0361 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01452: LearningRateScheduler reducing learning rate to 0.000284844550126191.\n",
      "Epoch 1452/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01452: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0269 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01453: LearningRateScheduler reducing learning rate to 0.0002848239741262551.\n",
      "Epoch 1453/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0542\n",
      "Epoch 01453: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0542 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01454: LearningRateScheduler reducing learning rate to 0.0002848033849202715.\n",
      "Epoch 1454/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01454: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0315 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01455: LearningRateScheduler reducing learning rate to 0.0002847827825102797.\n",
      "Epoch 1455/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01455: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0245 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01456: LearningRateScheduler reducing learning rate to 0.00028476216689832023.\n",
      "Epoch 1456/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462\n",
      "Epoch 01456: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0462 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01457: LearningRateScheduler reducing learning rate to 0.0002847415380864353.\n",
      "Epoch 1457/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01457: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0294 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01458: LearningRateScheduler reducing learning rate to 0.0002847208960766681.\n",
      "Epoch 1458/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0401\n",
      "Epoch 01458: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0401 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01459: LearningRateScheduler reducing learning rate to 0.0002847002408710633.\n",
      "Epoch 1459/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01459: val_loss did not improve from 0.02006\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0278 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01460: LearningRateScheduler reducing learning rate to 0.00028467957247166694.\n",
      "Epoch 1460/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 01460: val_loss improved from 0.02006 to 0.01994, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0253 - val_loss: 0.0199\n",
      "\n",
      "Epoch 01461: LearningRateScheduler reducing learning rate to 0.0002846588908805262.\n",
      "Epoch 1461/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0389\n",
      "Epoch 01461: val_loss improved from 0.01994 to 0.01808, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0389 - val_loss: 0.0181\n",
      "\n",
      "Epoch 01462: LearningRateScheduler reducing learning rate to 0.0002846381960996896.\n",
      "Epoch 1462/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01462: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0353 - val_loss: 0.0195\n",
      "\n",
      "Epoch 01463: LearningRateScheduler reducing learning rate to 0.0002846174881312071.\n",
      "Epoch 1463/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0286\n",
      "Epoch 01463: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0286 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01464: LearningRateScheduler reducing learning rate to 0.0002845967669771298.\n",
      "Epoch 1464/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01464: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0338 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01465: LearningRateScheduler reducing learning rate to 0.00028457603263951016.\n",
      "Epoch 1465/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01465: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0236 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01466: LearningRateScheduler reducing learning rate to 0.00028455528512040204.\n",
      "Epoch 1466/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 01466: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0370 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01467: LearningRateScheduler reducing learning rate to 0.00028453452442186045.\n",
      "Epoch 1467/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462\n",
      "Epoch 01467: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0462 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01468: LearningRateScheduler reducing learning rate to 0.00028451375054594193.\n",
      "Epoch 1468/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01468: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0372 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01469: LearningRateScheduler reducing learning rate to 0.000284492963494704.\n",
      "Epoch 1469/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01469: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0354 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01470: LearningRateScheduler reducing learning rate to 0.00028447216327020576.\n",
      "Epoch 1470/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01470: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0335 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01471: LearningRateScheduler reducing learning rate to 0.0002844513498745075.\n",
      "Epoch 1471/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01471: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0265 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01472: LearningRateScheduler reducing learning rate to 0.00028443052330967084.\n",
      "Epoch 1472/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 01472: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0327 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01473: LearningRateScheduler reducing learning rate to 0.0002844096835777587.\n",
      "Epoch 1473/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01473: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0288 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01474: LearningRateScheduler reducing learning rate to 0.0002843888306808353.\n",
      "Epoch 1474/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01474: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01475: LearningRateScheduler reducing learning rate to 0.0002843679646209662.\n",
      "Epoch 1475/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01475: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0292 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01476: LearningRateScheduler reducing learning rate to 0.00028434708540021815.\n",
      "Epoch 1476/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0307\n",
      "Epoch 01476: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0307 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01477: LearningRateScheduler reducing learning rate to 0.00028432619302065935.\n",
      "Epoch 1477/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 01477: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0257 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01478: LearningRateScheduler reducing learning rate to 0.00028430528748435923.\n",
      "Epoch 1478/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 01478: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0290 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01479: LearningRateScheduler reducing learning rate to 0.0002842843687933886.\n",
      "Epoch 1479/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0221\n",
      "Epoch 01479: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0221 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01480: LearningRateScheduler reducing learning rate to 0.0002842634369498193.\n",
      "Epoch 1480/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 01480: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0302 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01481: LearningRateScheduler reducing learning rate to 0.0002842424919557249.\n",
      "Epoch 1481/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01481: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0295 - val_loss: 0.0254\n",
      "\n",
      "Epoch 01482: LearningRateScheduler reducing learning rate to 0.00028422153381317994.\n",
      "Epoch 1482/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01482: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0314 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01483: LearningRateScheduler reducing learning rate to 0.0002842005625242604.\n",
      "Epoch 1483/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0389\n",
      "Epoch 01483: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0389 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01484: LearningRateScheduler reducing learning rate to 0.0002841795780910435.\n",
      "Epoch 1484/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01484: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0265 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01485: LearningRateScheduler reducing learning rate to 0.0002841585805156078.\n",
      "Epoch 1485/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 01485: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0387 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01486: LearningRateScheduler reducing learning rate to 0.00028413756980003323.\n",
      "Epoch 1486/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0360\n",
      "Epoch 01486: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0360 - val_loss: 0.0314\n",
      "\n",
      "Epoch 01487: LearningRateScheduler reducing learning rate to 0.00028411654594640094.\n",
      "Epoch 1487/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0271\n",
      "Epoch 01487: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0271 - val_loss: 0.0348\n",
      "\n",
      "Epoch 01488: LearningRateScheduler reducing learning rate to 0.00028409550895679326.\n",
      "Epoch 1488/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01488: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0352 - val_loss: 0.0314\n",
      "\n",
      "Epoch 01489: LearningRateScheduler reducing learning rate to 0.0002840744588332941.\n",
      "Epoch 1489/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0411\n",
      "Epoch 01489: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0411 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01490: LearningRateScheduler reducing learning rate to 0.0002840533955779885.\n",
      "Epoch 1490/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 01490: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0223 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01491: LearningRateScheduler reducing learning rate to 0.0002840323191929627.\n",
      "Epoch 1491/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 01491: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0237 - val_loss: 0.0190\n",
      "\n",
      "Epoch 01492: LearningRateScheduler reducing learning rate to 0.0002840112296803045.\n",
      "Epoch 1492/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01492: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0298 - val_loss: 0.0201\n",
      "\n",
      "Epoch 01493: LearningRateScheduler reducing learning rate to 0.0002839901270421028.\n",
      "Epoch 1493/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0423\n",
      "Epoch 01493: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0423 - val_loss: 0.0198\n",
      "\n",
      "Epoch 01494: LearningRateScheduler reducing learning rate to 0.0002839690112804479.\n",
      "Epoch 1494/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480\n",
      "Epoch 01494: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0480 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01495: LearningRateScheduler reducing learning rate to 0.0002839478823974314.\n",
      "Epoch 1495/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0362\n",
      "Epoch 01495: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0362 - val_loss: 0.0205\n",
      "\n",
      "Epoch 01496: LearningRateScheduler reducing learning rate to 0.00028392674039514606.\n",
      "Epoch 1496/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0398\n",
      "Epoch 01496: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0398 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01497: LearningRateScheduler reducing learning rate to 0.0002839055852756861.\n",
      "Epoch 1497/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01497: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0332 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01498: LearningRateScheduler reducing learning rate to 0.00028388441704114704.\n",
      "Epoch 1498/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0376\n",
      "Epoch 01498: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0376 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01499: LearningRateScheduler reducing learning rate to 0.0002838632356936255.\n",
      "Epoch 1499/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0285\n",
      "Epoch 01499: val_loss did not improve from 0.01808\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0285 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01500: LearningRateScheduler reducing learning rate to 0.0002838420412352197.\n",
      "Epoch 1500/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01500: val_loss improved from 0.01808 to 0.01615, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0274 - val_loss: 0.0161\n",
      "\n",
      "Epoch 01501: LearningRateScheduler reducing learning rate to 0.0002838208336680289.\n",
      "Epoch 1501/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01501: val_loss improved from 0.01615 to 0.01614, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0249 - val_loss: 0.0161\n",
      "\n",
      "Epoch 01502: LearningRateScheduler reducing learning rate to 0.0002837996129941538.\n",
      "Epoch 1502/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 01502: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0228 - val_loss: 0.0171\n",
      "\n",
      "Epoch 01503: LearningRateScheduler reducing learning rate to 0.0002837783792156963.\n",
      "Epoch 1503/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01503: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0294 - val_loss: 0.0175\n",
      "\n",
      "Epoch 01504: LearningRateScheduler reducing learning rate to 0.0002837571323347598.\n",
      "Epoch 1504/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 01504: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0279 - val_loss: 0.0187\n",
      "\n",
      "Epoch 01505: LearningRateScheduler reducing learning rate to 0.00028373587235344867.\n",
      "Epoch 1505/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 01505: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0284 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01506: LearningRateScheduler reducing learning rate to 0.0002837145992738688.\n",
      "Epoch 1506/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01506: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0244 - val_loss: 0.0180\n",
      "\n",
      "Epoch 01507: LearningRateScheduler reducing learning rate to 0.00028369331309812746.\n",
      "Epoch 1507/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0521\n",
      "Epoch 01507: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0521 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01508: LearningRateScheduler reducing learning rate to 0.00028367201382833296.\n",
      "Epoch 1508/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0409\n",
      "Epoch 01508: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0409 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01509: LearningRateScheduler reducing learning rate to 0.00028365070146659513.\n",
      "Epoch 1509/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01509: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0393 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01510: LearningRateScheduler reducing learning rate to 0.000283629376015025.\n",
      "Epoch 1510/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 01510: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0235 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01511: LearningRateScheduler reducing learning rate to 0.00028360803747573484.\n",
      "Epoch 1511/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0304\n",
      "Epoch 01511: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0304 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01512: LearningRateScheduler reducing learning rate to 0.0002835866858508383.\n",
      "Epoch 1512/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 01512: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0345 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01513: LearningRateScheduler reducing learning rate to 0.00028356532114245036.\n",
      "Epoch 1513/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01513: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0322 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01514: LearningRateScheduler reducing learning rate to 0.00028354394335268723.\n",
      "Epoch 1514/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 01514: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0313 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01515: LearningRateScheduler reducing learning rate to 0.0002835225524836664.\n",
      "Epoch 1515/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01515: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0264 - val_loss: 0.0178\n",
      "\n",
      "Epoch 01516: LearningRateScheduler reducing learning rate to 0.0002835011485375067.\n",
      "Epoch 1516/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0324\n",
      "Epoch 01516: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0324 - val_loss: 0.0174\n",
      "\n",
      "Epoch 01517: LearningRateScheduler reducing learning rate to 0.0002834797315163282.\n",
      "Epoch 1517/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0426\n",
      "Epoch 01517: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0426 - val_loss: 0.0169\n",
      "\n",
      "Epoch 01518: LearningRateScheduler reducing learning rate to 0.0002834583014222524.\n",
      "Epoch 1518/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01518: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0283 - val_loss: 0.0162\n",
      "\n",
      "Epoch 01519: LearningRateScheduler reducing learning rate to 0.0002834368582574019.\n",
      "Epoch 1519/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01519: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0380 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01520: LearningRateScheduler reducing learning rate to 0.00028341540202390074.\n",
      "Epoch 1520/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 01520: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0345 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01521: LearningRateScheduler reducing learning rate to 0.0002833939327238742.\n",
      "Epoch 1521/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0312\n",
      "Epoch 01521: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0312 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01522: LearningRateScheduler reducing learning rate to 0.00028337245035944895.\n",
      "Epoch 1522/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01522: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0283 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01523: LearningRateScheduler reducing learning rate to 0.0002833509549327527.\n",
      "Epoch 1523/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01523: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0298 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01524: LearningRateScheduler reducing learning rate to 0.00028332944644591477.\n",
      "Epoch 1524/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 01524: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0208 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01525: LearningRateScheduler reducing learning rate to 0.0002833079249010655.\n",
      "Epoch 1525/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0411\n",
      "Epoch 01525: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0411 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01526: LearningRateScheduler reducing learning rate to 0.00028328639030033675.\n",
      "Epoch 1526/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 01526: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0209 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01527: LearningRateScheduler reducing learning rate to 0.00028326484264586154.\n",
      "Epoch 1527/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01527: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0262 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01528: LearningRateScheduler reducing learning rate to 0.00028324328193977423.\n",
      "Epoch 1528/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 01528: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0279 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01529: LearningRateScheduler reducing learning rate to 0.0002832217081842104.\n",
      "Epoch 1529/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0364\n",
      "Epoch 01529: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0364 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01530: LearningRateScheduler reducing learning rate to 0.000283200121381307.\n",
      "Epoch 1530/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0395\n",
      "Epoch 01530: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0395 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01531: LearningRateScheduler reducing learning rate to 0.00028317852153320234.\n",
      "Epoch 1531/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 01531: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0257 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01532: LearningRateScheduler reducing learning rate to 0.00028315690864203586.\n",
      "Epoch 1532/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 01532: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0270 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01533: LearningRateScheduler reducing learning rate to 0.00028313528270994834.\n",
      "Epoch 1533/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0271\n",
      "Epoch 01533: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0271 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01534: LearningRateScheduler reducing learning rate to 0.00028311364373908194.\n",
      "Epoch 1534/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 01534: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0257 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01535: LearningRateScheduler reducing learning rate to 0.00028309199173158007.\n",
      "Epoch 1535/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01535: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0291 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01536: LearningRateScheduler reducing learning rate to 0.0002830703266895873.\n",
      "Epoch 1536/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01536: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0315 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01537: LearningRateScheduler reducing learning rate to 0.00028304864861524966.\n",
      "Epoch 1537/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 01537: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0237 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01538: LearningRateScheduler reducing learning rate to 0.0002830269575107144.\n",
      "Epoch 1538/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01538: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0245 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01539: LearningRateScheduler reducing learning rate to 0.0002830052533781302.\n",
      "Epoch 1539/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 01539: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0325 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01540: LearningRateScheduler reducing learning rate to 0.0002829835362196467.\n",
      "Epoch 1540/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 01540: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0296 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01541: LearningRateScheduler reducing learning rate to 0.0002829618060374151.\n",
      "Epoch 1541/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0227\n",
      "Epoch 01541: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0227 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01542: LearningRateScheduler reducing learning rate to 0.00028294006283358797.\n",
      "Epoch 1542/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01542: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0245 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01543: LearningRateScheduler reducing learning rate to 0.00028291830661031884.\n",
      "Epoch 1543/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0344\n",
      "Epoch 01543: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0344 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01544: LearningRateScheduler reducing learning rate to 0.0002828965373697629.\n",
      "Epoch 1544/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0346\n",
      "Epoch 01544: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 0.0346 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01545: LearningRateScheduler reducing learning rate to 0.00028287475511407616.\n",
      "Epoch 1545/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01545: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0249 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01546: LearningRateScheduler reducing learning rate to 0.00028285295984541647.\n",
      "Epoch 1546/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0396\n",
      "Epoch 01546: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0396 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01547: LearningRateScheduler reducing learning rate to 0.00028283115156594257.\n",
      "Epoch 1547/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01547: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0206 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01548: LearningRateScheduler reducing learning rate to 0.0002828093302778147.\n",
      "Epoch 1548/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0430\n",
      "Epoch 01548: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0430 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01549: LearningRateScheduler reducing learning rate to 0.00028278749598319424.\n",
      "Epoch 1549/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01549: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0220 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01550: LearningRateScheduler reducing learning rate to 0.00028276564868424394.\n",
      "Epoch 1550/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01550: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0322 - val_loss: 0.0308\n",
      "\n",
      "Epoch 01551: LearningRateScheduler reducing learning rate to 0.0002827437883831279.\n",
      "Epoch 1551/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01551: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0276 - val_loss: 0.0316\n",
      "\n",
      "Epoch 01552: LearningRateScheduler reducing learning rate to 0.0002827219150820113.\n",
      "Epoch 1552/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01552: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0213 - val_loss: 0.0320\n",
      "\n",
      "Epoch 01553: LearningRateScheduler reducing learning rate to 0.00028270002878306083.\n",
      "Epoch 1553/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01553: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0259 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01554: LearningRateScheduler reducing learning rate to 0.00028267812948844435.\n",
      "Epoch 1554/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 01554: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0209 - val_loss: 0.0305\n",
      "\n",
      "Epoch 01555: LearningRateScheduler reducing learning rate to 0.000282656217200331.\n",
      "Epoch 1555/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0174\n",
      "Epoch 01555: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0174 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01556: LearningRateScheduler reducing learning rate to 0.0002826342919208914.\n",
      "Epoch 1556/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01556: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0391 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01557: LearningRateScheduler reducing learning rate to 0.00028261235365229713.\n",
      "Epoch 1557/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 01557: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0267 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01558: LearningRateScheduler reducing learning rate to 0.0002825904023967213.\n",
      "Epoch 1558/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476\n",
      "Epoch 01558: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0476 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01559: LearningRateScheduler reducing learning rate to 0.0002825684381563382.\n",
      "Epoch 1559/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 01559: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0290 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01560: LearningRateScheduler reducing learning rate to 0.00028254646093332347.\n",
      "Epoch 1560/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01560: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0291 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01561: LearningRateScheduler reducing learning rate to 0.000282524470729854.\n",
      "Epoch 1561/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 01561: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0228 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01562: LearningRateScheduler reducing learning rate to 0.0002825024675481079.\n",
      "Epoch 1562/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01562: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0407 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01563: LearningRateScheduler reducing learning rate to 0.0002824804513902648.\n",
      "Epoch 1563/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0426\n",
      "Epoch 01563: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0426 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01564: LearningRateScheduler reducing learning rate to 0.00028245842225850526.\n",
      "Epoch 1564/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0435\n",
      "Epoch 01564: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0435 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01565: LearningRateScheduler reducing learning rate to 0.0002824363801550115.\n",
      "Epoch 1565/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 01565: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0339 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01566: LearningRateScheduler reducing learning rate to 0.00028241432508196665.\n",
      "Epoch 1566/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 01566: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01567: LearningRateScheduler reducing learning rate to 0.00028239225704155543.\n",
      "Epoch 1567/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0221\n",
      "Epoch 01567: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0221 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01568: LearningRateScheduler reducing learning rate to 0.0002823701760359637.\n",
      "Epoch 1568/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0312\n",
      "Epoch 01568: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0312 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01569: LearningRateScheduler reducing learning rate to 0.0002823480820673787.\n",
      "Epoch 1569/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01569: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0300 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01570: LearningRateScheduler reducing learning rate to 0.0002823259751379888.\n",
      "Epoch 1570/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 01570: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0175 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01571: LearningRateScheduler reducing learning rate to 0.00028230385524998376.\n",
      "Epoch 1571/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 01571: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0301 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01572: LearningRateScheduler reducing learning rate to 0.00028228172240555465.\n",
      "Epoch 1572/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01572: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0338 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01573: LearningRateScheduler reducing learning rate to 0.0002822595766068937.\n",
      "Epoch 1573/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01573: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0337 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01574: LearningRateScheduler reducing learning rate to 0.00028223741785619465.\n",
      "Epoch 1574/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 01574: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0267 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01575: LearningRateScheduler reducing learning rate to 0.0002822152461556522.\n",
      "Epoch 1575/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 01575: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 0.0211 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01576: LearningRateScheduler reducing learning rate to 0.00028219306150746267.\n",
      "Epoch 1576/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01576: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0206 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01577: LearningRateScheduler reducing learning rate to 0.0002821708639138234.\n",
      "Epoch 1577/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01577: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0303 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01578: LearningRateScheduler reducing learning rate to 0.0002821486533769331.\n",
      "Epoch 1578/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01578: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0413 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01579: LearningRateScheduler reducing learning rate to 0.00028212642989899184.\n",
      "Epoch 1579/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 01579: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0370 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01580: LearningRateScheduler reducing learning rate to 0.00028210419348220084.\n",
      "Epoch 1580/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0394\n",
      "Epoch 01580: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0394 - val_loss: 0.0290\n",
      "\n",
      "Epoch 01581: LearningRateScheduler reducing learning rate to 0.00028208194412876274.\n",
      "Epoch 1581/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0261\n",
      "Epoch 01581: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0261 - val_loss: 0.0328\n",
      "\n",
      "Epoch 01582: LearningRateScheduler reducing learning rate to 0.0002820596818408813.\n",
      "Epoch 1582/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01582: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0245 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01583: LearningRateScheduler reducing learning rate to 0.00028203740662076176.\n",
      "Epoch 1583/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01583: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0249 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01584: LearningRateScheduler reducing learning rate to 0.0002820151184706104.\n",
      "Epoch 1584/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 01584: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0301 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01585: LearningRateScheduler reducing learning rate to 0.0002819928173926351.\n",
      "Epoch 1585/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01585: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0317 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01586: LearningRateScheduler reducing learning rate to 0.0002819705033890447.\n",
      "Epoch 1586/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01586: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0278 - val_loss: 0.0262\n",
      "\n",
      "Epoch 01587: LearningRateScheduler reducing learning rate to 0.0002819481764620494.\n",
      "Epoch 1587/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01587: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0295 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01588: LearningRateScheduler reducing learning rate to 0.00028192583661386085.\n",
      "Epoch 1588/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01588: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0369 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01589: LearningRateScheduler reducing learning rate to 0.00028190348384669184.\n",
      "Epoch 1589/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01589: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0280 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01590: LearningRateScheduler reducing learning rate to 0.00028188111816275643.\n",
      "Epoch 1590/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01590: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0303 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01591: LearningRateScheduler reducing learning rate to 0.00028185873956427.\n",
      "Epoch 1591/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0388\n",
      "Epoch 01591: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0388 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01592: LearningRateScheduler reducing learning rate to 0.0002818363480534492.\n",
      "Epoch 1592/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0417\n",
      "Epoch 01592: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0417 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01593: LearningRateScheduler reducing learning rate to 0.00028181394363251203.\n",
      "Epoch 1593/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 01593: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0182 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01594: LearningRateScheduler reducing learning rate to 0.00028179152630367765.\n",
      "Epoch 1594/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 01594: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0272 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01595: LearningRateScheduler reducing learning rate to 0.0002817690960691665.\n",
      "Epoch 1595/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 01595: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01596: LearningRateScheduler reducing learning rate to 0.00028174665293120035.\n",
      "Epoch 1596/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 01596: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0222 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01597: LearningRateScheduler reducing learning rate to 0.00028172419689200234.\n",
      "Epoch 1597/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 01597: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0229 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01598: LearningRateScheduler reducing learning rate to 0.0002817017279537967.\n",
      "Epoch 1598/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 01598: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0320 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01599: LearningRateScheduler reducing learning rate to 0.00028167924611880905.\n",
      "Epoch 1599/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01599: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0334 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01600: LearningRateScheduler reducing learning rate to 0.0002816567513892663.\n",
      "Epoch 1600/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 01600: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0250 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01601: LearningRateScheduler reducing learning rate to 0.00028163424376739663.\n",
      "Epoch 1601/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0409\n",
      "Epoch 01601: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0409 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01602: LearningRateScheduler reducing learning rate to 0.00028161172325542937.\n",
      "Epoch 1602/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0423\n",
      "Epoch 01602: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0423 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01603: LearningRateScheduler reducing learning rate to 0.0002815891898555953.\n",
      "Epoch 1603/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0350\n",
      "Epoch 01603: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0350 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01604: LearningRateScheduler reducing learning rate to 0.0002815666435701264.\n",
      "Epoch 1604/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0221\n",
      "Epoch 01604: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0221 - val_loss: 0.0210\n",
      "\n",
      "Epoch 01605: LearningRateScheduler reducing learning rate to 0.0002815440844012559.\n",
      "Epoch 1605/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 01605: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0241 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01606: LearningRateScheduler reducing learning rate to 0.00028152151235121843.\n",
      "Epoch 1606/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 01606: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0233 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01607: LearningRateScheduler reducing learning rate to 0.00028149892742224965.\n",
      "Epoch 1607/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 01607: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0252 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01608: LearningRateScheduler reducing learning rate to 0.0002814763296165868.\n",
      "Epoch 1608/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01608: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0326 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01609: LearningRateScheduler reducing learning rate to 0.00028145371893646817.\n",
      "Epoch 1609/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0261\n",
      "Epoch 01609: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0261 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01610: LearningRateScheduler reducing learning rate to 0.0002814310953841334.\n",
      "Epoch 1610/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01610: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0277 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01611: LearningRateScheduler reducing learning rate to 0.0002814084589618234.\n",
      "Epoch 1611/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0410\n",
      "Epoch 01611: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0410 - val_loss: 0.0199\n",
      "\n",
      "Epoch 01612: LearningRateScheduler reducing learning rate to 0.0002813858096717804.\n",
      "Epoch 1612/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01612: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0276 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01613: LearningRateScheduler reducing learning rate to 0.00028136314751624783.\n",
      "Epoch 1613/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01613: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0240 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01614: LearningRateScheduler reducing learning rate to 0.00028134047249747044.\n",
      "Epoch 1614/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0342\n",
      "Epoch 01614: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0342 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01615: LearningRateScheduler reducing learning rate to 0.0002813177846176942.\n",
      "Epoch 1615/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01615: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0288 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01616: LearningRateScheduler reducing learning rate to 0.0002812950838791665.\n",
      "Epoch 1616/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01616: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0326 - val_loss: 0.0200\n",
      "\n",
      "Epoch 01617: LearningRateScheduler reducing learning rate to 0.0002812723702841358.\n",
      "Epoch 1617/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 01617: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0215 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01618: LearningRateScheduler reducing learning rate to 0.00028124964383485204.\n",
      "Epoch 1618/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01618: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0289 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01619: LearningRateScheduler reducing learning rate to 0.0002812269045335662.\n",
      "Epoch 1619/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01619: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0212 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01620: LearningRateScheduler reducing learning rate to 0.0002812041523825307.\n",
      "Epoch 1620/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01620: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0262 - val_loss: 0.0176\n",
      "\n",
      "Epoch 01621: LearningRateScheduler reducing learning rate to 0.00028118138738399926.\n",
      "Epoch 1621/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 01621: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0208 - val_loss: 0.0188\n",
      "\n",
      "Epoch 01622: LearningRateScheduler reducing learning rate to 0.00028115860954022677.\n",
      "Epoch 1622/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01622: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0256 - val_loss: 0.0181\n",
      "\n",
      "Epoch 01623: LearningRateScheduler reducing learning rate to 0.00028113581885346936.\n",
      "Epoch 1623/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01623: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0380 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01624: LearningRateScheduler reducing learning rate to 0.0002811130153259846.\n",
      "Epoch 1624/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01624: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0303 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01625: LearningRateScheduler reducing learning rate to 0.0002810901989600312.\n",
      "Epoch 1625/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0371\n",
      "Epoch 01625: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0371 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01626: LearningRateScheduler reducing learning rate to 0.0002810673697578691.\n",
      "Epoch 1626/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 01626: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0257 - val_loss: 0.0314\n",
      "\n",
      "Epoch 01627: LearningRateScheduler reducing learning rate to 0.0002810445277217597.\n",
      "Epoch 1627/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01627: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0380 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01628: LearningRateScheduler reducing learning rate to 0.00028102167285396553.\n",
      "Epoch 1628/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0309\n",
      "Epoch 01628: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0309 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01629: LearningRateScheduler reducing learning rate to 0.0002809988051567504.\n",
      "Epoch 1629/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01629: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0369 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01630: LearningRateScheduler reducing learning rate to 0.00028097592463237933.\n",
      "Epoch 1630/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495\n",
      "Epoch 01630: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0495 - val_loss: 0.0205\n",
      "\n",
      "Epoch 01631: LearningRateScheduler reducing learning rate to 0.00028095303128311875.\n",
      "Epoch 1631/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 01631: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0211 - val_loss: 0.0188\n",
      "\n",
      "Epoch 01632: LearningRateScheduler reducing learning rate to 0.0002809301251112364.\n",
      "Epoch 1632/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01632: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0280 - val_loss: 0.0199\n",
      "\n",
      "Epoch 01633: LearningRateScheduler reducing learning rate to 0.00028090720611900104.\n",
      "Epoch 1633/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 01633: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0228 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01634: LearningRateScheduler reducing learning rate to 0.0002808842743086829.\n",
      "Epoch 1634/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 01634: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0239 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01635: LearningRateScheduler reducing learning rate to 0.0002808613296825534.\n",
      "Epoch 1635/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0147\n",
      "Epoch 01635: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0147 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01636: LearningRateScheduler reducing learning rate to 0.0002808383722428854.\n",
      "Epoch 1636/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 01636: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0186 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01637: LearningRateScheduler reducing learning rate to 0.00028081540199195276.\n",
      "Epoch 1637/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 01637: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0383 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01638: LearningRateScheduler reducing learning rate to 0.00028079241893203075.\n",
      "Epoch 1638/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 01638: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0328 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01639: LearningRateScheduler reducing learning rate to 0.00028076942306539586.\n",
      "Epoch 1639/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01639: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0216 - val_loss: 0.0174\n",
      "\n",
      "Epoch 01640: LearningRateScheduler reducing learning rate to 0.00028074641439432594.\n",
      "Epoch 1640/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 01640: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0331 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01641: LearningRateScheduler reducing learning rate to 0.0002807233929211001.\n",
      "Epoch 1641/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0282\n",
      "Epoch 01641: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0282 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01642: LearningRateScheduler reducing learning rate to 0.0002807003586479985.\n",
      "Epoch 1642/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 01642: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0345 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01643: LearningRateScheduler reducing learning rate to 0.0002806773115773029.\n",
      "Epoch 1643/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01643: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0206 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01644: LearningRateScheduler reducing learning rate to 0.00028065425171129615.\n",
      "Epoch 1644/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 01644: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0211 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01645: LearningRateScheduler reducing learning rate to 0.0002806311790522623.\n",
      "Epoch 1645/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01645: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0338 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01646: LearningRateScheduler reducing learning rate to 0.0002806080936024868.\n",
      "Epoch 1646/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 01646: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01647: LearningRateScheduler reducing learning rate to 0.00028058499536425636.\n",
      "Epoch 1647/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 01647: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0253 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01648: LearningRateScheduler reducing learning rate to 0.00028056188433985886.\n",
      "Epoch 1648/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0406\n",
      "Epoch 01648: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0406 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01649: LearningRateScheduler reducing learning rate to 0.00028053876053158345.\n",
      "Epoch 1649/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471\n",
      "Epoch 01649: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0471 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01650: LearningRateScheduler reducing learning rate to 0.00028051562394172067.\n",
      "Epoch 1650/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 01650: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0239 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01651: LearningRateScheduler reducing learning rate to 0.00028049247457256226.\n",
      "Epoch 1651/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01651: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0247 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01652: LearningRateScheduler reducing learning rate to 0.0002804693124264012.\n",
      "Epoch 1652/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0197\n",
      "Epoch 01652: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0197 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01653: LearningRateScheduler reducing learning rate to 0.00028044613750553176.\n",
      "Epoch 1653/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01653: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0254 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01654: LearningRateScheduler reducing learning rate to 0.0002804229498122494.\n",
      "Epoch 1654/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01654: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0269 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01655: LearningRateScheduler reducing learning rate to 0.000280399749348851.\n",
      "Epoch 1655/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 01655: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0301 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01656: LearningRateScheduler reducing learning rate to 0.00028037653611763465.\n",
      "Epoch 1656/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0364\n",
      "Epoch 01656: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0364 - val_loss: 0.0200\n",
      "\n",
      "Epoch 01657: LearningRateScheduler reducing learning rate to 0.0002803533101208996.\n",
      "Epoch 1657/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01657: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0317 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01658: LearningRateScheduler reducing learning rate to 0.00028033007136094636.\n",
      "Epoch 1658/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01658: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0210 - val_loss: 0.0187\n",
      "\n",
      "Epoch 01659: LearningRateScheduler reducing learning rate to 0.000280306819840077.\n",
      "Epoch 1659/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 01659: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0279 - val_loss: 0.0188\n",
      "\n",
      "Epoch 01660: LearningRateScheduler reducing learning rate to 0.0002802835555605945.\n",
      "Epoch 1660/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01660: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0210 - val_loss: 0.0192\n",
      "\n",
      "Epoch 01661: LearningRateScheduler reducing learning rate to 0.00028026027852480316.\n",
      "Epoch 1661/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 01661: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0207 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01662: LearningRateScheduler reducing learning rate to 0.0002802369887350088.\n",
      "Epoch 1662/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01662: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0354 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01663: LearningRateScheduler reducing learning rate to 0.0002802136861935182.\n",
      "Epoch 1663/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 01663: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0370 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01664: LearningRateScheduler reducing learning rate to 0.00028019037090263964.\n",
      "Epoch 1664/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01664: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0288 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01665: LearningRateScheduler reducing learning rate to 0.00028016704286468246.\n",
      "Epoch 1665/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 01665: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0168 - val_loss: 0.0192\n",
      "\n",
      "Epoch 01666: LearningRateScheduler reducing learning rate to 0.00028014370208195735.\n",
      "Epoch 1666/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 01666: val_loss did not improve from 0.01614\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0301 - val_loss: 0.0162\n",
      "\n",
      "Epoch 01667: LearningRateScheduler reducing learning rate to 0.00028012034855677635.\n",
      "Epoch 1667/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01667: val_loss improved from 0.01614 to 0.01411, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0413 - val_loss: 0.0141\n",
      "\n",
      "Epoch 01668: LearningRateScheduler reducing learning rate to 0.0002800969822914527.\n",
      "Epoch 1668/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 01668: val_loss improved from 0.01411 to 0.01317, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0229 - val_loss: 0.0132\n",
      "\n",
      "Epoch 01669: LearningRateScheduler reducing learning rate to 0.0002800736032883007.\n",
      "Epoch 1669/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0275\n",
      "Epoch 01669: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0275 - val_loss: 0.0155\n",
      "\n",
      "Epoch 01670: LearningRateScheduler reducing learning rate to 0.00028005021154963627.\n",
      "Epoch 1670/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01670: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0212 - val_loss: 0.0191\n",
      "\n",
      "Epoch 01671: LearningRateScheduler reducing learning rate to 0.00028002680707777636.\n",
      "Epoch 1671/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0183\n",
      "Epoch 01671: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0183 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01672: LearningRateScheduler reducing learning rate to 0.00028000338987503923.\n",
      "Epoch 1672/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 01672: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0328 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01673: LearningRateScheduler reducing learning rate to 0.00027997995994374436.\n",
      "Epoch 1673/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01673: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0295 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01674: LearningRateScheduler reducing learning rate to 0.0002799565172862127.\n",
      "Epoch 1674/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01674: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0254 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01675: LearningRateScheduler reducing learning rate to 0.00027993306190476615.\n",
      "Epoch 1675/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01675: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0308 - val_loss: 0.0306\n",
      "\n",
      "Epoch 01676: LearningRateScheduler reducing learning rate to 0.00027990959380172806.\n",
      "Epoch 1676/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01676: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0348 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01677: LearningRateScheduler reducing learning rate to 0.000279886112979423.\n",
      "Epoch 1677/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 01677: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0233 - val_loss: 0.0311\n",
      "\n",
      "Epoch 01678: LearningRateScheduler reducing learning rate to 0.0002798626194401768.\n",
      "Epoch 1678/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 01678: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0383 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01679: LearningRateScheduler reducing learning rate to 0.00027983911318631654.\n",
      "Epoch 1679/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447\n",
      "Epoch 01679: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0447 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01680: LearningRateScheduler reducing learning rate to 0.0002798155942201706.\n",
      "Epoch 1680/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0365\n",
      "Epoch 01680: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0365 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01681: LearningRateScheduler reducing learning rate to 0.0002797920625440685.\n",
      "Epoch 1681/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 01681: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0272 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01682: LearningRateScheduler reducing learning rate to 0.0002797685181603413.\n",
      "Epoch 1682/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01682: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0317 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01683: LearningRateScheduler reducing learning rate to 0.00027974496107132095.\n",
      "Epoch 1683/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01683: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0278 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01684: LearningRateScheduler reducing learning rate to 0.0002797213912793408.\n",
      "Epoch 1684/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01684: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0295 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01685: LearningRateScheduler reducing learning rate to 0.00027969780878673575.\n",
      "Epoch 1685/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 01685: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0270 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01686: LearningRateScheduler reducing learning rate to 0.00027967421359584136.\n",
      "Epoch 1686/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 01686: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0208 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01687: LearningRateScheduler reducing learning rate to 0.00027965060570899505.\n",
      "Epoch 1687/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01687: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0247 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01688: LearningRateScheduler reducing learning rate to 0.0002796269851285351.\n",
      "Epoch 1688/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0431\n",
      "Epoch 01688: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0431 - val_loss: 0.0191\n",
      "\n",
      "Epoch 01689: LearningRateScheduler reducing learning rate to 0.00027960335185680125.\n",
      "Epoch 1689/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01689: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0195 - val_loss: 0.0197\n",
      "\n",
      "Epoch 01690: LearningRateScheduler reducing learning rate to 0.00027957970589613437.\n",
      "Epoch 1690/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0231\n",
      "Epoch 01690: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0231 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01691: LearningRateScheduler reducing learning rate to 0.00027955604724887677.\n",
      "Epoch 1691/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01691: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0315 - val_loss: 0.0205\n",
      "\n",
      "Epoch 01692: LearningRateScheduler reducing learning rate to 0.0002795323759173717.\n",
      "Epoch 1692/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0356\n",
      "Epoch 01692: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0356 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01693: LearningRateScheduler reducing learning rate to 0.000279508691903964.\n",
      "Epoch 1693/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 01693: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0184 - val_loss: 0.0210\n",
      "\n",
      "Epoch 01694: LearningRateScheduler reducing learning rate to 0.00027948499521099964.\n",
      "Epoch 1694/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0377\n",
      "Epoch 01694: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0377 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01695: LearningRateScheduler reducing learning rate to 0.0002794612858408257.\n",
      "Epoch 1695/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01695: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01696: LearningRateScheduler reducing learning rate to 0.00027943756379579076.\n",
      "Epoch 1696/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0309\n",
      "Epoch 01696: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0309 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01697: LearningRateScheduler reducing learning rate to 0.00027941382907824454.\n",
      "Epoch 1697/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01697: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0212 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01698: LearningRateScheduler reducing learning rate to 0.00027939008169053803.\n",
      "Epoch 1698/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0285\n",
      "Epoch 01698: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0285 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01699: LearningRateScheduler reducing learning rate to 0.0002793663216350234.\n",
      "Epoch 1699/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01699: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0220 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01700: LearningRateScheduler reducing learning rate to 0.0002793425489140541.\n",
      "Epoch 1700/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 01700: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0191 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01701: LearningRateScheduler reducing learning rate to 0.000279318763529985.\n",
      "Epoch 1701/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 01701: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0238 - val_loss: 0.0219\n",
      "\n",
      "Epoch 01702: LearningRateScheduler reducing learning rate to 0.00027929496548517203.\n",
      "Epoch 1702/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 01702: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0193 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01703: LearningRateScheduler reducing learning rate to 0.00027927115478197244.\n",
      "Epoch 1703/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01703: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0244 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01704: LearningRateScheduler reducing learning rate to 0.00027924733142274476.\n",
      "Epoch 1704/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01704: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0337 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01705: LearningRateScheduler reducing learning rate to 0.0002792234954098487.\n",
      "Epoch 1705/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0323\n",
      "Epoch 01705: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0323 - val_loss: 0.0249\n",
      "\n",
      "Epoch 01706: LearningRateScheduler reducing learning rate to 0.0002791996467456453.\n",
      "Epoch 1706/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01706: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0292 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01707: LearningRateScheduler reducing learning rate to 0.00027917578543249687.\n",
      "Epoch 1707/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0161\n",
      "Epoch 01707: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0161 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01708: LearningRateScheduler reducing learning rate to 0.0002791519114727669.\n",
      "Epoch 1708/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01708: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0216 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01709: LearningRateScheduler reducing learning rate to 0.0002791280248688201.\n",
      "Epoch 1709/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 01709: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0199 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01710: LearningRateScheduler reducing learning rate to 0.0002791041256230226.\n",
      "Epoch 1710/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 01710: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0207 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01711: LearningRateScheduler reducing learning rate to 0.0002790802137377415.\n",
      "Epoch 1711/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01711: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0278 - val_loss: 0.0206\n",
      "\n",
      "Epoch 01712: LearningRateScheduler reducing learning rate to 0.00027905628921534554.\n",
      "Epoch 1712/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0150\n",
      "Epoch 01712: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0150 - val_loss: 0.0201\n",
      "\n",
      "Epoch 01713: LearningRateScheduler reducing learning rate to 0.0002790323520582044.\n",
      "Epoch 1713/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 01713: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0230 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01714: LearningRateScheduler reducing learning rate to 0.0002790084022686891.\n",
      "Epoch 1714/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 01714: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0345 - val_loss: 0.0192\n",
      "\n",
      "Epoch 01715: LearningRateScheduler reducing learning rate to 0.00027898443984917187.\n",
      "Epoch 1715/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01715: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0258 - val_loss: 0.0200\n",
      "\n",
      "Epoch 01716: LearningRateScheduler reducing learning rate to 0.0002789604648020264.\n",
      "Epoch 1716/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01716: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0337 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01717: LearningRateScheduler reducing learning rate to 0.00027893647712962734.\n",
      "Epoch 1717/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01717: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0265 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01718: LearningRateScheduler reducing learning rate to 0.0002789124768343508.\n",
      "Epoch 1718/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01718: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0354 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01719: LearningRateScheduler reducing learning rate to 0.000278888463918574.\n",
      "Epoch 1719/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01719: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0288 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01720: LearningRateScheduler reducing learning rate to 0.00027886443838467554.\n",
      "Epoch 1720/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0321\n",
      "Epoch 01720: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0321 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01721: LearningRateScheduler reducing learning rate to 0.0002788404002350352.\n",
      "Epoch 1721/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 01721: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0226 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01722: LearningRateScheduler reducing learning rate to 0.0002788163494720339.\n",
      "Epoch 1722/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0221\n",
      "Epoch 01722: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0221 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01723: LearningRateScheduler reducing learning rate to 0.00027879228609805403.\n",
      "Epoch 1723/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 01723: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0238 - val_loss: 0.0198\n",
      "\n",
      "Epoch 01724: LearningRateScheduler reducing learning rate to 0.00027876821011547915.\n",
      "Epoch 1724/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01724: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0264 - val_loss: 0.0206\n",
      "\n",
      "Epoch 01725: LearningRateScheduler reducing learning rate to 0.00027874412152669397.\n",
      "Epoch 1725/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01725: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0288 - val_loss: 0.0205\n",
      "\n",
      "Epoch 01726: LearningRateScheduler reducing learning rate to 0.0002787200203340845.\n",
      "Epoch 1726/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 01726: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0302 - val_loss: 0.0205\n",
      "\n",
      "Epoch 01727: LearningRateScheduler reducing learning rate to 0.00027869590654003816.\n",
      "Epoch 1727/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01727: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0352 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01728: LearningRateScheduler reducing learning rate to 0.0002786717801469433.\n",
      "Epoch 1728/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01728: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0315 - val_loss: 0.0173\n",
      "\n",
      "Epoch 01729: LearningRateScheduler reducing learning rate to 0.0002786476411571898.\n",
      "Epoch 1729/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 01729: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0318 - val_loss: 0.0181\n",
      "\n",
      "Epoch 01730: LearningRateScheduler reducing learning rate to 0.00027862348957316866.\n",
      "Epoch 1730/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0234\n",
      "Epoch 01730: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0234 - val_loss: 0.0199\n",
      "\n",
      "Epoch 01731: LearningRateScheduler reducing learning rate to 0.0002785993253972721.\n",
      "Epoch 1731/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01731: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0262 - val_loss: 0.0198\n",
      "\n",
      "Epoch 01732: LearningRateScheduler reducing learning rate to 0.0002785751486318937.\n",
      "Epoch 1732/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01732: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0254 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01733: LearningRateScheduler reducing learning rate to 0.00027855095927942827.\n",
      "Epoch 1733/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0422\n",
      "Epoch 01733: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0422 - val_loss: 0.0192\n",
      "\n",
      "Epoch 01734: LearningRateScheduler reducing learning rate to 0.00027852675734227176.\n",
      "Epoch 1734/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 01734: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0320 - val_loss: 0.0177\n",
      "\n",
      "Epoch 01735: LearningRateScheduler reducing learning rate to 0.0002785025428228214.\n",
      "Epoch 1735/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0305\n",
      "Epoch 01735: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0305 - val_loss: 0.0201\n",
      "\n",
      "Epoch 01736: LearningRateScheduler reducing learning rate to 0.0002784783157234757.\n",
      "Epoch 1736/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0400\n",
      "Epoch 01736: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0400 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01737: LearningRateScheduler reducing learning rate to 0.00027845407604663446.\n",
      "Epoch 1737/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0385\n",
      "Epoch 01737: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0385 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01738: LearningRateScheduler reducing learning rate to 0.00027842982379469865.\n",
      "Epoch 1738/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01738: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0269 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01739: LearningRateScheduler reducing learning rate to 0.0002784055589700705.\n",
      "Epoch 1739/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 01739: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0351 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01740: LearningRateScheduler reducing learning rate to 0.0002783812815751535.\n",
      "Epoch 1740/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01740: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0254 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01741: LearningRateScheduler reducing learning rate to 0.00027835699161235245.\n",
      "Epoch 1741/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01741: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0299 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01742: LearningRateScheduler reducing learning rate to 0.0002783326890840732.\n",
      "Epoch 1742/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0364\n",
      "Epoch 01742: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0364 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01743: LearningRateScheduler reducing learning rate to 0.000278308373992723.\n",
      "Epoch 1743/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 01743: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0279 - val_loss: 0.0262\n",
      "\n",
      "Epoch 01744: LearningRateScheduler reducing learning rate to 0.00027828404634071046.\n",
      "Epoch 1744/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01744: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0335 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01745: LearningRateScheduler reducing learning rate to 0.0002782597061304452.\n",
      "Epoch 1745/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0311\n",
      "Epoch 01745: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0311 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01746: LearningRateScheduler reducing learning rate to 0.00027823535336433806.\n",
      "Epoch 1746/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 01746: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0241 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01747: LearningRateScheduler reducing learning rate to 0.0002782109880448014.\n",
      "Epoch 1747/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 01747: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0225 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01748: LearningRateScheduler reducing learning rate to 0.0002781866101742485.\n",
      "Epoch 1748/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01748: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0236 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01749: LearningRateScheduler reducing learning rate to 0.00027816221975509426.\n",
      "Epoch 1749/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 01749: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0222 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01750: LearningRateScheduler reducing learning rate to 0.00027813781678975445.\n",
      "Epoch 1750/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 01750: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0229 - val_loss: 0.0283\n",
      "\n",
      "Epoch 01751: LearningRateScheduler reducing learning rate to 0.00027811340128064627.\n",
      "Epoch 1751/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 01751: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0272 - val_loss: 0.0290\n",
      "\n",
      "Epoch 01752: LearningRateScheduler reducing learning rate to 0.00027808897323018813.\n",
      "Epoch 1752/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 01752: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0375 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01753: LearningRateScheduler reducing learning rate to 0.00027806453264079975.\n",
      "Epoch 1753/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01753: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0283 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01754: LearningRateScheduler reducing learning rate to 0.00027804007951490195.\n",
      "Epoch 1754/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0465\n",
      "Epoch 01754: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0465 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01755: LearningRateScheduler reducing learning rate to 0.0002780156138549168.\n",
      "Epoch 1755/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0319\n",
      "Epoch 01755: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0319 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01756: LearningRateScheduler reducing learning rate to 0.0002779911356632679.\n",
      "Epoch 1756/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 01756: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0272 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01757: LearningRateScheduler reducing learning rate to 0.0002779666449423797.\n",
      "Epoch 1757/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01757: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0289 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01758: LearningRateScheduler reducing learning rate to 0.0002779421416946781.\n",
      "Epoch 1758/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01758: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0315 - val_loss: 0.0311\n",
      "\n",
      "Epoch 01759: LearningRateScheduler reducing learning rate to 0.0002779176259225902.\n",
      "Epoch 1759/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0203\n",
      "Epoch 01759: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0203 - val_loss: 0.0309\n",
      "\n",
      "Epoch 01760: LearningRateScheduler reducing learning rate to 0.0002778930976285444.\n",
      "Epoch 1760/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01760: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0259 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01761: LearningRateScheduler reducing learning rate to 0.0002778685568149702.\n",
      "Epoch 1761/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0248\n",
      "Epoch 01761: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0248 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01762: LearningRateScheduler reducing learning rate to 0.0002778440034842985.\n",
      "Epoch 1762/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 01762: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0272 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01763: LearningRateScheduler reducing learning rate to 0.00027781943763896126.\n",
      "Epoch 1763/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01763: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0287 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01764: LearningRateScheduler reducing learning rate to 0.0002777948592813919.\n",
      "Epoch 1764/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01764: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0263 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01765: LearningRateScheduler reducing learning rate to 0.0002777702684140249.\n",
      "Epoch 1765/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01765: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0263 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01766: LearningRateScheduler reducing learning rate to 0.000277745665039296.\n",
      "Epoch 1766/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 01766: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 216ms/step - loss: 0.0251 - val_loss: 0.0292\n",
      "\n",
      "Epoch 01767: LearningRateScheduler reducing learning rate to 0.0002777210491596424.\n",
      "Epoch 1767/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01767: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0244 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01768: LearningRateScheduler reducing learning rate to 0.0002776964207775021.\n",
      "Epoch 1768/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01768: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0220 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01769: LearningRateScheduler reducing learning rate to 0.0002776717798953148.\n",
      "Epoch 1769/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01769: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0247 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01770: LearningRateScheduler reducing learning rate to 0.00027764712651552116.\n",
      "Epoch 1770/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 01770: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0225 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01771: LearningRateScheduler reducing learning rate to 0.0002776224606405632.\n",
      "Epoch 1771/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0273\n",
      "Epoch 01771: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0273 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01772: LearningRateScheduler reducing learning rate to 0.0002775977822728841.\n",
      "Epoch 1772/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01772: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0244 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01773: LearningRateScheduler reducing learning rate to 0.00027757309141492823.\n",
      "Epoch 1773/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01773: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0249 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01774: LearningRateScheduler reducing learning rate to 0.0002775483880691414.\n",
      "Epoch 1774/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01774: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0300 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01775: LearningRateScheduler reducing learning rate to 0.0002775236722379705.\n",
      "Epoch 1775/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 01775: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0200 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01776: LearningRateScheduler reducing learning rate to 0.00027749894392386365.\n",
      "Epoch 1776/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0365\n",
      "Epoch 01776: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0365 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01777: LearningRateScheduler reducing learning rate to 0.0002774742031292703.\n",
      "Epoch 1777/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01777: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0259 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01778: LearningRateScheduler reducing learning rate to 0.000277449449856641.\n",
      "Epoch 1778/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 01778: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0328 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01779: LearningRateScheduler reducing learning rate to 0.00027742468410842775.\n",
      "Epoch 1779/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01779: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0245 - val_loss: 0.0198\n",
      "\n",
      "Epoch 01780: LearningRateScheduler reducing learning rate to 0.00027739990588708356.\n",
      "Epoch 1780/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 01780: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0181 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01781: LearningRateScheduler reducing learning rate to 0.00027737511519506275.\n",
      "Epoch 1781/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0390\n",
      "Epoch 01781: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0390 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01782: LearningRateScheduler reducing learning rate to 0.000277350312034821.\n",
      "Epoch 1782/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0196\n",
      "Epoch 01782: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0196 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01783: LearningRateScheduler reducing learning rate to 0.000277325496408815.\n",
      "Epoch 1783/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0356\n",
      "Epoch 01783: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0356 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01784: LearningRateScheduler reducing learning rate to 0.00027730066831950284.\n",
      "Epoch 1784/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01784: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0256 - val_loss: 0.0219\n",
      "\n",
      "Epoch 01785: LearningRateScheduler reducing learning rate to 0.00027727582776934385.\n",
      "Epoch 1785/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01785: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0236 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01786: LearningRateScheduler reducing learning rate to 0.0002772509747607985.\n",
      "Epoch 1786/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0194\n",
      "Epoch 01786: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0194 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01787: LearningRateScheduler reducing learning rate to 0.00027722610929632854.\n",
      "Epoch 1787/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 01787: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0230 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01788: LearningRateScheduler reducing learning rate to 0.00027720123137839694.\n",
      "Epoch 1788/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 01788: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0201 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01789: LearningRateScheduler reducing learning rate to 0.0002771763410094679.\n",
      "Epoch 1789/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 01789: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0267 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01790: LearningRateScheduler reducing learning rate to 0.00027715143819200693.\n",
      "Epoch 1790/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01790: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0274 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01791: LearningRateScheduler reducing learning rate to 0.00027712652292848067.\n",
      "Epoch 1791/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 01791: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0193 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01792: LearningRateScheduler reducing learning rate to 0.00027710159522135704.\n",
      "Epoch 1792/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 01792: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0217 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01793: LearningRateScheduler reducing learning rate to 0.0002770766550731052.\n",
      "Epoch 1793/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0293\n",
      "Epoch 01793: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0293 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01794: LearningRateScheduler reducing learning rate to 0.00027705170248619557.\n",
      "Epoch 1794/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01794: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0249 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01795: LearningRateScheduler reducing learning rate to 0.0002770267374630997.\n",
      "Epoch 1795/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0248\n",
      "Epoch 01795: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0248 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01796: LearningRateScheduler reducing learning rate to 0.00027700176000629035.\n",
      "Epoch 1796/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 01796: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0228 - val_loss: 0.0198\n",
      "\n",
      "Epoch 01797: LearningRateScheduler reducing learning rate to 0.00027697677011824176.\n",
      "Epoch 1797/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 01797: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0209 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01798: LearningRateScheduler reducing learning rate to 0.00027695176780142915.\n",
      "Epoch 1798/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01798: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0240 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01799: LearningRateScheduler reducing learning rate to 0.0002769267530583291.\n",
      "Epoch 1799/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0304\n",
      "Epoch 01799: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0304 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01800: LearningRateScheduler reducing learning rate to 0.0002769017258914193.\n",
      "Epoch 1800/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0219\n",
      "Epoch 01800: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0219 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01801: LearningRateScheduler reducing learning rate to 0.0002768766863031788.\n",
      "Epoch 1801/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 01801: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0168 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01802: LearningRateScheduler reducing learning rate to 0.00027685163429608787.\n",
      "Epoch 1802/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 01802: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0190 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01803: LearningRateScheduler reducing learning rate to 0.0002768265698726279.\n",
      "Epoch 1803/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01803: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0277 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01804: LearningRateScheduler reducing learning rate to 0.0002768014930352816.\n",
      "Epoch 1804/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01804: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0269 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01805: LearningRateScheduler reducing learning rate to 0.0002767764037865329.\n",
      "Epoch 1805/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01805: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0288 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01806: LearningRateScheduler reducing learning rate to 0.000276751302128867.\n",
      "Epoch 1806/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01806: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0322 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01807: LearningRateScheduler reducing learning rate to 0.00027672618806477017.\n",
      "Epoch 1807/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0203\n",
      "Epoch 01807: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0203 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01808: LearningRateScheduler reducing learning rate to 0.00027670106159673.\n",
      "Epoch 1808/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01808: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0295 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01809: LearningRateScheduler reducing learning rate to 0.00027667592272723554.\n",
      "Epoch 1809/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 01809: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0306 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01810: LearningRateScheduler reducing learning rate to 0.0002766507714587766.\n",
      "Epoch 1810/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01810: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0289 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01811: LearningRateScheduler reducing learning rate to 0.0002766256077938446.\n",
      "Epoch 1811/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 01811: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0182 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01812: LearningRateScheduler reducing learning rate to 0.00027660043173493204.\n",
      "Epoch 1812/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0273\n",
      "Epoch 01812: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0273 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01813: LearningRateScheduler reducing learning rate to 0.0002765752432845327.\n",
      "Epoch 1813/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 01813: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0241 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01814: LearningRateScheduler reducing learning rate to 0.00027655004244514145.\n",
      "Epoch 1814/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0312\n",
      "Epoch 01814: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0312 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01815: LearningRateScheduler reducing learning rate to 0.00027652482921925456.\n",
      "Epoch 1815/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01815: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0317 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01816: LearningRateScheduler reducing learning rate to 0.00027649960360936945.\n",
      "Epoch 1816/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0417\n",
      "Epoch 01816: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0417 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01817: LearningRateScheduler reducing learning rate to 0.00027647436561798486.\n",
      "Epoch 1817/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0357\n",
      "Epoch 01817: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0357 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01818: LearningRateScheduler reducing learning rate to 0.00027644911524760053.\n",
      "Epoch 1818/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01818: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0291 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01819: LearningRateScheduler reducing learning rate to 0.0002764238525007176.\n",
      "Epoch 1819/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01819: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0308 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01820: LearningRateScheduler reducing learning rate to 0.0002763985773798385.\n",
      "Epoch 1820/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01820: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0269 - val_loss: 0.0199\n",
      "\n",
      "Epoch 01821: LearningRateScheduler reducing learning rate to 0.0002763732898874667.\n",
      "Epoch 1821/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01821: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0315 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01822: LearningRateScheduler reducing learning rate to 0.00027634799002610697.\n",
      "Epoch 1822/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0218\n",
      "Epoch 01822: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0218 - val_loss: 0.0191\n",
      "\n",
      "Epoch 01823: LearningRateScheduler reducing learning rate to 0.0002763226777982654.\n",
      "Epoch 1823/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01823: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0292 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01824: LearningRateScheduler reducing learning rate to 0.0002762973532064491.\n",
      "Epoch 1824/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01824: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0247 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01825: LearningRateScheduler reducing learning rate to 0.0002762720162531667.\n",
      "Epoch 1825/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01825: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0310 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01826: LearningRateScheduler reducing learning rate to 0.0002762466669409278.\n",
      "Epoch 1826/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 01826: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0170 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01827: LearningRateScheduler reducing learning rate to 0.0002762213052722433.\n",
      "Epoch 1827/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01827: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0295 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01828: LearningRateScheduler reducing learning rate to 0.0002761959312496253.\n",
      "Epoch 1828/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 01828: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0302 - val_loss: 0.0254\n",
      "\n",
      "Epoch 01829: LearningRateScheduler reducing learning rate to 0.0002761705448755872.\n",
      "Epoch 1829/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01829: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0348 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01830: LearningRateScheduler reducing learning rate to 0.00027614514615264353.\n",
      "Epoch 1830/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0275\n",
      "Epoch 01830: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0275 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01831: LearningRateScheduler reducing learning rate to 0.00027611973508331016.\n",
      "Epoch 1831/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0273\n",
      "Epoch 01831: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0273 - val_loss: 0.0306\n",
      "\n",
      "Epoch 01832: LearningRateScheduler reducing learning rate to 0.00027609431167010406.\n",
      "Epoch 1832/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 01832: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0242 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01833: LearningRateScheduler reducing learning rate to 0.0002760688759155436.\n",
      "Epoch 1833/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01833: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0220 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01834: LearningRateScheduler reducing learning rate to 0.00027604342782214806.\n",
      "Epoch 1834/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01834: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0220 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01835: LearningRateScheduler reducing learning rate to 0.0002760179673924382.\n",
      "Epoch 1835/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 01835: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0290 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01836: LearningRateScheduler reducing learning rate to 0.00027599249462893603.\n",
      "Epoch 1836/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01836: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0265 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01837: LearningRateScheduler reducing learning rate to 0.0002759670095341646.\n",
      "Epoch 1837/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 01837: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0252 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01838: LearningRateScheduler reducing learning rate to 0.00027594151211064825.\n",
      "Epoch 1838/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 01838: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0339 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01839: LearningRateScheduler reducing learning rate to 0.00027591600236091265.\n",
      "Epoch 1839/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479\n",
      "Epoch 01839: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0479 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01840: LearningRateScheduler reducing learning rate to 0.0002758904802874845.\n",
      "Epoch 1840/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01840: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0254 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01841: LearningRateScheduler reducing learning rate to 0.0002758649458928919.\n",
      "Epoch 1841/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01841: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0254 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01842: LearningRateScheduler reducing learning rate to 0.00027583939917966404.\n",
      "Epoch 1842/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01842: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0337 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01843: LearningRateScheduler reducing learning rate to 0.0002758138401503314.\n",
      "Epoch 1843/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0179\n",
      "Epoch 01843: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0179 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01844: LearningRateScheduler reducing learning rate to 0.0002757882688074256.\n",
      "Epoch 1844/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 01844: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0178 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01845: LearningRateScheduler reducing learning rate to 0.00027576268515347967.\n",
      "Epoch 1845/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01845: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0259 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01846: LearningRateScheduler reducing learning rate to 0.00027573708919102763.\n",
      "Epoch 1846/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01846: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0280 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01847: LearningRateScheduler reducing learning rate to 0.0002757114809226048.\n",
      "Epoch 1847/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01847: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0259 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01848: LearningRateScheduler reducing learning rate to 0.00027568586035074785.\n",
      "Epoch 1848/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 01848: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0383 - val_loss: 0.0200\n",
      "\n",
      "Epoch 01849: LearningRateScheduler reducing learning rate to 0.00027566022747799445.\n",
      "Epoch 1849/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 01849: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0223 - val_loss: 0.0179\n",
      "\n",
      "Epoch 01850: LearningRateScheduler reducing learning rate to 0.00027563458230688367.\n",
      "Epoch 1850/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01850: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0256 - val_loss: 0.0175\n",
      "\n",
      "Epoch 01851: LearningRateScheduler reducing learning rate to 0.0002756089248399557.\n",
      "Epoch 1851/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01851: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0352 - val_loss: 0.0197\n",
      "\n",
      "Epoch 01852: LearningRateScheduler reducing learning rate to 0.00027558325507975184.\n",
      "Epoch 1852/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0323\n",
      "Epoch 01852: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0323 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01853: LearningRateScheduler reducing learning rate to 0.000275557573028815.\n",
      "Epoch 1853/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01853: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0280 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01854: LearningRateScheduler reducing learning rate to 0.0002755318786896888.\n",
      "Epoch 1854/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0192\n",
      "Epoch 01854: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0192 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01855: LearningRateScheduler reducing learning rate to 0.00027550617206491853.\n",
      "Epoch 1855/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 01855: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0175 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01856: LearningRateScheduler reducing learning rate to 0.00027548045315705033.\n",
      "Epoch 1856/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 01856: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0178 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01857: LearningRateScheduler reducing learning rate to 0.0002754547219686318.\n",
      "Epoch 1857/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 01857: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0211 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01858: LearningRateScheduler reducing learning rate to 0.00027542897850221164.\n",
      "Epoch 1858/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 01858: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0184 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01859: LearningRateScheduler reducing learning rate to 0.0002754032227603398.\n",
      "Epoch 1859/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 01859: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0222 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01860: LearningRateScheduler reducing learning rate to 0.0002753774547455675.\n",
      "Epoch 1860/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 01860: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0230 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01861: LearningRateScheduler reducing learning rate to 0.00027535167446044703.\n",
      "Epoch 1861/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 01861: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0368 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01862: LearningRateScheduler reducing learning rate to 0.00027532588190753207.\n",
      "Epoch 1862/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 01862: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0180 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01863: LearningRateScheduler reducing learning rate to 0.0002753000770893774.\n",
      "Epoch 1863/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0454\n",
      "Epoch 01863: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0454 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01864: LearningRateScheduler reducing learning rate to 0.000275274260008539.\n",
      "Epoch 1864/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01864: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0264 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01865: LearningRateScheduler reducing learning rate to 0.00027524843066757416.\n",
      "Epoch 1865/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01865: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0210 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01866: LearningRateScheduler reducing learning rate to 0.0002752225890690413.\n",
      "Epoch 1866/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 01866: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0239 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01867: LearningRateScheduler reducing learning rate to 0.0002751967352155002.\n",
      "Epoch 1867/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01867: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0294 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01868: LearningRateScheduler reducing learning rate to 0.00027517086910951166.\n",
      "Epoch 1868/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01868: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0204 - val_loss: 0.0179\n",
      "\n",
      "Epoch 01869: LearningRateScheduler reducing learning rate to 0.0002751449907536378.\n",
      "Epoch 1869/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 01869: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0301 - val_loss: 0.0191\n",
      "\n",
      "Epoch 01870: LearningRateScheduler reducing learning rate to 0.0002751191001504418.\n",
      "Epoch 1870/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0140\n",
      "Epoch 01870: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0140 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01871: LearningRateScheduler reducing learning rate to 0.0002750931973024884.\n",
      "Epoch 1871/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01871: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0332 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01872: LearningRateScheduler reducing learning rate to 0.0002750672822123432.\n",
      "Epoch 1872/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0286\n",
      "Epoch 01872: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0286 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01873: LearningRateScheduler reducing learning rate to 0.00027504135488257316.\n",
      "Epoch 1873/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0153\n",
      "Epoch 01873: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0153 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01874: LearningRateScheduler reducing learning rate to 0.0002750154153157465.\n",
      "Epoch 1874/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 01874: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0243 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01875: LearningRateScheduler reducing learning rate to 0.0002749894635144326.\n",
      "Epoch 1875/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0198\n",
      "Epoch 01875: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0198 - val_loss: 0.0195\n",
      "\n",
      "Epoch 01876: LearningRateScheduler reducing learning rate to 0.0002749634994812019.\n",
      "Epoch 1876/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01876: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0353 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01877: LearningRateScheduler reducing learning rate to 0.0002749375232186264.\n",
      "Epoch 1877/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01877: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0249 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01878: LearningRateScheduler reducing learning rate to 0.000274911534729279.\n",
      "Epoch 1878/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0165\n",
      "Epoch 01878: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0165 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01879: LearningRateScheduler reducing learning rate to 0.00027488553401573386.\n",
      "Epoch 1879/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01879: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0220 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01880: LearningRateScheduler reducing learning rate to 0.00027485952108056657.\n",
      "Epoch 1880/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01880: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0262 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01881: LearningRateScheduler reducing learning rate to 0.00027483349592635363.\n",
      "Epoch 1881/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01881: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0315 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01882: LearningRateScheduler reducing learning rate to 0.000274807458555673.\n",
      "Epoch 1882/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0156\n",
      "Epoch 01882: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0156 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01883: LearningRateScheduler reducing learning rate to 0.0002747814089711036.\n",
      "Epoch 1883/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01883: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0372 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01884: LearningRateScheduler reducing learning rate to 0.00027475534717522586.\n",
      "Epoch 1884/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01884: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0276 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01885: LearningRateScheduler reducing learning rate to 0.00027472927317062114.\n",
      "Epoch 1885/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0271\n",
      "Epoch 01885: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0271 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01886: LearningRateScheduler reducing learning rate to 0.00027470318695987223.\n",
      "Epoch 1886/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01886: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0244 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01887: LearningRateScheduler reducing learning rate to 0.00027467708854556294.\n",
      "Epoch 1887/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0203\n",
      "Epoch 01887: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0203 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01888: LearningRateScheduler reducing learning rate to 0.00027465097793027846.\n",
      "Epoch 1888/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01888: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0213 - val_loss: 0.0200\n",
      "\n",
      "Epoch 01889: LearningRateScheduler reducing learning rate to 0.0002746248551166051.\n",
      "Epoch 1889/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0367\n",
      "Epoch 01889: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0367 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01890: LearningRateScheduler reducing learning rate to 0.00027459872010713026.\n",
      "Epoch 1890/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 01890: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0202 - val_loss: 0.0182\n",
      "\n",
      "Epoch 01891: LearningRateScheduler reducing learning rate to 0.0002745725729044428.\n",
      "Epoch 1891/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 01891: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0205 - val_loss: 0.0167\n",
      "\n",
      "Epoch 01892: LearningRateScheduler reducing learning rate to 0.0002745464135111326.\n",
      "Epoch 1892/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01892: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0340 - val_loss: 0.0177\n",
      "\n",
      "Epoch 01893: LearningRateScheduler reducing learning rate to 0.00027452024192979086.\n",
      "Epoch 1893/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01893: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0314 - val_loss: 0.0182\n",
      "\n",
      "Epoch 01894: LearningRateScheduler reducing learning rate to 0.0002744940581630099.\n",
      "Epoch 1894/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0174\n",
      "Epoch 01894: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0174 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01895: LearningRateScheduler reducing learning rate to 0.0002744678622133833.\n",
      "Epoch 1895/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01895: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0195 - val_loss: 0.0200\n",
      "\n",
      "Epoch 01896: LearningRateScheduler reducing learning rate to 0.00027444165408350583.\n",
      "Epoch 1896/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0234\n",
      "Epoch 01896: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0234 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01897: LearningRateScheduler reducing learning rate to 0.00027441543377597344.\n",
      "Epoch 1897/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 01897: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0267 - val_loss: 0.0219\n",
      "\n",
      "Epoch 01898: LearningRateScheduler reducing learning rate to 0.0002743892012933833.\n",
      "Epoch 1898/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 01898: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0223 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01899: LearningRateScheduler reducing learning rate to 0.00027436295663833387.\n",
      "Epoch 1899/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01899: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0276 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01900: LearningRateScheduler reducing learning rate to 0.0002743366998134247.\n",
      "Epoch 1900/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 01900: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0229 - val_loss: 0.0198\n",
      "\n",
      "Epoch 01901: LearningRateScheduler reducing learning rate to 0.00027431043082125654.\n",
      "Epoch 1901/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01901: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0280 - val_loss: 0.0180\n",
      "\n",
      "Epoch 01902: LearningRateScheduler reducing learning rate to 0.0002742841496644315.\n",
      "Epoch 1902/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01902: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0299 - val_loss: 0.0161\n",
      "\n",
      "Epoch 01903: LearningRateScheduler reducing learning rate to 0.0002742578563455527.\n",
      "Epoch 1903/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0214\n",
      "Epoch 01903: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0214 - val_loss: 0.0175\n",
      "\n",
      "Epoch 01904: LearningRateScheduler reducing learning rate to 0.00027423155086722455.\n",
      "Epoch 1904/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 01904: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0237 - val_loss: 0.0188\n",
      "\n",
      "Epoch 01905: LearningRateScheduler reducing learning rate to 0.0002742052332320528.\n",
      "Epoch 1905/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 01905: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0255 - val_loss: 0.0205\n",
      "\n",
      "Epoch 01906: LearningRateScheduler reducing learning rate to 0.0002741789034426441.\n",
      "Epoch 1906/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01906: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0292 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01907: LearningRateScheduler reducing learning rate to 0.0002741525615016066.\n",
      "Epoch 1907/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0157\n",
      "Epoch 01907: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0157 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01908: LearningRateScheduler reducing learning rate to 0.0002741262074115495.\n",
      "Epoch 1908/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 01908: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0217 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01909: LearningRateScheduler reducing learning rate to 0.0002740998411750831.\n",
      "Epoch 1909/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01909: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0263 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01910: LearningRateScheduler reducing learning rate to 0.0002740734627948193.\n",
      "Epoch 1910/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0373\n",
      "Epoch 01910: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0373 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01911: LearningRateScheduler reducing learning rate to 0.0002740470722733707.\n",
      "Epoch 1911/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0164\n",
      "Epoch 01911: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0164 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01912: LearningRateScheduler reducing learning rate to 0.00027402066961335146.\n",
      "Epoch 1912/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0381\n",
      "Epoch 01912: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0381 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01913: LearningRateScheduler reducing learning rate to 0.0002739942548173768.\n",
      "Epoch 1913/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01913: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0283 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01914: LearningRateScheduler reducing learning rate to 0.0002739678278880632.\n",
      "Epoch 1914/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01914: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0329 - val_loss: 0.0206\n",
      "\n",
      "Epoch 01915: LearningRateScheduler reducing learning rate to 0.0002739413888280282.\n",
      "Epoch 1915/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01915: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0259 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01916: LearningRateScheduler reducing learning rate to 0.0002739149376398908.\n",
      "Epoch 1916/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01916: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0291 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01917: LearningRateScheduler reducing learning rate to 0.000273888474326271.\n",
      "Epoch 1917/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01917: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0204 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01918: LearningRateScheduler reducing learning rate to 0.0002738619988897899.\n",
      "Epoch 1918/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01918: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0247 - val_loss: 0.0262\n",
      "\n",
      "Epoch 01919: LearningRateScheduler reducing learning rate to 0.00027383551133307023.\n",
      "Epoch 1919/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 01919: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0224 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01920: LearningRateScheduler reducing learning rate to 0.0002738090116587354.\n",
      "Epoch 1920/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01920: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0277 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01921: LearningRateScheduler reducing learning rate to 0.0002737824998694104.\n",
      "Epoch 1921/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0192\n",
      "Epoch 01921: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0192 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01922: LearningRateScheduler reducing learning rate to 0.0002737559759677212.\n",
      "Epoch 1922/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0174\n",
      "Epoch 01922: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0174 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01923: LearningRateScheduler reducing learning rate to 0.00027372943995629514.\n",
      "Epoch 1923/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 01923: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0239 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01924: LearningRateScheduler reducing learning rate to 0.00027370289183776067.\n",
      "Epoch 1924/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01924: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0195 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01925: LearningRateScheduler reducing learning rate to 0.0002736763316147473.\n",
      "Epoch 1925/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01925: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0262 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01926: LearningRateScheduler reducing learning rate to 0.00027364975928988606.\n",
      "Epoch 1926/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 01926: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0251 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01927: LearningRateScheduler reducing learning rate to 0.0002736231748658089.\n",
      "Epoch 1927/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01927: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0254 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01928: LearningRateScheduler reducing learning rate to 0.0002735965783451491.\n",
      "Epoch 1928/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 01928: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0181 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01929: LearningRateScheduler reducing learning rate to 0.0002735699697305411.\n",
      "Epoch 1929/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0285\n",
      "Epoch 01929: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0285 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01930: LearningRateScheduler reducing learning rate to 0.0002735433490246205.\n",
      "Epoch 1930/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01930: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0212 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01931: LearningRateScheduler reducing learning rate to 0.0002735167162300243.\n",
      "Epoch 1931/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01931: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0264 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01932: LearningRateScheduler reducing learning rate to 0.0002734900713493903.\n",
      "Epoch 1932/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 01932: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0253 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01933: LearningRateScheduler reducing learning rate to 0.00027346341438535794.\n",
      "Epoch 1933/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01933: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0210 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01934: LearningRateScheduler reducing learning rate to 0.0002734367453405676.\n",
      "Epoch 1934/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01934: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0291 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01935: LearningRateScheduler reducing learning rate to 0.0002734100642176608.\n",
      "Epoch 1935/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 01935: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0242 - val_loss: 0.0210\n",
      "\n",
      "Epoch 01936: LearningRateScheduler reducing learning rate to 0.00027338337101928055.\n",
      "Epoch 1936/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 01936: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0178 - val_loss: 0.0192\n",
      "\n",
      "Epoch 01937: LearningRateScheduler reducing learning rate to 0.0002733566657480707.\n",
      "Epoch 1937/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0189\n",
      "Epoch 01937: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0189 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01938: LearningRateScheduler reducing learning rate to 0.0002733299484066766.\n",
      "Epoch 1938/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01938: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0206 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01939: LearningRateScheduler reducing learning rate to 0.0002733032189977446.\n",
      "Epoch 1939/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 01939: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0184 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01940: LearningRateScheduler reducing learning rate to 0.0002732764775239223.\n",
      "Epoch 1940/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01940: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0213 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01941: LearningRateScheduler reducing learning rate to 0.00027324972398785853.\n",
      "Epoch 1941/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01941: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0294 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01942: LearningRateScheduler reducing learning rate to 0.00027322295839220327.\n",
      "Epoch 1942/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0218\n",
      "Epoch 01942: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0218 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01943: LearningRateScheduler reducing learning rate to 0.00027319618073960776.\n",
      "Epoch 1943/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 01943: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0178 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01944: LearningRateScheduler reducing learning rate to 0.00027316939103272435.\n",
      "Epoch 1944/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 01944: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0184 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01945: LearningRateScheduler reducing learning rate to 0.00027314258927420667.\n",
      "Epoch 1945/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01945: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0262 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01946: LearningRateScheduler reducing learning rate to 0.00027311577546670944.\n",
      "Epoch 1946/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0248\n",
      "Epoch 01946: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0248 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01947: LearningRateScheduler reducing learning rate to 0.0002730889496128886.\n",
      "Epoch 1947/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0192\n",
      "Epoch 01947: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0192 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01948: LearningRateScheduler reducing learning rate to 0.0002730621117154014.\n",
      "Epoch 1948/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 01948: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0175 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01949: LearningRateScheduler reducing learning rate to 0.0002730352617769061.\n",
      "Epoch 1949/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 01949: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0200 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01950: LearningRateScheduler reducing learning rate to 0.0002730083998000624.\n",
      "Epoch 1950/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 01950: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0200 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01951: LearningRateScheduler reducing learning rate to 0.00027298152578753086.\n",
      "Epoch 1951/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0341\n",
      "Epoch 01951: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0341 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01952: LearningRateScheduler reducing learning rate to 0.00027295463974197354.\n",
      "Epoch 1952/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 01952: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0222 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01953: LearningRateScheduler reducing learning rate to 0.0002729277416660535.\n",
      "Epoch 1953/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01953: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0213 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01954: LearningRateScheduler reducing learning rate to 0.0002729008315624351.\n",
      "Epoch 1954/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 01954: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0201 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01955: LearningRateScheduler reducing learning rate to 0.00027287390943378384.\n",
      "Epoch 1955/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01955: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0210 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01956: LearningRateScheduler reducing learning rate to 0.00027284697528276637.\n",
      "Epoch 1956/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 01956: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0201 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01957: LearningRateScheduler reducing learning rate to 0.0002728200291120506.\n",
      "Epoch 1957/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01957: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0294 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01958: LearningRateScheduler reducing learning rate to 0.00027279307092430567.\n",
      "Epoch 1958/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0161\n",
      "Epoch 01958: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0161 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01959: LearningRateScheduler reducing learning rate to 0.00027276610072220176.\n",
      "Epoch 1959/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01959: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0283 - val_loss: 0.0183\n",
      "\n",
      "Epoch 01960: LearningRateScheduler reducing learning rate to 0.0002727391185084104.\n",
      "Epoch 1960/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 01960: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0199 - val_loss: 0.0187\n",
      "\n",
      "Epoch 01961: LearningRateScheduler reducing learning rate to 0.00027271212428560426.\n",
      "Epoch 1961/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01961: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0245 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01962: LearningRateScheduler reducing learning rate to 0.00027268511805645713.\n",
      "Epoch 1962/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0196\n",
      "Epoch 01962: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0196 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01963: LearningRateScheduler reducing learning rate to 0.00027265809982364404.\n",
      "Epoch 1963/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 01963: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0178 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01964: LearningRateScheduler reducing learning rate to 0.00027263106958984117.\n",
      "Epoch 1964/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0309\n",
      "Epoch 01964: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0309 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01965: LearningRateScheduler reducing learning rate to 0.000272604027357726.\n",
      "Epoch 1965/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 01965: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0158 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01966: LearningRateScheduler reducing learning rate to 0.0002725769731299771.\n",
      "Epoch 1966/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01966: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0256 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01967: LearningRateScheduler reducing learning rate to 0.00027254990690927426.\n",
      "Epoch 1967/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 01967: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.0215 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01968: LearningRateScheduler reducing learning rate to 0.00027252282869829844.\n",
      "Epoch 1968/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 01968: val_loss did not improve from 0.01317\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0306 - val_loss: 0.0264\n",
      "Epoch 01968: early stopping\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_dataloader, epochs=Max_epoch, initial_epoch=stop_epoch, validation_data=val_dataloader, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the trajectory of loss function\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss']+history2.history['loss'])\n",
    "plt.plot(history.history['val_loss']+history2.history['val_loss'])\n",
    "plt.axvline(stop_epoch, color=\"red\")\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('YOLO loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(100)\n",
    "plt.legend(['Train', 'val', 'UnFreeze_Epoch'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the trajectory of loss function\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['lr']+history2.history['lr'])\n",
    "plt.axvline(stop_epoch, color=\"red\")\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('YOLO loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(3)\n",
    "plt.ylim(0.00025, 0.00032)\n",
    "plt.legend(['Train', 'val', 'UnFreeze_Epoch'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3114c5a3de934491c2bf9db70eab0f2401d1d0e147bdce7e226ba3ca05564493"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ops_keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
